{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNIXjEWSa8qOqFHVpL7NNl6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"63fQQmFpatwy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768569695385,"user_tz":300,"elapsed":9,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"747fbb01-fc1d-4ce7-991b-aa54a19f157c"},"outputs":[{"output_type":"stream","name":"stdout","text":["range(0, 10)\n","<class 'range'>\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n"]}],"source":["def count_chars(input_str):\n","    counts = {}\n","\n","    for c in input_str:\n","        counts[c] = counts.get(c, 0) + 1\n","\n","    return counts\n","\n","print(count_chars('hello there'))"]},{"cell_type":"code","source":["def replace_pairs(lst, target_pair, combined_id):\n","    replaced_ids = []\n","\n","    i = 0\n","    while i < len(lst):\n","        if tuple(lst[i:i+2]) == target_pair:\n","            replaced_ids.append(combined_id)\n","            i += 2\n","        else:\n","            replaced_ids.append(lst[i])\n","            i += 1\n","\n","    return replaced_ids\n","\n","\n","print(replace_pairs([1, 2, 3, 1, 2], (1, 2), 4))"],"metadata":{"id":"FDM9pKOuKkTp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","\n","def count_pairs(nums):\n","    counts = Counter()\n","\n","    for pair in zip(nums, nums[1:]):\n","        counts[pair] = counts.get(pair, 0) + 1\n","\n","    return counts\n","\n","prompt = 'rug pug hug pun bun hugs run gun'\n","\n","string_bytes = list(prompt.encode('utf-8'))\n","vocab = {idx: bytes([idx]) for idx in range(256)}\n","\n","print(string_bytes)\n","\n","pairs = count_pairs(string_bytes)\n","print(pairs)\n","\n","merged_string = replace_pairs(string_bytes, pairs.most_common()[0][0], 257)\n","print(merged_string)"],"metadata":{"id":"Rw4FpjXQeWgY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Cat():\n","    def __init__(self, color):\n","        self.color = color\n","\n","    def purr(self):\n","        print('puurrrrrr')\n","\n","    def meow(self):\n","        print('meow')\n","\n","    def get_color(self):\n","        print(self.color)"],"metadata":{"id":"Hm8MWTnVKh6H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Minimal (byte-level) Byte Pair Encoding tokenizer.\n","\n","Algorithmically follows along the GPT tokenizer:\n","https://github.com/openai/gpt-2/blob/master/src/encoder.py\n","\"\"\"\n","\n","class Simple_Tokenizer():\n","    def __init__(self):\n","        self.merges = {} # (int, int) -> int\n","\n","        self.special_tokens = {'<|sos|>': 256, '<|eos|>': 257} # str -> int, e.g. {'<|endoftext|>': 100257}\n","        self.vocab = {idx: bytes([idx]) for idx in range(256)} | {idx: special.encode(\"utf-8\") for special, idx in self.special_tokens.items()}\n","\n","    def _count_pairs(self, ids, counts=None):\n","        \"\"\"\n","        Given a list of integers, return a dictionary of counts of consecutive pairs\n","        Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n","        Optionally allows to update an existing dictionary of counts\n","        \"\"\"\n","        counts = {} if counts is None else counts\n","\n","        for pair in zip(ids, ids[1:]): # iterate consecutive elements\n","            counts[pair] = counts.get(pair, 0) + 1\n","\n","        return counts\n","\n","    def _merge_pairs(self, ids, pair, idx):\n","        \"\"\"\n","        In the list of integers (ids), replace all consecutive occurrences\n","        of pair with the new integer token idx\n","        Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n","        \"\"\"\n","        newids = []\n","        i = 0\n","\n","        while i < len(ids):\n","            # if not at the very last position AND the pair matches, replace it\n","            if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n","                newids.append(idx)\n","                i += 2\n","            else:\n","                newids.append(ids[i])\n","                i += 1\n","\n","        return newids\n","\n","    def train(self, text, max_vocab_size):\n","        vocab_size = len(self.vocab)\n","        num_merges = max_vocab_size - vocab_size\n","\n","        # input text preprocessing\n","        text_bytes = text.encode(\"utf-8\") # raw bytes\n","        ids = list(text_bytes) # list of integers in range 0..255\n","\n","        for i in range(num_merges):\n","            # count up the number of times every consecutive pair appears\n","            stats = self._count_pairs(ids)\n","\n","            # find the pair with the highest count\n","            pair = max(stats, key=stats.get)\n","\n","            # mint a new token: assign it the next available id\n","            idx = vocab_size + i\n","\n","            # replace all occurrences of pair in ids with idx\n","            ids = self._merge_pairs(ids, pair, idx)\n","\n","            # save the merge\n","            self.merges[pair] = idx\n","            self.vocab[idx] = self.vocab[pair[0]] + self.vocab[pair[1]]\n","\n","    def decode(self, ids):\n","        # given ids (list of integers), return Python string\n","        text_bytes = b\"\".join(self.vocab[idx] for idx in ids)\n","        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n","        return text\n","\n","    def encode(self, text):\n","        import re\n","        special_pattern = \"(\" + \"|\".join(re.escape(k) for k in self.special_tokens) + \")\"\n","        special_chunks = re.split(special_pattern, text)\n","\n","        encoded_str = []\n","        for chunk in special_chunks[1:-1]:\n","            if chunk in self.special_tokens:\n","                # this is a special token, encode it separately as a special case\n","                encoded_str.append(self.special_tokens[chunk])\n","            else:\n","                # given a string text, return the token ids\n","                text_bytes = chunk.encode(\"utf-8\") # raw bytes\n","                chunk_ids = list(text_bytes) # list of integers in range 0..255\n","\n","                while len(chunk_ids) >= 2:\n","                    # find the pair with the lowest merge index\n","                    counted_pairs = self._count_pairs(chunk_ids)\n","                    earliest_pair = min(counted_pairs, key=lambda p: self.merges.get(p, float(\"inf\")))\n","\n","                    # just the first pair in the list, arbitrarily\n","                    # we can detect this terminating case by a membership check\n","                    if earliest_pair not in self.merges:\n","                        break # nothing else can be merged anymore\n","\n","                    # otherwise let's merge the best pair (lowest merge index)\n","                    pair_idx = self.merges[earliest_pair]\n","                    chunk_ids = self._merge_pairs(chunk_ids, earliest_pair, pair_idx)\n","\n","                encoded_str += chunk_ids\n","\n","        return encoded_str\n","\n","    def visualize_tokenization(self, ids):\n","        \"\"\"Small helper function useful in debugging: visualize the tokenization of render_conversation\"\"\"\n","        GREEN = '\\033[92m'\n","        RESET = '\\033[0m'\n","        GRAY = '\\033[90m'\n","\n","        tokens = []\n","        for token_id in ids:\n","            token_str = self.decode([token_id])\n","            tokens.append(f\"{GREEN}{token_str}{GRAY}({token_id}){RESET}\")\n","\n","            if token_str in ['<|sos|>', '<|eos|>']:\n","                tokens.append('\\n\\n\\t')\n","\n","        return ' | '.join(tokens)"],"metadata":{"id":"s-PjQGMLIlSR"},"execution_count":null,"outputs":[]}]}