{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOHeXHRaVoaQyhsPJU/r13w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"PvqYppy8Jk4G"},"outputs":[],"source":["def create_matrix(rows, cols):\n","    matrix = []\n","\n","    for r in range(rows):\n","        matrix.append([])\n","        for i in range(cols):\n","            matrix[r].append(0)\n","\n","    return matrix\n","\n","print(create_matrix(2, 2))"]},{"cell_type":"code","source":["def create_matrix(rows, cols):\n","    return [[0] * cols for r in range(rows)]\n","\n","print(create_matrix(2, 2))"],"metadata":{"id":"pKO77lk0JqA5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","\n","def _count_pairs(tokens):\n","    pair_counts = Counter()\n","\n","    for pair in zip(tokens, tokens[1:]):\n","        pair_counts[pair] += 1\n","\n","    return pair_counts\n","\n","def _merge_pair(tokens, target_pair, combined_id):\n","    merged_tokens = []\n","\n","    i = 0\n","    while i < len(tokens):\n","        if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == target_pair:\n","            merged_tokens.append(combined_id)\n","            i += 2\n","        else:\n","            merged_tokens.append(tokens[i])\n","            i += 1\n","\n","    return merged_tokens\n","\n","prompt = 'rug pug hug pun bun hugs run gun bug'\n","\n","string_bytes = list(prompt.encode('utf-8'))\n","print('String ASCII Values (bytes):', string_bytes)\n","\n","vocab = {idx: bytes([idx]) for idx in range(256)}\n","print('Initial Vocab:', vocab)\n","\n","pairs = _count_pairs(string_bytes)\n","print('Pairs and Counts:', pairs)\n","\n","merged_string = _merge_pair(string_bytes, (117, 103), 257)\n","print('After First Merge:', merged_string)\n","\n","merged_string = _merge_pair(merged_string, (117, 110), 258)\n","print('After Second Merge:', merged_string)"],"metadata":{"id":"P-vUG_MaVEJz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Cat():\n","    def __init__(self, color):\n","        self.color = color\n","\n","    def purr(self):\n","        print('puurrrrrr')\n","\n","    def meow(self):\n","        print('meow')\n","\n","    def get_color(self):\n","        print(self.color)"],"metadata":{"id":"im8Cn-KiVecv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Minimal (byte-level) Byte Pair Encoding tokenizer.\n","\n","Algorithmically follows along the GPT tokenizer:\n","https://github.com/openai/gpt-2/blob/master/src/encoder.py\n","\"\"\"\n","\n","class Simple_Tokenizer():\n","    def __init__(self):\n","        self.merges = {} # (int, int) -> int\n","\n","        self.special_tokens = {'<|sos|>': 256, '<|eos|>': 257} # str -> int, e.g. {'<|endoftext|>': 100257}\n","        self.vocab = {idx: bytes([idx]) for idx in range(256)} | {idx: special.encode(\"utf-8\") for special, idx in self.special_tokens.items()}\n","\n","\n","    def _count_pairs(self, tokens, counts=None):\n","        pair_counts = Counter()\n","\n","        for pair in zip(tokens, tokens[1:]):\n","            pair_counts[pair] += 1\n","\n","        return pair_counts\n","\n","    def _merge_pair(self, tokens, target_pair, combined_id):\n","        merged_tokens = []\n","\n","        i = 0\n","        while i < len(tokens):\n","            if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == target_pair:\n","                merged_tokens.append(combined_id)\n","                i += 2\n","            else:\n","                merged_tokens.append(tokens[i])\n","                i += 1\n","\n","        return merged_tokens\n","\n","    def train(self, text, max_vocab_size):\n","        vocab_size = len(self.vocab)\n","        num_merges = max_vocab_size - vocab_size\n","\n","        text_bytes = text.encode('utf-8')\n","        tokens = list(text_bytes)\n","\n","        for i in range(num_merges):\n","            pairs = self._count_pairs(ids)\n","\n","            pair = pairs.most_common()[0][0]\n","\n","            merged_token_id = vocab_size + i\n","\n","            ids = self._merge_pair(tokens, pair, merged_token_id)\n","\n","            self.merges[pair] = merged_token_id\n","            self.vocab[merged_token_id] = self.vocab[pair[0]] + self.vocab[pair[1]]\n","\n","    def decode(self, ids):\n","        # given ids (list of integers), return Python string\n","        text_bytes = b\"\".join(self.vocab[idx] for idx in ids)\n","        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n","        return text\n","\n","    def encode(self, text):\n","        import re\n","        special_pattern = \"(\" + \"|\".join(re.escape(k) for k in self.special_tokens) + \")\"\n","        special_chunks = re.split(special_pattern, text)\n","\n","        encoded_str = []\n","        for chunk in special_chunks[1:-1]:\n","            if chunk in self.special_tokens:\n","                # this is a special token, encode it separately as a special case\n","                encoded_str.append(self.special_tokens[chunk])\n","            else:\n","                # given a string text, return the token ids\n","                text_bytes = chunk.encode(\"utf-8\") # raw bytes\n","                chunk_ids = list(text_bytes) # list of integers in range 0..255\n","\n","                while len(chunk_ids) >= 2:\n","                    # find the pair with the lowest merge index\n","                    counted_pairs = self._count_pairs(chunk_ids)\n","                    earliest_pair = min(counted_pairs, key=lambda p: self.merges.get(p, float(\"inf\")))\n","\n","                    # just the first pair in the list, arbitrarily\n","                    # we can detect this terminating case by a membership check\n","                    if earliest_pair not in self.merges:\n","                        break # nothing else can be merged anymore\n","\n","                    # otherwise let's merge the best pair (lowest merge index)\n","                    pair_idx = self.merges[earliest_pair]\n","                    chunk_ids = self._merge_pairs(chunk_ids, earliest_pair, pair_idx)\n","\n","                encoded_str += chunk_ids\n","\n","        return encoded_str"],"metadata":{"id":"uvPQDVZ6WDuZ"},"execution_count":null,"outputs":[]}]}