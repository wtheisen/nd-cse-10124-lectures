{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPh5PPgV5Udzop+bBd30MB0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-001.png\" class=\"img-responsive\"/>\n","</div>\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-002.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"EUZY_VNVJVKT"}},{"cell_type":"code","source":["l = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n","\n","for i in l:\n","  print(i + 5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Sj_A8jo8vbE","executionInfo":{"status":"ok","timestamp":1768570913861,"user_tz":300,"elapsed":62,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"f34aa801-a847-41b8-a102-de690e18a1fc"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n"]}]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-003.png\" class=\"img-responsive\"/>\n","</div>\n","\n"],"metadata":{"id":"mxAAF5GY8vxw"}},{"cell_type":"code","source":["#print(5 + '5')"],"metadata":{"id":"58YQQupe_7UK","executionInfo":{"status":"ok","timestamp":1768570913863,"user_tz":300,"elapsed":2,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-004.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"mtQM5cbm_6f7"}},{"cell_type":"code","source":["combined = 5 + int('5')\n","print(type(combined))\n","print(combined)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BLGTx_tM-IAn","executionInfo":{"status":"ok","timestamp":1768572010336,"user_tz":300,"elapsed":6,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"44ded49a-0b16-4791-9089-4502b19b84ed"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'int'>\n","10\n"]}]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-005.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"0jUdJ9FmIsfB"}},{"cell_type":"code","source":["def count_consecutive_pairs(ids, counts=None):\n","\n","    # TODO\n","\n","    return counts"],"metadata":{"id":"MoxgX40-IuO7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-006.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"5spfu41pJHGy"}},{"cell_type":"code","source":["!pwd\n","%cd ./sample_data/\n","!pwd\n","%cd /content/\n","!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yKQ7MmBznvwj","executionInfo":{"status":"ok","timestamp":1768580771425,"user_tz":300,"elapsed":327,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"ab3ab78d-dfbd-4212-c50b-e603433d2dd1"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["/root\n","[Errno 2] No such file or directory: './sample_data/'\n","/root\n","/root\n","/content\n","/content\n"]}]},{"cell_type":"code","source":["import os\n","\n","try:\n","    import google.colab\n","    REPO_URL = \"https://github.com/wtheisen/nd-cse-10124-lectures.git\"\n","\n","    REPO_NAME = \"/content/nd-cse-10124-lectures\"\n","    L_PATH = \"nd-cse-10124-lectures/Datasets\"\n","\n","    %cd /content/\n","    !rm -r {REPO_NAME}\n","\n","    # Clone repo\n","    if not os.path.exists(REPO_NAME):\n","        !git clone {REPO_URL}\n","\n","        # cd into the data folder\n","        %cd {L_PATH}\n","        !pwd\n","\n","except ImportError:\n","    print(\"Unable to download repo, either:\")\n","    print(\"\\tA.) You're not on colab\")\n","    print(\"\\tB.) It has already been cloned\")\n","\n","import utilities"],"metadata":{"id":"XauIb-liOtSM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"06691b0d-e522-4a70-adf3-dcbab979f811"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","rm: cannot remove '/content/nd-cse-10124-lectures': No such file or directory\n","Cloning into 'nd-cse-10124-lectures'...\n"]}]},{"cell_type":"code","source":["class Cat():\n","    def __init__(self, color):\n","        self.color = color\n","\n","    # TODO"],"metadata":{"id":"dv3sQu75KJyT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Minimal (byte-level) Byte Pair Encoding tokenizer.\n","\n","Algorithmically follows along the GPT tokenizer:\n","https://github.com/openai/gpt-2/blob/master/src/encoder.py\n","\"\"\"\n","\n","class BasicTokenizer():\n","    def __init__(self):\n","        self.merges = {} # (int, int) -> int\n","        self.pattern = \"\" # str\n","        self.special_tokens = {} # str -> int, e.g. {'<|endoftext|>': 100257}\n","        self.vocab = self._build_vocab() # int -> bytes\n","\n","    def _build_vocab(self):\n","        # vocab is simply and deterministically derived from merges\n","        vocab = {idx: bytes([idx]) for idx in range(256)}\n","        for (p0, p1), idx in self.merges.items():\n","            vocab[idx] = vocab[p0] + vocab[p1]\n","        for special, idx in self.special_tokens.items():\n","            vocab[idx] = special.encode(\"utf-8\")\n","        return vocab\n","\n","    def _get_stats(self, ids, counts=None):\n","        \"\"\"\n","        Given a list of integers, return a dictionary of counts of consecutive pairs\n","        Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n","        Optionally allows to update an existing dictionary of counts\n","        \"\"\"\n","        counts = {} if counts is None else counts\n","        for pair in zip(ids, ids[1:]): # iterate consecutive elements\n","            counts[pair] = counts.get(pair, 0) + 1\n","        return counts\n","\n","    def _merge(self, ids, pair, idx):\n","        \"\"\"\n","        In the list of integers (ids), replace all consecutive occurrences\n","        of pair with the new integer token idx\n","        Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n","        \"\"\"\n","        newids = []\n","        i = 0\n","        while i < len(ids):\n","            # if not at the very last position AND the pair matches, replace it\n","            if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n","                newids.append(idx)\n","                i += 2\n","            else:\n","                newids.append(ids[i])\n","                i += 1\n","        return newids\n","\n","    def train(self, text, vocab_size, verbose=False):\n","        assert vocab_size >= 256\n","        num_merges = vocab_size - 256\n","\n","        # input text preprocessing\n","        text_bytes = text.encode(\"utf-8\") # raw bytes\n","        ids = list(text_bytes) # list of integers in range 0..255\n","\n","        # iteratively merge the most common pairs to create new tokens\n","        merges = {} # (int, int) -> int\n","        vocab = {idx: bytes([idx]) for idx in range(256)} # int -> bytes\n","        for i in range(num_merges):\n","            # count up the number of times every consecutive pair appears\n","            stats = self._get_stats(ids)\n","            # find the pair with the highest count\n","            pair = max(stats, key=stats.get)\n","            # mint a new token: assign it the next available id\n","            idx = 256 + i\n","            # replace all occurrences of pair in ids with idx\n","            ids = self._merge(ids, pair, idx)\n","            # save the merge\n","            merges[pair] = idx\n","            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n","            # prints\n","            if verbose:\n","                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n","\n","        # save class variables\n","        self.merges = merges # used in encode()\n","        self.vocab = vocab   # used in decode()\n","\n","    def decode(self, ids):\n","        # given ids (list of integers), return Python string\n","        text_bytes = b\"\".join(self.vocab[idx] for idx in ids)\n","        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n","        return text\n","\n","    def encode(self, text):\n","        # given a string text, return the token ids\n","        text_bytes = text.encode(\"utf-8\") # raw bytes\n","        ids = list(text_bytes) # list of integers in range 0..255\n","        while len(ids) >= 2:\n","            # find the pair with the lowest merge index\n","            stats = self._get_stats(ids)\n","            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n","            # subtle: if there are no more merges available, the key will\n","            # result in an inf for every single pair, and the min will be\n","            # just the first pair in the list, arbitrarily\n","            # we can detect this terminating case by a membership check\n","            if pair not in self.merges:\n","                break # nothing else can be merged anymore\n","            # otherwise let's merge the best pair (lowest merge index)\n","            idx = self.merges[pair]\n","            ids = self._merge(ids, pair, idx)\n","        return ids"],"metadata":{"id":"FMNlxMh4B7Eu","executionInfo":{"status":"ok","timestamp":1768572739292,"user_tz":300,"elapsed":3,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["def get_file_str(source_file): # Define a function named 'get_lines_str' that takes an argument called 'source_file'\n","    zoomer_text = '' # Create a variable named 'lines' points to a list data structure\n","    with open(source_file) as f:\n","        for line in f: # For each item in the variable 'f', set the variable named 'line' equal to it, one by one\n","            zoomer_text += line.strip() # Add the content of the variable 'line' to the end of the list named 'lines' (after removing whitespace and newlines on either end)\n","\n","    return zoomer_text # This function returns the variable named 'lines'\n","\n","def get_lines_str(source_file): # Define a function named 'get_lines_str' that takes an argument called 'source_file'\n","    lines = [] # Create a variable named 'lines' points to a list data structure\n","    with open(source_file) as f:\n","        for line in f: # For each item in the variable 'f', set the variable named 'line' equal to it, one by one\n","            lines.append(line.strip()) # Add the content of the variable 'line' to the end of the list named 'lines' (after removing whitespace and newlines on either end)\n","\n","    return lines # This function returns the variable named 'lines'\n","\n","zoomer_text = get_file_str('zoomer.txt')\n","zoomer_lines = get_lines_str('zoomer.txt')\n","\n","tokenizer = BasicTokenizer()\n","tokenizer.train(zoomer_text, 256 + 3)\n","\n","for i in range(0, 7):\n","  print(zoomer_lines[i])\n","  tokenized_str = tokenizer.encode(zoomer_lines[i])\n","  print(tokenized_str)\n","  print(tokenizer.decode(tokenized_str))\n","  print('-' * 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SQw48jfcDSL6","executionInfo":{"status":"ok","timestamp":1768572741760,"user_tz":300,"elapsed":69,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"20c1f359-e337-4dba-88ad-7f3e1f19ca08"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Got the job today, big W!\n","[71, 111, 257, 116, 104, 256, 106, 111, 98, 258, 111, 100, 97, 121, 44, 32, 98, 105, 103, 32, 87, 33]\n","Got the job today, big W!\n","--------------------------------------------------------------------------------\n","I forgot my wallet at home, that’s an L.\n","[73, 32, 102, 111, 114, 103, 111, 257, 109, 121, 32, 119, 97, 108, 108, 101, 257, 97, 257, 104, 111, 109, 101, 44, 258, 104, 97, 116, 226, 128, 153, 115, 32, 97, 110, 32, 76, 46]\n","I forgot my wallet at home, that’s an L.\n","--------------------------------------------------------------------------------\n","Your tweet got 5 likes and 100 replies calling you out. L + ratio.\n","[89, 111, 117, 114, 258, 119, 101, 101, 257, 103, 111, 257, 53, 32, 108, 105, 107, 101, 115, 32, 97, 110, 100, 32, 49, 48, 48, 32, 114, 101, 112, 108, 105, 101, 115, 32, 99, 97, 108, 108, 105, 110, 103, 32, 121, 111, 117, 32, 111, 117, 116, 46, 32, 76, 32, 43, 32, 114, 97, 116, 105, 111, 46]\n","Your tweet got 5 likes and 100 replies calling you out. L + ratio.\n","--------------------------------------------------------------------------------\n","That meme is so dank!\n","[84, 104, 97, 257, 109, 101, 109, 256, 105, 115, 32, 115, 111, 32, 100, 97, 110, 107, 33]\n","That meme is so dank!\n","--------------------------------------------------------------------------------\n","That phrase is so cheugy, no one says that anymore.\n","[84, 104, 97, 257, 112, 104, 114, 97, 115, 256, 105, 115, 32, 115, 111, 32, 99, 104, 101, 117, 103, 121, 44, 32, 110, 111, 32, 111, 110, 256, 115, 97, 121, 115, 258, 104, 97, 257, 97, 110, 121, 109, 111, 114, 101, 46]\n","That phrase is so cheugy, no one says that anymore.\n","--------------------------------------------------------------------------------\n","TFW you finish a big project and can finally relax\n","[84, 70, 87, 32, 121, 111, 117, 32, 102, 105, 110, 105, 115, 104, 32, 97, 32, 98, 105, 103, 32, 112, 114, 111, 106, 101, 99, 257, 97, 110, 100, 32, 99, 97, 110, 32, 102, 105, 110, 97, 108, 108, 121, 32, 114, 101, 108, 97, 120]\n","TFW you finish a big project and can finally relax\n","--------------------------------------------------------------------------------\n","He’s really woke about climate change.\n","[72, 101, 226, 128, 153, 115, 32, 114, 101, 97, 108, 108, 121, 32, 119, 111, 107, 256, 97, 98, 111, 117, 257, 99, 108, 105, 109, 97, 116, 256, 99, 104, 97, 110, 103, 101, 46]\n","He’s really woke about climate change.\n","--------------------------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Minimal (byte-level) Byte Pair Encoding tokenizer.\n","\n","Algorithmically follows along the GPT tokenizer:\n","https://github.com/openai/gpt-2/blob/master/src/encoder.py\n","\n","Unlike BasicTokenizer:\n","- RegexTokenizer handles an optional regex splitting pattern.\n","- RegexTokenizer handles optional special tokens.\n","\"\"\"\n","\n","import regex as re\n","\n","\n","# the main GPT text split patterns, see\n","# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n","GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n","GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n","\n","\n","class RegexTokenizer():\n","    def __init__(self, pattern=None):\n","        \"\"\"\n","        - pattern: optional string to override the default (GPT-4 split pattern)\n","        - special_tokens: str -> int dictionary of special tokens\n","          example: {'<|endoftext|>': 100257}\n","        \"\"\"\n","        self.merges = {} # (int, int) -> int\n","        self.pattern = \"\" # str\n","        self.special_tokens = {} # str -> int, e.g. {'<|endoftext|>': 100257}\n","        self.vocab = self._build_vocab() # int -> bytes\n","        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern\n","        self.compiled_pattern = re.compile(self.pattern)\n","        self.special_tokens = {}\n","        self.inverse_special_tokens = {}\n","\n","    def _build_vocab(self):\n","        # vocab is simply and deterministically derived from merges\n","        vocab = {idx: bytes([idx]) for idx in range(256)}\n","        for (p0, p1), idx in self.merges.items():\n","            vocab[idx] = vocab[p0] + vocab[p1]\n","        for special, idx in self.special_tokens.items():\n","            vocab[idx] = special.encode(\"utf-8\")\n","        return vocab\n","\n","    def _get_stats(self, ids, counts=None):\n","        \"\"\"\n","        Given a list of integers, return a dictionary of counts of consecutive pairs\n","        Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n","        Optionally allows to update an existing dictionary of counts\n","        \"\"\"\n","        counts = {} if counts is None else counts\n","        for pair in zip(ids, ids[1:]): # iterate consecutive elements\n","            counts[pair] = counts.get(pair, 0) + 1\n","        return counts\n","\n","    def _merge(self, ids, pair, idx):\n","        \"\"\"\n","        In the list of integers (ids), replace all consecutive occurrences\n","        of pair with the new integer token idx\n","        Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n","        \"\"\"\n","        newids = []\n","        i = 0\n","        while i < len(ids):\n","            # if not at the very last position AND the pair matches, replace it\n","            if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n","                newids.append(idx)\n","                i += 2\n","            else:\n","                newids.append(ids[i])\n","                i += 1\n","        return newids\n","\n","    def train(self, text, vocab_size, verbose=False):\n","        assert vocab_size >= 256\n","        num_merges = vocab_size - 256\n","\n","        # split the text up into text chunks\n","        text_chunks = re.findall(self.compiled_pattern, text)\n","\n","        # input text preprocessing\n","        ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks]\n","\n","        # iteratively merge the most common pairs to create new tokens\n","        merges = {} # (int, int) -> int\n","        vocab = {idx: bytes([idx]) for idx in range(256)} # idx -> bytes\n","        for i in range(num_merges):\n","            # count the number of times every consecutive pair appears\n","            stats = {}\n","            for chunk_ids in ids:\n","                # passing in stats will update it in place, adding up counts\n","                self._get_stats(chunk_ids, stats)\n","            # find the pair with the highest count\n","            pair = max(stats, key=stats.get)\n","            # mint a new token: assign it the next available id\n","            idx = 256 + i\n","            # replace all occurrences of pair in ids with idx\n","            ids = [self._merge(chunk_ids, pair, idx) for chunk_ids in ids]\n","            # save the merge\n","            merges[pair] = idx\n","            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n","            # prints\n","            if verbose:\n","                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n","\n","        # save class variables\n","        self.merges = merges # used in encode()\n","        self.vocab = vocab   # used in decode()\n","\n","    def register_special_tokens(self, special_tokens):\n","        # special_tokens is a dictionary of str -> int\n","        # example: {\"<|endoftext|>\": 100257}\n","        self.special_tokens = special_tokens\n","        self.inverse_special_tokens = {v: k for k, v in special_tokens.items()}\n","\n","    def decode(self, ids):\n","        # given ids (list of integers), return Python string\n","        part_bytes = []\n","        for idx in ids:\n","            if idx in self.vocab:\n","                part_bytes.append(self.vocab[idx])\n","            elif idx in self.inverse_special_tokens:\n","                part_bytes.append(self.inverse_special_tokens[idx].encode(\"utf-8\"))\n","            else:\n","                raise ValueError(f\"invalid token id: {idx}\")\n","        text_bytes = b\"\".join(part_bytes)\n","        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n","        return text\n","\n","    def _encode_chunk(self, text_bytes):\n","        # return the token ids\n","        # let's begin. first, convert all bytes to integers in range 0..255\n","        ids = list(text_bytes)\n","        while len(ids) >= 2:\n","            # find the pair with the lowest merge index\n","            stats = self._get_stats(ids)\n","            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n","            # subtle: if there are no more merges available, the key will\n","            # result in an inf for every single pair, and the min will be\n","            # just the first pair in the list, arbitrarily\n","            # we can detect this terminating case by a membership check\n","            if pair not in self.merges:\n","                break # nothing else can be merged anymore\n","            # otherwise let's merge the best pair (lowest merge index)\n","            idx = self.merges[pair]\n","            ids = self._merge(ids, pair, idx)\n","        return ids\n","\n","    def encode_ordinary(self, text):\n","        \"\"\"Encoding that ignores any special tokens.\"\"\"\n","        # split text into chunks of text by categories defined in regex pattern\n","        text_chunks = re.findall(self.compiled_pattern, text)\n","        # all chunks of text are encoded separately, then results are joined\n","        ids = []\n","        for chunk in text_chunks:\n","            chunk_bytes = chunk.encode(\"utf-8\") # raw bytes\n","            chunk_ids = self._encode_chunk(chunk_bytes)\n","            ids.extend(chunk_ids)\n","        return ids\n","\n","    def encode(self, text, allowed_special=\"none_raise\"):\n","        \"\"\"\n","        Unlike encode_ordinary, this function handles special tokens.\n","        allowed_special: can be \"all\"|\"none\"|\"none_raise\" or a custom set of special tokens\n","        if none_raise, then an error is raised if any special token is encountered in text\n","        this is the default tiktoken behavior right now as well\n","        any other behavior is either annoying, or a major footgun\n","        \"\"\"\n","        # decode the user desire w.r.t. handling of special tokens\n","        special = None\n","        if allowed_special == \"all\":\n","            special = self.special_tokens\n","        elif allowed_special == \"none\":\n","            special = {}\n","        elif allowed_special == \"none_raise\":\n","            special = {}\n","            assert all(token not in text for token in self.special_tokens)\n","        elif isinstance(allowed_special, set):\n","            special = {k: v for k, v in self.special_tokens.items() if k in allowed_special}\n","        else:\n","            raise ValueError(f\"allowed_special={allowed_special} not understood\")\n","        if not special:\n","            # shortcut: if no special tokens, just use the ordinary encoding\n","            return self.encode_ordinary(text)\n","        # otherwise, we have to be careful with potential special tokens in text\n","        # we handle special tokens by splitting the text\n","        # based on the occurrence of any exact match with any of the special tokens\n","        # we can use re.split for this. note that surrounding the pattern with ()\n","        # makes it into a capturing group, so the special tokens will be included\n","        special_pattern = \"(\" + \"|\".join(re.escape(k) for k in special) + \")\"\n","        special_chunks = re.split(special_pattern, text)\n","        # now all the special characters are separated from the rest of the text\n","        # all chunks of text are encoded separately, then results are joined\n","        ids = []\n","        for part in special_chunks:\n","            if part in special:\n","                # this is a special token, encode it separately as a special case\n","                ids.append(special[part])\n","            else:\n","                # this is an ordinary sequence, encode it normally\n","                ids.extend(self.encode_ordinary(part))\n","        return ids\n","\n","    def save_vocab(self, vocab_file):\n","        # just for visualization purposes let's output the tokens\n","        # in the exact same format as the base class would.\n","\n","        # build vocab being mindful of the byte shuffle\n","        vocab = {idx: bytes([self.inverse_byte_shuffle[idx]]) for idx in range(256)}\n","\n","        for (p0, p1), idx in self.merges.items():\n","            vocab[idx] = vocab[p0] + vocab[p1]\n","\n","        # now merge the shuffled bytes and write to file\n","        inverted_merges = {idx: pair for pair, idx in self.merges.items()}\n","\n","        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n","            for idx, token in vocab.items():\n","                s = render_token(token)\n","\n","                if idx in inverted_merges:\n","                    idx0, idx1 = inverted_merges[idx]\n","                    s0 = render_token(vocab[idx0])\n","                    s1 = render_token(vocab[idx1])\n","\n","                    f.write(f\"[{s0}][{s1}] -> [{s}] {idx}\\n\")\n","                else:\n","                    f.write(f\"[{s}] {idx}\\n\")"],"metadata":{"id":"iwuSysCmCJRB","executionInfo":{"status":"ok","timestamp":1768577866020,"user_tz":300,"elapsed":24,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["\n","\n","zoomer_text = get_file_str('zoomer.txt')\n","zoomer_lines = get_lines_str('zoomer.txt')\n","\n","regex_tokenizer = RegexTokenizer()\n","regex_tokenizer.train(zoomer_text, 256 + 3)\n","\n","for i in range(0, 7):\n","  print(zoomer_lines[i])\n","  tokenized_str = regex_tokenizer.encode(zoomer_lines[i])\n","  print(tokenized_str)\n","  print(regex_tokenizer.decode(tokenized_str))\n","  print('-' * 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fV0k7aYjckGV","executionInfo":{"status":"ok","timestamp":1768577869995,"user_tz":300,"elapsed":104,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"b5b38a40-0b8c-44f3-d4fa-b79cc7279339"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Got the job today, big W!\n","[71, 111, 116, 256, 104, 101, 32, 106, 111, 98, 256, 111, 100, 97, 121, 44, 32, 98, 105, 103, 32, 87, 33]\n","Got the job today, big W!\n","--------------------------------------------------------------------------------\n","I forgot my wallet at home, that’s an L.\n","[73, 32, 102, 111, 114, 103, 111, 116, 32, 109, 121, 32, 119, 97, 108, 108, 101, 116, 32, 97, 116, 32, 104, 111, 109, 101, 44, 256, 104, 97, 116, 258, 115, 32, 97, 110, 32, 76, 46]\n","I forgot my wallet at home, that’s an L.\n","--------------------------------------------------------------------------------\n","Your tweet got 5 likes and 100 replies calling you out. L + ratio.\n","[89, 111, 117, 114, 256, 119, 101, 101, 116, 32, 103, 111, 116, 32, 53, 32, 108, 105, 107, 101, 115, 32, 97, 110, 100, 32, 49, 48, 48, 32, 114, 101, 112, 108, 105, 101, 115, 32, 99, 97, 108, 108, 105, 110, 103, 32, 121, 111, 117, 32, 111, 117, 116, 46, 32, 76, 32, 43, 32, 114, 97, 116, 105, 111, 46]\n","Your tweet got 5 likes and 100 replies calling you out. L + ratio.\n","--------------------------------------------------------------------------------\n","That meme is so dank!\n","[84, 104, 97, 116, 32, 109, 101, 109, 101, 32, 105, 115, 32, 115, 111, 32, 100, 97, 110, 107, 33]\n","That meme is so dank!\n","--------------------------------------------------------------------------------\n","That phrase is so cheugy, no one says that anymore.\n","[84, 104, 97, 116, 32, 112, 104, 114, 97, 115, 101, 32, 105, 115, 32, 115, 111, 32, 99, 104, 101, 117, 103, 121, 44, 32, 110, 111, 32, 111, 110, 101, 32, 115, 97, 121, 115, 256, 104, 97, 116, 32, 97, 110, 121, 109, 111, 114, 101, 46]\n","That phrase is so cheugy, no one says that anymore.\n","--------------------------------------------------------------------------------\n","TFW you finish a big project and can finally relax\n","[84, 70, 87, 32, 121, 111, 117, 32, 102, 105, 110, 105, 115, 104, 32, 97, 32, 98, 105, 103, 32, 112, 114, 111, 106, 101, 99, 116, 32, 97, 110, 100, 32, 99, 97, 110, 32, 102, 105, 110, 97, 108, 108, 121, 32, 114, 101, 108, 97, 120]\n","TFW you finish a big project and can finally relax\n","--------------------------------------------------------------------------------\n","He’s really woke about climate change.\n","[72, 101, 258, 115, 32, 114, 101, 97, 108, 108, 121, 32, 119, 111, 107, 101, 32, 97, 98, 111, 117, 116, 32, 99, 108, 105, 109, 97, 116, 101, 32, 99, 104, 97, 110, 103, 101, 46]\n","He’s really woke about climate change.\n","--------------------------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["## Export to HTML\n","\n","Uncomment the final line of the cell below and run it to export this notebook to HTML"],"metadata":{"id":"EhL2imFMtt7w"}},{"cell_type":"code","source":["import os, json\n","\n","def export_notebook():\n","  L_PATH = \"nd-cse-10124-lectures/Notebooks\"\n","  L = \"Lecture_03_Tokenization\"\n","\n","  try:\n","      from google.colab import _message, files\n","\n","      # where you WANT it to live (repo folder)\n","      repo_ipynb_path = f\"/content/{L_PATH}/{L}.ipynb\"\n","\n","      # grab current notebook contents from the UI\n","      nb = _message.blocking_request(\"get_ipynb\", timeout_sec=1)[\"ipynb\"]\n","\n","      # write it into the repo folder as a real file\n","      os.makedirs(os.path.dirname(repo_ipynb_path), exist_ok=True)\n","      with open(repo_ipynb_path, \"w\", encoding=\"utf-8\") as f:\n","          json.dump(nb, f)\n","\n","      # convert + download pdf\n","      !jupyter nbconvert --to html \"{repo_ipynb_path}\"\n","      files.download(repo_ipynb_path.replace(\".ipynb\", \".html\"))\n","  except:\n","      import subprocess\n","\n","      nb_fp = os.getcwd() + f'{L}.ipynb'\n","      print(os.getcwd())\n","\n","      subprocess.run([\"jupyter\", \"nbconvert\", \"--to\", \"html\", nb_fp], check=True)\n","\n","#export_notebook()"],"metadata":{"id":"WxqSmD0Htui7","executionInfo":{"status":"aborted","timestamp":1768570916194,"user_tz":300,"elapsed":2430,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install rustbpe\n","\n","# -----------------------------------------------------------------------------\n","# Tokenizer based on rustbpe + tiktoken combo\n","import os\n","import copy\n","import pickle\n","import rustbpe\n","import tiktoken\n","\n","from functools import lru_cache\n","\n","SPECIAL_TOKENS = [\n","    \"<|bos|>\", # every document begins with the Beginning of Sequence (BOS) token that delimits documents\n","    # tokens below are only used during finetuning to render Conversations into token ids\n","    \"<|user_start|>\", # user messages\n","    \"<|user_end|>\",\n","    \"<|assistant_start|>\", # assistant messages\n","    \"<|assistant_end|>\",\n","    \"<|python_start|>\", # assistant invokes python REPL tool\n","    \"<|python_end|>\",\n","    \"<|output_start|>\", # python REPL outputs back to assistant\n","    \"<|output_end|>\",\n","]\n","\n","SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,2}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n","\n","class RustBPETokenizer:\n","    def __init__(self, enc, bos_token):\n","        self.enc = enc\n","        self.bos_token_id = self.encode_special(bos_token)\n","\n","    @classmethod\n","    def train_from_iterator(cls, text_iterator, vocab_size):\n","        # 1) train using rustbpe\n","        tokenizer = rustbpe.Tokenizer()\n","\n","        # the special tokens are inserted later in __init__, we don't train them here\n","        vocab_size_no_special = vocab_size - len(SPECIAL_TOKENS)\n","        assert vocab_size_no_special >= 256, f\"vocab_size_no_special must be at least 256, got {vocab_size_no_special}\"\n","        tokenizer.train_from_iterator(text_iterator, vocab_size_no_special, pattern=SPLIT_PATTERN)\n","\n","        # 2) construct the associated tiktoken encoding for inference\n","        pattern = tokenizer.get_pattern()\n","        mergeable_ranks_list = tokenizer.get_mergeable_ranks()\n","        mergeable_ranks = {bytes(k): v for k, v in mergeable_ranks_list}\n","        tokens_offset = len(mergeable_ranks)\n","        special_tokens = {name: tokens_offset + i for i, name in enumerate(SPECIAL_TOKENS)}\n","\n","        enc = tiktoken.Encoding(name=\"rustbpe\", pat_str=pattern, mergeable_ranks=mergeable_ranks, special_tokens=special_tokens)\n","\n","        return cls(enc, \"<|bos|>\")\n","\n","    # def get_vocab_size(self):\n","    #     return self.enc.n_vocab\n","\n","    # def get_special_tokens(self):\n","    #     return self.enc.special_tokens_set\n","\n","    # def id_to_token(self, id):\n","    #     return self.enc.decode([id])\n","\n","    def encode_special(self, text):\n","        return self.enc.encode_single_token(text)\n","\n","    def get_bos_token_id(self):\n","        return self.bos_token_id\n","\n","    def encode(self, text, num_threads=8):\n","        ids = self.enc.encode_ordinary(text)\n","\n","        return ids\n","\n","    def decode(self, ids):\n","        return self.enc.decode(ids)\n","\n","    def render_conversation(self, conversation, max_tokens=2048):\n","        \"\"\"\n","        Tokenize a single Chat conversation (which we call a \"doc\" or \"document\" here).\n","        Returns:\n","        - ids: list[int] is a list of token ids of this rendered conversation\n","        - mask: list[int] of same length, mask = 1 for tokens that the Assistant is expected to train on.\n","        \"\"\"\n","        # ids, masks that we will return and a helper function to help build them up.\n","        ids, mask = [], []\n","        def add_tokens(token_ids, mask_val):\n","            if isinstance(token_ids, int):\n","                token_ids = [token_ids]\n","            ids.extend(token_ids)\n","            mask.extend([mask_val] * len(token_ids))\n","\n","        messages = conversation[\"messages\"]\n","\n","        # fetch all the special tokens we need\n","        bos = self.get_bos_token_id()\n","        user_start, user_end = self.encode_special(\"<|user_start|>\"), self.encode_special(\"<|user_end|>\")\n","        assistant_start, assistant_end = self.encode_special(\"<|assistant_start|>\"), self.encode_special(\"<|assistant_end|>\")\n","        python_start, python_end = self.encode_special(\"<|python_start|>\"), self.encode_special(\"<|python_end|>\")\n","        output_start, output_end = self.encode_special(\"<|output_start|>\"), self.encode_special(\"<|output_end|>\")\n","\n","        # now we can tokenize the conversation\n","        add_tokens(bos, 0)\n","        for i, message in enumerate(messages):\n","            content = message[\"content\"]\n","\n","            if message[\"role\"] == \"user\":\n","                value_ids = self.encode(content)\n","                add_tokens(user_start, 0)\n","                add_tokens(value_ids, 0)\n","                add_tokens(user_end, 0)\n","            elif message[\"role\"] == \"assistant\":\n","                add_tokens(assistant_start, 0)\n","                # simple string => simply add the tokens\n","                value_ids = self.encode(content)\n","                add_tokens(value_ids, 1)\n","\n","                add_tokens(assistant_end, 1)\n","\n","        # truncate to max_tokens tokens MAX (helps prevent OOMs)\n","        ids = ids[:max_tokens]\n","        mask = mask[:max_tokens]\n","        return ids, mask\n","\n","    def visualize_tokenization(self, ids, mask, with_token_id=False):\n","        \"\"\"Small helper function useful in debugging: visualize the tokenization of render_conversation\"\"\"\n","        RED = '\\033[91m'\n","        GREEN = '\\033[92m'\n","        RESET = '\\033[0m'\n","        GRAY = '\\033[90m'\n","        tokens = []\n","        for i, (token_id, mask_val) in enumerate(zip(ids, mask)):\n","            token_str = self.decode([token_id])\n","            color = GREEN if mask_val == 1 else RED\n","            tokens.append(f\"{color}{token_str}{RESET}\")\n","            if with_token_id:\n","                tokens.append(f\"{GRAY}({token_id}){RESET}\")\n","        return '|'.join(tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nPVeFvJzd7d5","executionInfo":{"status":"ok","timestamp":1768579905172,"user_tz":300,"elapsed":5107,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"6a0eebe1-0212-4c52-ebaa-d4bb23ccdddc"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: rustbpe in /usr/local/lib/python3.12/dist-packages (0.1.0)\n"]}]},{"cell_type":"code","source":["zoomer_text = get_file_str('zoomer.txt')\n","\n","nanochat_tokenizer = RustBPETokenizer.train_from_iterator(zoomer_text, 256 + len(SPECIAL_TOKENS))\n","\n","conversation = {\n","    'messages': [\n","        {'role': 'user', 'content': 'Hello there general kenobi'},\n","        {'role': 'assistant', 'content': 'Hello there grevious'},\n","        {'role': 'user', 'content': 'I love neffer'},\n","    ]\n","}\n","\n","ids, masks = nanochat_tokenizer.render_conversation(conversation)\n","print(nanochat_tokenizer.visualize_tokenization(ids, masks, with_token_id=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oO7H8-XJfg84","executionInfo":{"status":"ok","timestamp":1768579954745,"user_tz":300,"elapsed":87,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"c8cf4426-f8d3-4157-f669-281102776ebc"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[91m<|bos|>\u001b[0m|\u001b[90m(256)\u001b[0m|\u001b[91m<|user_start|>\u001b[0m|\u001b[90m(257)\u001b[0m|\u001b[91mH\u001b[0m|\u001b[90m(72)\u001b[0m|\u001b[91me\u001b[0m|\u001b[90m(101)\u001b[0m|\u001b[91ml\u001b[0m|\u001b[90m(108)\u001b[0m|\u001b[91ml\u001b[0m|\u001b[90m(108)\u001b[0m|\u001b[91mo\u001b[0m|\u001b[90m(111)\u001b[0m|\u001b[91m \u001b[0m|\u001b[90m(32)\u001b[0m|\u001b[91mt\u001b[0m|\u001b[90m(116)\u001b[0m|\u001b[91mh\u001b[0m|\u001b[90m(104)\u001b[0m|\u001b[91me\u001b[0m|\u001b[90m(101)\u001b[0m|\u001b[91mr\u001b[0m|\u001b[90m(114)\u001b[0m|\u001b[91me\u001b[0m|\u001b[90m(101)\u001b[0m|\u001b[91m \u001b[0m|\u001b[90m(32)\u001b[0m|\u001b[91mg\u001b[0m|\u001b[90m(103)\u001b[0m|\u001b[91me\u001b[0m|\u001b[90m(101)\u001b[0m|\u001b[91mn\u001b[0m|\u001b[90m(110)\u001b[0m|\u001b[91me\u001b[0m|\u001b[90m(101)\u001b[0m|\u001b[91mr\u001b[0m|\u001b[90m(114)\u001b[0m|\u001b[91ma\u001b[0m|\u001b[90m(97)\u001b[0m|\u001b[91ml\u001b[0m|\u001b[90m(108)\u001b[0m|\u001b[91m \u001b[0m|\u001b[90m(32)\u001b[0m|\u001b[91mk\u001b[0m|\u001b[90m(107)\u001b[0m|\u001b[91me\u001b[0m|\u001b[90m(101)\u001b[0m|\u001b[91mn\u001b[0m|\u001b[90m(110)\u001b[0m|\u001b[91mo\u001b[0m|\u001b[90m(111)\u001b[0m|\u001b[91mb\u001b[0m|\u001b[90m(98)\u001b[0m|\u001b[91mi\u001b[0m|\u001b[90m(105)\u001b[0m|\u001b[91m<|user_end|>\u001b[0m|\u001b[90m(258)\u001b[0m|\u001b[91m<|assistant_start|>\u001b[0m|\u001b[90m(259)\u001b[0m|\u001b[92mH\u001b[0m|\u001b[90m(72)\u001b[0m|\u001b[92me\u001b[0m|\u001b[90m(101)\u001b[0m|\u001b[92ml\u001b[0m|\u001b[90m(108)\u001b[0m|\u001b[92ml\u001b[0m|\u001b[90m(108)\u001b[0m|\u001b[92mo\u001b[0m|\u001b[90m(111)\u001b[0m|\u001b[92m \u001b[0m|\u001b[90m(32)\u001b[0m|\u001b[92mt\u001b[0m|\u001b[90m(116)\u001b[0m|\u001b[92mh\u001b[0m|\u001b[90m(104)\u001b[0m|\u001b[92me\u001b[0m|\u001b[90m(101)\u001b[0m|\u001b[92mr\u001b[0m|\u001b[90m(114)\u001b[0m|\u001b[92me\u001b[0m|\u001b[90m(101)\u001b[0m|\u001b[92m \u001b[0m|\u001b[90m(32)\u001b[0m|\u001b[92mg\u001b[0m|\u001b[90m(103)\u001b[0m|\u001b[92mr\u001b[0m|\u001b[90m(114)\u001b[0m|\u001b[92me\u001b[0m|\u001b[90m(101)\u001b[0m|\u001b[92mv\u001b[0m|\u001b[90m(118)\u001b[0m|\u001b[92mi\u001b[0m|\u001b[90m(105)\u001b[0m|\u001b[92mo\u001b[0m|\u001b[90m(111)\u001b[0m|\u001b[92mu\u001b[0m|\u001b[90m(117)\u001b[0m|\u001b[92ms\u001b[0m|\u001b[90m(115)\u001b[0m|\u001b[92m<|assistant_end|>\u001b[0m|\u001b[90m(260)\u001b[0m|\u001b[91m<|user_start|>\u001b[0m|\u001b[90m(257)\u001b[0m|\u001b[91mI\u001b[0m|\u001b[90m(73)\u001b[0m|\u001b[91m \u001b[0m|\u001b[90m(32)\u001b[0m|\u001b[91ml\u001b[0m|\u001b[90m(108)\u001b[0m|\u001b[91mo\u001b[0m|\u001b[90m(111)\u001b[0m|\u001b[91mv\u001b[0m|\u001b[90m(118)\u001b[0m|\u001b[91me\u001b[0m|\u001b[90m(101)\u001b[0m|\u001b[91m \u001b[0m|\u001b[90m(32)\u001b[0m|\u001b[91mn\u001b[0m|\u001b[90m(110)\u001b[0m|\u001b[91me\u001b[0m|\u001b[90m(101)\u001b[0m|\u001b[91mf\u001b[0m|\u001b[90m(102)\u001b[0m|\u001b[91mf\u001b[0m|\u001b[90m(102)\u001b[0m|\u001b[91me\u001b[0m|\u001b[90m(101)\u001b[0m|\u001b[91mr\u001b[0m|\u001b[90m(114)\u001b[0m|\u001b[91m<|user_end|>\u001b[0m|\u001b[90m(258)\u001b[0m\n"]}]}]}