{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPM/IqjR+eZ7J+Xi8NEy1yJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-001.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-002.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"EUZY_VNVJVKT"}},{"cell_type":"code","source":["l = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n","\n","for i in l:\n","  print(i + 5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Sj_A8jo8vbE","executionInfo":{"status":"ok","timestamp":1768570913861,"user_tz":300,"elapsed":62,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"f34aa801-a847-41b8-a102-de690e18a1fc"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n"]}]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-003.png\" class=\"img-responsive\"/>\n","</div>\n","\n"],"metadata":{"id":"mxAAF5GY8vxw"}},{"cell_type":"code","source":["#print(5 + '5')"],"metadata":{"id":"58YQQupe_7UK","executionInfo":{"status":"ok","timestamp":1768570913863,"user_tz":300,"elapsed":2,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-004.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"mtQM5cbm_6f7"}},{"cell_type":"code","source":["combined = 5 + int('5')\n","print(type(combined))\n","print(combined)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BLGTx_tM-IAn","executionInfo":{"status":"ok","timestamp":1768572010336,"user_tz":300,"elapsed":6,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"44ded49a-0b16-4791-9089-4502b19b84ed"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'int'>\n","10\n"]}]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-005.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"0jUdJ9FmIsfB"}},{"cell_type":"code","source":["def count_consecutive_pairs(ids, counts=None):\n","\n","    # TODO\n","\n","    return counts"],"metadata":{"id":"MoxgX40-IuO7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-006.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"5spfu41pJHGy"}},{"cell_type":"code","source":["import os\n","\n","try:\n","    import google.colab\n","    REPO_URL = \"https://github.com/wtheisen/nd-cse-10124-lectures.git\"\n","\n","    REPO_NAME = \"/content/nd-cse-10124-lectures\"\n","    L_PATH = \"nd-cse-10124-lectures/Datasets\"\n","\n","    %cd /content/\n","    !rm -r {REPO_NAME}\n","\n","    # Clone repo\n","    if not os.path.exists(REPO_NAME):\n","        !git clone {REPO_URL}\n","\n","        # cd into the data folder\n","        %cd {L_PATH}\n","        !pwd\n","\n","except ImportError:\n","    print(\"Unable to download repo, either:\")\n","    print(\"\\tA.) You're not on colab\")\n","    print(\"\\tB.) It has already been cloned\")\n","\n","\n","import utilities as uts"],"metadata":{"id":"XauIb-liOtSM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768581029999,"user_tz":300,"elapsed":1735,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"d9a7b18c-f1b8-4fb5-ce8d-9cf2c45b2a61"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","Cloning into 'nd-cse-10124-lectures'...\n","remote: Enumerating objects: 63, done.\u001b[K\n","remote: Counting objects: 100% (63/63), done.\u001b[K\n","remote: Compressing objects: 100% (52/52), done.\u001b[K\n","remote: Total 63 (delta 15), reused 57 (delta 9), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (63/63), 16.60 MiB | 23.03 MiB/s, done.\n","Resolving deltas: 100% (15/15), done.\n","/content/nd-cse-10124-lectures/Datasets\n","/content/nd-cse-10124-lectures/Datasets\n"]}]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-007.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-008.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-009.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-010.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-011.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-012.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-013.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-014.png\" class=\"img-responsive\"/>\n","</div>\n","\n"],"metadata":{"id":"Jj4xnpaJqXaK"}},{"cell_type":"code","source":["class Cat():\n","    def __init__(self, color):\n","        self.color = color\n","\n","    # TODO"],"metadata":{"id":"dv3sQu75KJyT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-015.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"Vz8qWn5aq9m-"}},{"cell_type":"code","source":["class BasicTokenizer():\n","    def __init__(self):\n","        pass"],"metadata":{"id":"UJ2jKdJ0o-BO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-016.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"hIgpsgViq_tP"}},{"cell_type":"code","source":["\"\"\"\n","Minimal (byte-level) Byte Pair Encoding tokenizer.\n","\n","Algorithmically follows along the GPT tokenizer:\n","https://github.com/openai/gpt-2/blob/master/src/encoder.py\n","\"\"\"\n","\n","class BPE_Tokenizer():\n","    def __init__(self):\n","        self.merges = {} # (int, int) -> int\n","        self.vocab = self._build_vocab() # int -> bytes\n","\n","    def _build_vocab(self):\n","        # vocab is simply and deterministically derived from merges\n","        vocab = {idx: bytes([idx]) for idx in range(256)}\n","\n","        for (p0, p1), idx in self.merges.items():\n","            vocab[idx] = vocab[p0] + vocab[p1]\n","\n","        return vocab\n","\n","    def _get_stats(self, ids, counts=None):\n","        \"\"\"\n","        Given a list of integers, return a dictionary of counts of consecutive pairs\n","        Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n","        Optionally allows to update an existing dictionary of counts\n","        \"\"\"\n","        counts = {} if counts is None else counts\n","\n","        for pair in zip(ids, ids[1:]): # iterate consecutive elements\n","            counts[pair] = counts.get(pair, 0) + 1\n","\n","        return counts\n","\n","    def _merge(self, ids, pair, idx):\n","        \"\"\"\n","        In the list of integers (ids), replace all consecutive occurrences\n","        of pair with the new integer token idx\n","        Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n","        \"\"\"\n","        newids = []\n","        i = 0\n","\n","        while i < len(ids):\n","            # if not at the very last position AND the pair matches, replace it\n","            if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n","                newids.append(idx)\n","                i += 2\n","            else:\n","                newids.append(ids[i])\n","                i += 1\n","\n","        return newids\n","\n","    def train(self, text, vocab_size, verbose=False):\n","        num_merges = vocab_size - 256\n","\n","        # input text preprocessing\n","        text_bytes = text.encode(\"utf-8\") # raw bytes\n","        ids = list(text_bytes) # list of integers in range 0..255\n","\n","        # iteratively merge the most common pairs to create new tokens\n","        merges = {} # (int, int) -> int\n","        vocab = {idx: bytes([idx]) for idx in range(256)} # int -> bytes\n","\n","        for i in range(num_merges):\n","            # count up the number of times every consecutive pair appears\n","            stats = self._get_stats(ids)\n","\n","            # find the pair with the highest count\n","            pair = max(stats, key=stats.get)\n","\n","            # mint a new token: assign it the next available id\n","            idx = 258 + i\n","\n","            # replace all occurrences of pair in ids with idx\n","            ids = self._merge(ids, pair, idx)\n","\n","            # save the merge\n","            merges[pair] = idx\n","            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n","\n","        # save class variables\n","        print(merges)\n","        self.merges = merges # used in encode()\n","        self.vocab = vocab   # used in decode()\n","\n","    def decode(self, ids):\n","        # given ids (list of integers), return Python string\n","        text_bytes = b\"\".join(self.vocab[idx] for idx in ids)\n","        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n","        return text\n","\n","    def encode(self, text):\n","        # given a string text, return the token ids\n","        text_bytes = text.encode(\"utf-8\") # raw bytes\n","        ids = list(text_bytes) # list of integers in range 0..255\n","        print('Raw Tokens:', ids)\n","\n","        while len(ids) >= 2:\n","            # find the pair with the lowest merge index\n","            stats = self._get_stats(ids)\n","            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n","\n","            # just the first pair in the list, arbitrarily\n","            # we can detect this terminating case by a membership check\n","            if pair not in self.merges:\n","                break # nothing else can be merged anymore\n","\n","            # otherwise let's merge the best pair (lowest merge index)\n","            idx = self.merges[pair]\n","            ids = self._merge(ids, pair, idx)\n","\n","        print('Merged Tokens:', ids)\n","        return ids\n","\n","    def visualize_tokenization(self, ids):\n","        \"\"\"Small helper function useful in debugging: visualize the tokenization of render_conversation\"\"\"\n","        RED = '\\033[91m'\n","        GREEN = '\\033[92m'\n","        RESET = '\\033[0m'\n","        GRAY = '\\033[90m'\n","\n","        tokens = []\n","        for token_id in ids:\n","            token_str = self.decode([token_id])\n","            tokens.append(f\"{GREEN}{token_str}{GRAY}({token_id}){RESET}\")\n","\n","            if token_str in ['<|SOS|>', '<|EOS|>']:\n","                tokens.append('\\n\\n\\t')\n","\n","        return ' | '.join(tokens)"],"metadata":{"id":"FMNlxMh4B7Eu","executionInfo":{"status":"ok","timestamp":1768586579876,"user_tz":300,"elapsed":24,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}}},"execution_count":74,"outputs":[]},{"cell_type":"code","source":["training_str = uts.get_file_str('zoomer.txt')\n","testing_lines = uts.get_lines_str('zoomer.txt')\n","\n","tokenizer = BPE_Tokenizer()\n","tokenizer.train(training_str, 512)\n","\n","print(len(tokenizer.vocab))\n","print(tokenizer.vocab)\n","\n","for i in range(0, 3):\n","    print(testing_lines[i])\n","\n","    tokenized_str = tokenizer.encode(testing_lines[i].lower())\n","    print(tokenizer.visualize_tokenization(tokenized_str))\n","\n","    print('-' * 80)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SQw48jfcDSL6","executionInfo":{"status":"ok","timestamp":1768586586067,"user_tz":300,"elapsed":4651,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"8cdc5964-b781-40f5-9ca1-3942420eb3b1"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["{(101, 32): 258, (116, 32): 259, (32, 116): 260, (44, 32): 261, (115, 32): 262, (226, 128): 263, (263, 153): 264, (105, 110): 265, (111, 117): 266, (104, 97): 267, (104, 258): 268, (265, 103): 269, (32, 97): 270, (101, 114): 271, (111, 114): 272, (111, 110): 273, (111, 32): 274, (260, 268): 275, (267, 259): 276, (121, 266): 277, (121, 32): 278, (101, 101): 279, (100, 32): 280, (108, 108): 281, (97, 110): 282, (73, 32): 283, (73, 264): 284, (264, 262): 285, (269, 32): 286, (105, 262): 287, (101, 110): 288, (119, 97): 289, (102, 272): 290, (101, 97): 291, (105, 116): 292, (259, 116): 293, (46, 84): 294, (109, 258): 295, (108, 97): 296, (103, 104): 297, (109, 32): 298, (101, 115): 299, (289, 262): 300, (32, 115): 301, (111, 119): 302, (281, 32): 303, (89, 266): 304, (101, 100): 305, (33, 84): 306, (103, 111): 307, (105, 115): 308, (284, 298): 309, (260, 111): 310, (105, 297): 311, (260, 274): 312, (118, 258): 313, (32, 119): 314, (260, 104): 315, (114, 258): 316, (117, 115): 317, (109, 101): 318, (32, 277): 319, (116, 97): 320, (104, 101): 321, (264, 259): 322, (270, 32): 323, (116, 285): 324, (116, 271): 325, (63, 32): 326, (97, 32): 327, (114, 111): 328, (46, 72): 329, (294, 276): 330, (109, 278): 331, (109, 97): 332, (99, 104): 333, (101, 261): 334, (116, 261): 335, (264, 316): 336, (116, 105): 337, (114, 32): 338, (260, 276): 339, (46, 283): 340, (107, 258): 341, (121, 261): 342, (97, 108): 343, (100, 97): 344, (97, 114): 345, (116, 104): 346, (111, 111): 347, (98, 117): 348, (110, 302): 349, (46, 304): 350, (105, 100): 351, (114, 97): 352, (99, 111): 353, (32, 290): 354, (110, 101): 355, (111, 118): 356, (109, 279): 357, (114, 291): 358, (99, 107): 359, (293, 268): 360, (46, 87): 361, (114, 101): 362, (115, 274): 363, (317, 259): 364, (108, 105): 365, (46, 73): 366, (277, 32): 367, (348, 259): 368, (306, 276): 369, (117, 112): 370, (103, 97): 371, (99, 282): 372, (110, 32): 373, (101, 99): 374, (292, 104): 375, (111, 102): 376, (105, 259): 377, (311, 116): 378, (110, 279): 379, (284, 303): 380, (315, 287): 381, (106, 364): 382, (101, 280): 383, (46, 83): 384, (112, 328): 385, (115, 116): 386, (117, 110): 387, (357, 116): 388, (105, 108): 389, (46, 309): 390, (101, 285): 391, (288, 100): 392, (104, 287): 393, (115, 261): 394, (273, 32): 395, (271, 32): 396, (353, 109): 397, (106, 111): 398, (267, 110): 399, (355, 119): 400, (379, 100): 401, (33, 304): 402, (33, 283): 403, (33, 87): 404, (101, 118): 405, (115, 97): 406, (115, 259): 407, (98, 279): 408, (108, 111): 409, (115, 117): 410, (119, 272): 411, (119, 279): 412, (99, 97): 413, (98, 266): 414, (33, 73): 415, (32, 100): 416, (267, 324): 417, (109, 356): 418, (293, 274): 419, (32, 265): 420, (102, 265): 421, (98, 101): 422, (107, 32): 423, (307, 259): 424, (105, 99): 425, (108, 101): 426, (114, 105): 427, (103, 101): 428, (112, 345): 429, (110, 111): 430, (111, 112): 431, (311, 259): 432, (273, 322): 433, (98, 258): 434, (412, 107): 435, (261, 368): 436, (267, 116): 437, (280, 295): 438, (79, 84): 439, (106, 374): 440, (116, 268): 441, (337, 273): 442, (385, 440): 443, (114, 299): 444, (110, 322): 445, (33, 32): 446, (291, 114): 447, (281, 278): 448, (418, 105): 449, (270, 414): 450, (277, 338): 451, (261, 283): 452, (98, 97): 453, (101, 120): 454, (284, 313): 455, (269, 261): 456, (344, 121): 457, (108, 32): 458, (33, 72): 459, (321, 285): 460, (112, 296): 461, (401, 312): 462, (76, 101): 463, (296, 325): 464, (308, 104): 465, (429, 116): 466, (33, 309): 467, (32, 287): 468, (104, 105): 469, (114, 432): 470, (296, 407): 471, (65, 70): 472, (266, 116): 473, (109, 272): 474, (32, 331): 475, (101, 108): 476, (119, 375): 477, (305, 275): 478, (296, 116): 479, (319, 32): 480, (347, 107): 481, (292, 285): 482, (104, 271): 483, (270, 110): 484, (261, 119): 485, (258, 115): 486, (310, 275): 487, (273, 378): 488, (112, 112): 489, (301, 111): 490, (108, 280): 491, (112, 108): 492, (408, 373): 493, (321, 108): 494, (314, 375): 495, (288, 259): 496, (267, 438): 497, (79, 77): 498, (294, 268): 499, (494, 112): 500, (71, 111): 501, (282, 32): 502, (388, 269): 503, (405, 271): 504, (435, 392): 505, (266, 297): 506, (101, 336): 507, (267, 313): 508, (400, 32): 509, (46, 67): 510, (104, 111): 511, (107, 101): 512, (421, 465): 513}\n","512\n","{0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff', 258: b'e ', 259: b't ', 260: b' t', 261: b', ', 262: b's ', 263: b'\\xe2\\x80', 264: b'\\xe2\\x80\\x99', 265: b'in', 266: b'ou', 267: b'ha', 268: b'he ', 269: b'ing', 270: b' a', 271: b'er', 272: b'or', 273: b'on', 274: b'o ', 275: b' the ', 276: b'hat ', 277: b'you', 278: b'y ', 279: b'ee', 280: b'd ', 281: b'll', 282: b'an', 283: b'I ', 284: b'I\\xe2\\x80\\x99', 285: b'\\xe2\\x80\\x99s ', 286: b'ing ', 287: b'is ', 288: b'en', 289: b'wa', 290: b'for', 291: b'ea', 292: b'it', 293: b't t', 294: b'.T', 295: b'me ', 296: b'la', 297: b'gh', 298: b'm ', 299: b'es', 300: b'was ', 301: b' s', 302: b'ow', 303: b'll ', 304: b'You', 305: b'ed', 306: b'!T', 307: b'go', 308: b'is', 309: b'I\\xe2\\x80\\x99m ', 310: b' to', 311: b'igh', 312: b' to ', 313: b've ', 314: b' w', 315: b' th', 316: b're ', 317: b'us', 318: b'me', 319: b' you', 320: b'ta', 321: b'he', 322: b'\\xe2\\x80\\x99t ', 323: b' a ', 324: b't\\xe2\\x80\\x99s ', 325: b'ter', 326: b'? ', 327: b'a ', 328: b'ro', 329: b'.H', 330: b'.That ', 331: b'my ', 332: b'ma', 333: b'ch', 334: b'e, ', 335: b't, ', 336: b'\\xe2\\x80\\x99re ', 337: b'ti', 338: b'r ', 339: b' that ', 340: b'.I ', 341: b'ke ', 342: b'y, ', 343: b'al', 344: b'da', 345: b'ar', 346: b'th', 347: b'oo', 348: b'bu', 349: b'now', 350: b'.You', 351: b'id', 352: b'ra', 353: b'co', 354: b' for', 355: b'ne', 356: b'ov', 357: b'mee', 358: b'rea', 359: b'ck', 360: b't the ', 361: b'.W', 362: b're', 363: b'so ', 364: b'ust ', 365: b'li', 366: b'.I', 367: b'you ', 368: b'but ', 369: b'!That ', 370: b'up', 371: b'ga', 372: b'can', 373: b'n ', 374: b'ec', 375: b'ith', 376: b'of', 377: b'it ', 378: b'ight', 379: b'nee', 380: b'I\\xe2\\x80\\x99ll ', 381: b' this ', 382: b'just ', 383: b'ed ', 384: b'.S', 385: b'pro', 386: b'st', 387: b'un', 388: b'meet', 389: b'il', 390: b'.I\\xe2\\x80\\x99m ', 391: b'e\\xe2\\x80\\x99s ', 392: b'end', 393: b'his ', 394: b's, ', 395: b'on ', 396: b'er ', 397: b'com', 398: b'jo', 399: b'han', 400: b'new', 401: b'need', 402: b'!You', 403: b'!I ', 404: b'!W', 405: b'ev', 406: b'sa', 407: b'st ', 408: b'bee', 409: b'lo', 410: b'su', 411: b'wor', 412: b'wee', 413: b'ca', 414: b'bou', 415: b'!I', 416: b' d', 417: b'hat\\xe2\\x80\\x99s ', 418: b'mov', 419: b't to ', 420: b' in', 421: b'fin', 422: b'be', 423: b'k ', 424: b'got ', 425: b'ic', 426: b'le', 427: b'ri', 428: b'ge', 429: b'par', 430: b'no', 431: b'op', 432: b'ight ', 433: b'on\\xe2\\x80\\x99t ', 434: b'be ', 435: b'week', 436: b', but ', 437: b'hat', 438: b'd me ', 439: b'OT', 440: b'jec', 441: b'the ', 442: b'tion', 443: b'projec', 444: b'res', 445: b'n\\xe2\\x80\\x99t ', 446: b'! ', 447: b'ear', 448: b'lly ', 449: b'movi', 450: b' abou', 451: b'your ', 452: b', I ', 453: b'ba', 454: b'ex', 455: b'I\\xe2\\x80\\x99ve ', 456: b'ing, ', 457: b'day', 458: b'l ', 459: b'!H', 460: b'he\\xe2\\x80\\x99s ', 461: b'pla', 462: b'need to ', 463: b'Le', 464: b'later', 465: b'ish', 466: b'part', 467: b'!I\\xe2\\x80\\x99m ', 468: b' is ', 469: b'hi', 470: b'right ', 471: b'last ', 472: b'AF', 473: b'out', 474: b'mor', 475: b' my ', 476: b'el', 477: b'with', 478: b'ed the ', 479: b'lat', 480: b' you ', 481: b'ook', 482: b'it\\xe2\\x80\\x99s ', 483: b'her', 484: b' an', 485: b', w', 486: b'e s', 487: b' to the ', 488: b'onight', 489: b'pp', 490: b' so', 491: b'ld ', 492: b'pl', 493: b'been ', 494: b'hel', 495: b' with', 496: b'ent ', 497: b'had me ', 498: b'OM', 499: b'.The ', 500: b'help', 501: b'Go', 502: b'an ', 503: b'meeting', 504: b'ever', 505: b'weekend', 506: b'ough', 507: b'e\\xe2\\x80\\x99re ', 508: b'have ', 509: b'new ', 510: b'.C', 511: b'ho', 512: b'ke', 513: b'finish'}\n","Got the job today, big W!\n","Raw Tokens: [103, 111, 116, 32, 116, 104, 101, 32, 106, 111, 98, 32, 116, 111, 100, 97, 121, 44, 32, 98, 105, 103, 32, 119, 33]\n","Merged Tokens: [307, 360, 398, 98, 310, 344, 342, 98, 105, 103, 314, 33]\n","\u001b[92mgo\u001b[90m(307)\u001b[0m | \u001b[92mt the \u001b[90m(360)\u001b[0m | \u001b[92mjo\u001b[90m(398)\u001b[0m | \u001b[92mb\u001b[90m(98)\u001b[0m | \u001b[92m to\u001b[90m(310)\u001b[0m | \u001b[92mda\u001b[90m(344)\u001b[0m | \u001b[92my, \u001b[90m(342)\u001b[0m | \u001b[92mb\u001b[90m(98)\u001b[0m | \u001b[92mi\u001b[90m(105)\u001b[0m | \u001b[92mg\u001b[90m(103)\u001b[0m | \u001b[92m w\u001b[90m(314)\u001b[0m | \u001b[92m!\u001b[90m(33)\u001b[0m\n","--------------------------------------------------------------------------------\n","\n","I forgot my wallet at home, that’s an L.\n","Raw Tokens: [105, 32, 102, 111, 114, 103, 111, 116, 32, 109, 121, 32, 119, 97, 108, 108, 101, 116, 32, 97, 116, 32, 104, 111, 109, 101, 44, 32, 116, 104, 97, 116, 226, 128, 153, 115, 32, 97, 110, 32, 108, 46]\n","Merged Tokens: [105, 354, 424, 331, 289, 281, 101, 259, 97, 259, 511, 318, 44, 260, 417, 502, 108, 46]\n","\u001b[92mi\u001b[90m(105)\u001b[0m | \u001b[92m for\u001b[90m(354)\u001b[0m | \u001b[92mgot \u001b[90m(424)\u001b[0m | \u001b[92mmy \u001b[90m(331)\u001b[0m | \u001b[92mwa\u001b[90m(289)\u001b[0m | \u001b[92mll\u001b[90m(281)\u001b[0m | \u001b[92me\u001b[90m(101)\u001b[0m | \u001b[92mt \u001b[90m(259)\u001b[0m | \u001b[92ma\u001b[90m(97)\u001b[0m | \u001b[92mt \u001b[90m(259)\u001b[0m | \u001b[92mho\u001b[90m(511)\u001b[0m | \u001b[92mme\u001b[90m(318)\u001b[0m | \u001b[92m,\u001b[90m(44)\u001b[0m | \u001b[92m t\u001b[90m(260)\u001b[0m | \u001b[92mhat’s \u001b[90m(417)\u001b[0m | \u001b[92man \u001b[90m(502)\u001b[0m | \u001b[92ml\u001b[90m(108)\u001b[0m | \u001b[92m.\u001b[90m(46)\u001b[0m\n","--------------------------------------------------------------------------------\n","\n","Your tweet got 5 likes and 100 replies calling you out. L + ratio.\n","Raw Tokens: [121, 111, 117, 114, 32, 116, 119, 101, 101, 116, 32, 103, 111, 116, 32, 53, 32, 108, 105, 107, 101, 115, 32, 97, 110, 100, 32, 49, 48, 48, 32, 114, 101, 112, 108, 105, 101, 115, 32, 99, 97, 108, 108, 105, 110, 103, 32, 121, 111, 117, 32, 111, 117, 116, 46, 32, 108, 32, 43, 32, 114, 97, 116, 105, 111, 46]\n","Merged Tokens: [277, 114, 260, 412, 259, 424, 53, 32, 365, 512, 262, 282, 280, 49, 48, 48, 32, 362, 112, 365, 101, 262, 413, 281, 286, 367, 473, 46, 32, 458, 43, 32, 352, 337, 111, 46]\n","\u001b[92myou\u001b[90m(277)\u001b[0m | \u001b[92mr\u001b[90m(114)\u001b[0m | \u001b[92m t\u001b[90m(260)\u001b[0m | \u001b[92mwee\u001b[90m(412)\u001b[0m | \u001b[92mt \u001b[90m(259)\u001b[0m | \u001b[92mgot \u001b[90m(424)\u001b[0m | \u001b[92m5\u001b[90m(53)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92mli\u001b[90m(365)\u001b[0m | \u001b[92mke\u001b[90m(512)\u001b[0m | \u001b[92ms \u001b[90m(262)\u001b[0m | \u001b[92man\u001b[90m(282)\u001b[0m | \u001b[92md \u001b[90m(280)\u001b[0m | \u001b[92m1\u001b[90m(49)\u001b[0m | \u001b[92m0\u001b[90m(48)\u001b[0m | \u001b[92m0\u001b[90m(48)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92mre\u001b[90m(362)\u001b[0m | \u001b[92mp\u001b[90m(112)\u001b[0m | \u001b[92mli\u001b[90m(365)\u001b[0m | \u001b[92me\u001b[90m(101)\u001b[0m | \u001b[92ms \u001b[90m(262)\u001b[0m | \u001b[92mca\u001b[90m(413)\u001b[0m | \u001b[92mll\u001b[90m(281)\u001b[0m | \u001b[92ming \u001b[90m(286)\u001b[0m | \u001b[92myou \u001b[90m(367)\u001b[0m | \u001b[92mout\u001b[90m(473)\u001b[0m | \u001b[92m.\u001b[90m(46)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92ml \u001b[90m(458)\u001b[0m | \u001b[92m+\u001b[90m(43)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92mra\u001b[90m(352)\u001b[0m | \u001b[92mti\u001b[90m(337)\u001b[0m | \u001b[92mo\u001b[90m(111)\u001b[0m | \u001b[92m.\u001b[90m(46)\u001b[0m\n","--------------------------------------------------------------------------------\n","\n"]}]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-0016.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"LasfRRlwqKAD"}},{"cell_type":"code","source":["\"\"\"\n","Minimal (byte-level) Byte Pair Encoding tokenizer.\n","\n","Algorithmically follows along the GPT tokenizer:\n","https://github.com/openai/gpt-2/blob/master/src/encoder.py\n","\n","Unlike BasicTokenizer:\n","- RegexTokenizer handles an optional regex splitting pattern.\n","- RegexTokenizer handles optional special tokens.\n","\"\"\"\n","\n","import regex as re\n","\n","# the main GPT text split patterns, see\n","# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n","GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n","GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n","\n","\n","class RegexTokenizer():\n","    def __init__(self, pattern=None):\n","        \"\"\"\n","        - pattern: optional string to override the default (GPT-4 split pattern)\n","        - special_tokens: str -> int dictionary of special tokens\n","          example: {'<|endoftext|>': 100257}\n","        \"\"\"\n","        self.merges = {} # (int, int) -> int\n","        self.pattern = \"\" # str\n","        self.special_tokens = {} # str -> int, e.g. {'<|endoftext|>': 100257}\n","        self.vocab = self._build_vocab() # int -> bytes\n","        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern\n","        self.compiled_pattern = re.compile(self.pattern)\n","        self.special_tokens = {}\n","        self.inverse_special_tokens = {}\n","\n","    def _build_vocab(self):\n","        # vocab is simply and deterministically derived from merges\n","        vocab = {idx: bytes([idx]) for idx in range(256)}\n","        for (p0, p1), idx in self.merges.items():\n","            vocab[idx] = vocab[p0] + vocab[p1]\n","        for special, idx in self.special_tokens.items():\n","            vocab[idx] = special.encode(\"utf-8\")\n","        return vocab\n","\n","    def _get_stats(self, ids, counts=None):\n","        \"\"\"\n","        Given a list of integers, return a dictionary of counts of consecutive pairs\n","        Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n","        Optionally allows to update an existing dictionary of counts\n","        \"\"\"\n","        counts = {} if counts is None else counts\n","        for pair in zip(ids, ids[1:]): # iterate consecutive elements\n","            counts[pair] = counts.get(pair, 0) + 1\n","        return counts\n","\n","    def _merge(self, ids, pair, idx):\n","        \"\"\"\n","        In the list of integers (ids), replace all consecutive occurrences\n","        of pair with the new integer token idx\n","        Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n","        \"\"\"\n","        newids = []\n","        i = 0\n","        while i < len(ids):\n","            # if not at the very last position AND the pair matches, replace it\n","            if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n","                newids.append(idx)\n","                i += 2\n","            else:\n","                newids.append(ids[i])\n","                i += 1\n","        return newids\n","\n","    def train(self, text, vocab_size, verbose=False):\n","        assert vocab_size >= 256\n","        num_merges = vocab_size - 256\n","\n","        # split the text up into text chunks\n","        text_chunks = re.findall(self.compiled_pattern, text)\n","\n","        # input text preprocessing\n","        ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks]\n","\n","        # iteratively merge the most common pairs to create new tokens\n","        merges = {} # (int, int) -> int\n","        vocab = {idx: bytes([idx]) for idx in range(256)} # idx -> bytes\n","        for i in range(num_merges):\n","            # count the number of times every consecutive pair appears\n","            stats = {}\n","            for chunk_ids in ids:\n","                # passing in stats will update it in place, adding up counts\n","                self._get_stats(chunk_ids, stats)\n","            # find the pair with the highest count\n","            pair = max(stats, key=stats.get)\n","            # mint a new token: assign it the next available id\n","            idx = 256 + i\n","            # replace all occurrences of pair in ids with idx\n","            ids = [self._merge(chunk_ids, pair, idx) for chunk_ids in ids]\n","            # save the merge\n","            merges[pair] = idx\n","            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n","            # prints\n","            if verbose:\n","                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n","\n","        # save class variables\n","        self.merges = merges # used in encode()\n","        self.vocab = vocab   # used in decode()\n","\n","    def register_special_tokens(self, special_tokens):\n","        # special_tokens is a dictionary of str -> int\n","        # example: {\"<|endoftext|>\": 100257}\n","        self.special_tokens = special_tokens\n","        self.inverse_special_tokens = {v: k for k, v in special_tokens.items()}\n","\n","    def decode(self, ids):\n","        # given ids (list of integers), return Python string\n","        part_bytes = []\n","        for idx in ids:\n","            if idx in self.vocab:\n","                part_bytes.append(self.vocab[idx])\n","            elif idx in self.inverse_special_tokens:\n","                part_bytes.append(self.inverse_special_tokens[idx].encode(\"utf-8\"))\n","            else:\n","                raise ValueError(f\"invalid token id: {idx}\")\n","        text_bytes = b\"\".join(part_bytes)\n","        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n","        return text\n","\n","    def _encode_chunk(self, text_bytes):\n","        # return the token ids\n","        # let's begin. first, convert all bytes to integers in range 0..255\n","        ids = list(text_bytes)\n","        while len(ids) >= 2:\n","            # find the pair with the lowest merge index\n","            stats = self._get_stats(ids)\n","            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n","            # subtle: if there are no more merges available, the key will\n","            # result in an inf for every single pair, and the min will be\n","            # just the first pair in the list, arbitrarily\n","            # we can detect this terminating case by a membership check\n","            if pair not in self.merges:\n","                break # nothing else can be merged anymore\n","            # otherwise let's merge the best pair (lowest merge index)\n","            idx = self.merges[pair]\n","            ids = self._merge(ids, pair, idx)\n","        return ids\n","\n","    def encode_ordinary(self, text):\n","        \"\"\"Encoding that ignores any special tokens.\"\"\"\n","        # split text into chunks of text by categories defined in regex pattern\n","        text_chunks = re.findall(self.compiled_pattern, text)\n","        # all chunks of text are encoded separately, then results are joined\n","        ids = []\n","        for chunk in text_chunks:\n","            chunk_bytes = chunk.encode(\"utf-8\") # raw bytes\n","            chunk_ids = self._encode_chunk(chunk_bytes)\n","            ids.extend(chunk_ids)\n","        return ids\n","\n","    def encode(self, text, allowed_special=\"none_raise\"):\n","        \"\"\"\n","        Unlike encode_ordinary, this function handles special tokens.\n","        allowed_special: can be \"all\"|\"none\"|\"none_raise\" or a custom set of special tokens\n","        if none_raise, then an error is raised if any special token is encountered in text\n","        this is the default tiktoken behavior right now as well\n","        any other behavior is either annoying, or a major footgun\n","        \"\"\"\n","        # decode the user desire w.r.t. handling of special tokens\n","        special = None\n","        if allowed_special == \"all\":\n","            special = self.special_tokens\n","        elif allowed_special == \"none\":\n","            special = {}\n","        elif allowed_special == \"none_raise\":\n","            special = {}\n","            assert all(token not in text for token in self.special_tokens)\n","        elif isinstance(allowed_special, set):\n","            special = {k: v for k, v in self.special_tokens.items() if k in allowed_special}\n","        else:\n","            raise ValueError(f\"allowed_special={allowed_special} not understood\")\n","        if not special:\n","            # shortcut: if no special tokens, just use the ordinary encoding\n","            return self.encode_ordinary(text)\n","        # otherwise, we have to be careful with potential special tokens in text\n","        # we handle special tokens by splitting the text\n","        # based on the occurrence of any exact match with any of the special tokens\n","        # we can use re.split for this. note that surrounding the pattern with ()\n","        # makes it into a capturing group, so the special tokens will be included\n","        special_pattern = \"(\" + \"|\".join(re.escape(k) for k in special) + \")\"\n","        special_chunks = re.split(special_pattern, text)\n","        # now all the special characters are separated from the rest of the text\n","        # all chunks of text are encoded separately, then results are joined\n","        ids = []\n","        for part in special_chunks:\n","            if part in special:\n","                # this is a special token, encode it separately as a special case\n","                ids.append(special[part])\n","            else:\n","                # this is an ordinary sequence, encode it normally\n","                ids.extend(self.encode_ordinary(part))\n","        return ids\n","\n","    def visualize_tokenization(self, ids):\n","        \"\"\"Small helper function useful in debugging: visualize the tokenization of render_conversation\"\"\"\n","        RED = '\\033[91m'\n","        GREEN = '\\033[92m'\n","        RESET = '\\033[0m'\n","        GRAY = '\\033[90m'\n","\n","        tokens = []\n","        for token_id in ids:\n","            token_str = self.decode([token_id])\n","            tokens.append(f\"{GREEN}{token_str}{GRAY}({token_id}){RESET}\")\n","\n","            if token_str in ['<|SOS|>', '<|EOS|>', '<|EOAM|>']:\n","                tokens.append('\\n\\n\\t')\n","\n","        return ' | '.join(tokens)"],"metadata":{"id":"iwuSysCmCJRB","executionInfo":{"status":"ok","timestamp":1768587088609,"user_tz":300,"elapsed":43,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["training_text = uts.get_file_str('zoomer.txt')\n","testing_lines = uts.get_lines_str('zoomer.txt')\n","\n","regex_tokenizer = RegexTokenizer()\n","regex_tokenizer.train(training_text, 512)\n","\n","for i in range(355, 360):\n","  print(testing_lines[i])\n","\n","  tokenized_str = regex_tokenizer.encode(testing_lines[i])\n","  print(regex_tokenizer.visualize_tokenization(tokenized_str))\n","\n","  print('-' * 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fV0k7aYjckGV","executionInfo":{"status":"ok","timestamp":1768587097759,"user_tz":300,"elapsed":7543,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"23a39e33-94e8-4ebc-84a1-4a8e9d8e5763"},"execution_count":84,"outputs":[{"output_type":"stream","name":"stdout","text":["Everything is going smoothly, AISB.\n","\u001b[92mE\u001b[90m(69)\u001b[0m | \u001b[92mver\u001b[90m(381)\u001b[0m | \u001b[92my\u001b[90m(121)\u001b[0m | \u001b[92mt\u001b[90m(116)\u001b[0m | \u001b[92mhing\u001b[90m(486)\u001b[0m | \u001b[92m is\u001b[90m(310)\u001b[0m | \u001b[92m going\u001b[90m(464)\u001b[0m | \u001b[92m s\u001b[90m(265)\u001b[0m | \u001b[92mm\u001b[90m(109)\u001b[0m | \u001b[92mo\u001b[90m(111)\u001b[0m | \u001b[92mot\u001b[90m(303)\u001b[0m | \u001b[92mh\u001b[90m(104)\u001b[0m | \u001b[92mly\u001b[90m(428)\u001b[0m | \u001b[92m,\u001b[90m(44)\u001b[0m | \u001b[92m A\u001b[90m(331)\u001b[0m | \u001b[92mI\u001b[90m(73)\u001b[0m | \u001b[92mS\u001b[90m(83)\u001b[0m | \u001b[92mB\u001b[90m(66)\u001b[0m | \u001b[92m.\u001b[90m(46)\u001b[0m\n","--------------------------------------------------------------------------------\n","AISB, we need to stick to the original plan.\n","\u001b[92mA\u001b[90m(65)\u001b[0m | \u001b[92mI\u001b[90m(73)\u001b[0m | \u001b[92mS\u001b[90m(83)\u001b[0m | \u001b[92mB\u001b[90m(66)\u001b[0m | \u001b[92m,\u001b[90m(44)\u001b[0m | \u001b[92m we\u001b[90m(385)\u001b[0m | \u001b[92m need\u001b[90m(391)\u001b[0m | \u001b[92m to\u001b[90m(271)\u001b[0m | \u001b[92m st\u001b[90m(362)\u001b[0m | \u001b[92mic\u001b[90m(359)\u001b[0m | \u001b[92mk\u001b[90m(107)\u001b[0m | \u001b[92m to\u001b[90m(271)\u001b[0m | \u001b[92m the\u001b[90m(268)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92mor\u001b[90m(275)\u001b[0m | \u001b[92mi\u001b[90m(105)\u001b[0m | \u001b[92mg\u001b[90m(103)\u001b[0m | \u001b[92min\u001b[90m(260)\u001b[0m | \u001b[92mal\u001b[90m(337)\u001b[0m | \u001b[92m pl\u001b[90m(407)\u001b[0m | \u001b[92man\u001b[90m(289)\u001b[0m | \u001b[92m.\u001b[90m(46)\u001b[0m\n","--------------------------------------------------------------------------------\n","AISI, we should approach this problem differently.\n","\u001b[92mA\u001b[90m(65)\u001b[0m | \u001b[92mI\u001b[90m(73)\u001b[0m | \u001b[92mS\u001b[90m(83)\u001b[0m | \u001b[92mI\u001b[90m(73)\u001b[0m | \u001b[92m,\u001b[90m(44)\u001b[0m | \u001b[92m we\u001b[90m(385)\u001b[0m | \u001b[92m sh\u001b[90m(482)\u001b[0m | \u001b[92mould\u001b[90m(508)\u001b[0m | \u001b[92m a\u001b[90m(261)\u001b[0m | \u001b[92mpp\u001b[90m(439)\u001b[0m | \u001b[92mro\u001b[90m(338)\u001b[0m | \u001b[92ma\u001b[90m(97)\u001b[0m | \u001b[92mch\u001b[90m(345)\u001b[0m | \u001b[92m this\u001b[90m(348)\u001b[0m | \u001b[92m pro\u001b[90m(414)\u001b[0m | \u001b[92mb\u001b[90m(98)\u001b[0m | \u001b[92mle\u001b[90m(354)\u001b[0m | \u001b[92mm\u001b[90m(109)\u001b[0m | \u001b[92m d\u001b[90m(293)\u001b[0m | \u001b[92mi\u001b[90m(105)\u001b[0m | \u001b[92mf\u001b[90m(102)\u001b[0m | \u001b[92mf\u001b[90m(102)\u001b[0m | \u001b[92me\u001b[90m(101)\u001b[0m | \u001b[92mre\u001b[90m(269)\u001b[0m | \u001b[92mn\u001b[90m(110)\u001b[0m | \u001b[92mt\u001b[90m(116)\u001b[0m | \u001b[92mly\u001b[90m(428)\u001b[0m | \u001b[92m.\u001b[90m(46)\u001b[0m\n","--------------------------------------------------------------------------------\n","I need to be the AITR and handle this responsibly.\n","\u001b[92mI\u001b[90m(73)\u001b[0m | \u001b[92m need\u001b[90m(391)\u001b[0m | \u001b[92m to\u001b[90m(271)\u001b[0m | \u001b[92m be\u001b[90m(330)\u001b[0m | \u001b[92m the\u001b[90m(268)\u001b[0m | \u001b[92m A\u001b[90m(331)\u001b[0m | \u001b[92mI\u001b[90m(73)\u001b[0m | \u001b[92mT\u001b[90m(84)\u001b[0m | \u001b[92mR\u001b[90m(82)\u001b[0m | \u001b[92m and\u001b[90m(437)\u001b[0m | \u001b[92m h\u001b[90m(294)\u001b[0m | \u001b[92man\u001b[90m(289)\u001b[0m | \u001b[92md\u001b[90m(100)\u001b[0m | \u001b[92mle\u001b[90m(354)\u001b[0m | \u001b[92m this\u001b[90m(348)\u001b[0m | \u001b[92m re\u001b[90m(341)\u001b[0m | \u001b[92ms\u001b[90m(115)\u001b[0m | \u001b[92mp\u001b[90m(112)\u001b[0m | \u001b[92mon\u001b[90m(276)\u001b[0m | \u001b[92ms\u001b[90m(115)\u001b[0m | \u001b[92mi\u001b[90m(105)\u001b[0m | \u001b[92mb\u001b[90m(98)\u001b[0m | \u001b[92mly\u001b[90m(428)\u001b[0m | \u001b[92m.\u001b[90m(46)\u001b[0m\n","--------------------------------------------------------------------------------\n","He’s AKA the best coder in the company.\n","\u001b[92mHe\u001b[90m(334)\u001b[0m | \u001b[92m’s\u001b[90m(285)\u001b[0m | \u001b[92m A\u001b[90m(331)\u001b[0m | \u001b[92mK\u001b[90m(75)\u001b[0m | \u001b[92mA\u001b[90m(65)\u001b[0m | \u001b[92m the\u001b[90m(268)\u001b[0m | \u001b[92m be\u001b[90m(330)\u001b[0m | \u001b[92mst\u001b[90m(297)\u001b[0m | \u001b[92m co\u001b[90m(372)\u001b[0m | \u001b[92md\u001b[90m(100)\u001b[0m | \u001b[92mer\u001b[90m(281)\u001b[0m | \u001b[92m in\u001b[90m(323)\u001b[0m | \u001b[92m the\u001b[90m(268)\u001b[0m | \u001b[92m com\u001b[90m(420)\u001b[0m | \u001b[92mp\u001b[90m(112)\u001b[0m | \u001b[92man\u001b[90m(289)\u001b[0m | \u001b[92my\u001b[90m(121)\u001b[0m | \u001b[92m.\u001b[90m(46)\u001b[0m\n","--------------------------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["!pip install rustbpe\n","\n","# -----------------------------------------------------------------------------\n","# Tokenizer based on rustbpe + tiktoken combo\n","import rustbpe\n","import tiktoken\n","\n","SPECIAL_TOKENS = [\n","    \"<|SOS|>\", # every document begins with the Start of Sequence (SOS) token that delimits documents\n","    \"<|SOUM|>\", # user message start\n","    \"<|EOUM|>\", # user message end\n","    \"<|SOAM|>\", # assistant message start\n","    \"<|EOAM|>\", # assistant message end\n","]\n","\n","SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,2}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n","\n","class RustBPETokenizer:\n","    def __init__(self, enc, bos_token):\n","        self.enc = enc\n","        self.bos_token_id = self.encode_special(bos_token)\n","\n","    @classmethod\n","    def train_from_iterator(cls, text_iterator, vocab_size):\n","        # 1) train using rustbpe\n","        tokenizer = rustbpe.Tokenizer()\n","\n","        # the special tokens are inserted later in __init__, we don't train them here\n","        vocab_size_no_special = vocab_size - len(SPECIAL_TOKENS)\n","        tokenizer.train_from_iterator(text_iterator, vocab_size_no_special, pattern=SPLIT_PATTERN)\n","\n","        # 2) construct the associated tiktoken encoding for inference\n","        pattern = tokenizer.get_pattern()\n","        mergeable_ranks_list = tokenizer.get_mergeable_ranks()\n","        mergeable_ranks = {bytes(k): v for k, v in mergeable_ranks_list}\n","        tokens_offset = len(mergeable_ranks)\n","        special_tokens = {name: tokens_offset + i for i, name in enumerate(SPECIAL_TOKENS)}\n","\n","        enc = tiktoken.Encoding(name=\"rustbpe\", pat_str=pattern, mergeable_ranks=mergeable_ranks, special_tokens=special_tokens)\n","\n","        return cls(enc, \"<|SOS|>\")\n","\n","    def encode(self, text):\n","        return self.enc.encode_ordinary(text)\n","\n","    def encode_special(self, text):\n","        return self.enc.encode_single_token(text)\n","\n","    def decode(self, ids):\n","        return self.enc.decode(ids)\n","\n","    def render_conversation(self, conversation, max_tokens=2048):\n","        \"\"\"\n","        Tokenize a single Chat conversation (which we call a \"doc\" or \"document\" here).\n","        Returns:\n","        - ids: list[int] is a list of token ids of this rendered conversation\n","        \"\"\"\n","        messages = conversation[\"messages\"]\n","\n","        # now we can tokenize the conversation\n","        ids = [self.encode_special('<|SOS|>')]\n","\n","        for i, message in enumerate(messages):\n","            content = message[\"content\"]\n","\n","            if message[\"role\"] == \"user\":\n","                ids += [self.encode_special('<|SOUM|>')] + self.encode(content) + [self.encode_special('<|EOUM|>')]\n","\n","            elif message[\"role\"] == \"assistant\":\n","                ids += [self.encode_special('<|SOAM|>')] + self.encode(content) + [self.encode_special('<|EOAM|>')]\n","\n","        # truncate to max_tokens tokens MAX (helps prevent OOMs)\n","        ids = ids[:max_tokens]\n","\n","        return ids\n","\n","    def visualize_tokenization(self, ids):\n","        \"\"\"Small helper function useful in debugging: visualize the tokenization of render_conversation\"\"\"\n","        RED = '\\033[91m'\n","        GREEN = '\\033[92m'\n","        RESET = '\\033[0m'\n","        GRAY = '\\033[90m'\n","\n","        tokens = []\n","        for i, token_id in enumerate(ids):\n","            token_str = self.decode([token_id])\n","            tokens.append(f\"{GREEN}{token_str}{GRAY}({token_id}){RESET}\")\n","\n","            if token_str in ['<|SOS|>', '<|EOUM|>', '<|EOAM|>']:\n","                tokens.append('\\n\\n\\t')\n","\n","        return ' | '.join(tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nPVeFvJzd7d5","executionInfo":{"status":"ok","timestamp":1768583054890,"user_tz":300,"elapsed":4633,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"78a9ee30-dfce-468a-a65c-b02f75432ac3"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: rustbpe in /usr/local/lib/python3.12/dist-packages (0.1.0)\n"]}]},{"cell_type":"code","source":["training_text = uts.get_file_str('shakespeare.txt')\n","\n","nanochat_tokenizer = RustBPETokenizer.train_from_iterator(training_text, 256 + len(SPECIAL_TOKENS))\n","\n","conversation = {\n","    'messages': [\n","        {'role': 'user', 'content': 'Hello there general kenobi'},\n","        {'role': 'assistant', 'content': 'Hello there grevious'},\n","        {'role': 'user', 'content': 'I love neffer'},\n","    ]\n","}\n","\n","ids = nanochat_tokenizer.render_conversation(conversation)\n","print(nanochat_tokenizer.visualize_tokenization(ids))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oO7H8-XJfg84","executionInfo":{"status":"ok","timestamp":1768583324447,"user_tz":300,"elapsed":6664,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"c4329f3c-a7b2-4b31-f1c3-a18c4208544d"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[92m<|SOS|>\u001b[90m(256)\u001b[0m | \n","\n","\t | \u001b[92m<|SOUM|>\u001b[90m(257)\u001b[0m | \u001b[92mH\u001b[90m(72)\u001b[0m | \u001b[92me\u001b[90m(101)\u001b[0m | \u001b[92ml\u001b[90m(108)\u001b[0m | \u001b[92ml\u001b[90m(108)\u001b[0m | \u001b[92mo\u001b[90m(111)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92mt\u001b[90m(116)\u001b[0m | \u001b[92mh\u001b[90m(104)\u001b[0m | \u001b[92me\u001b[90m(101)\u001b[0m | \u001b[92mr\u001b[90m(114)\u001b[0m | \u001b[92me\u001b[90m(101)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92mg\u001b[90m(103)\u001b[0m | \u001b[92me\u001b[90m(101)\u001b[0m | \u001b[92mn\u001b[90m(110)\u001b[0m | \u001b[92me\u001b[90m(101)\u001b[0m | \u001b[92mr\u001b[90m(114)\u001b[0m | \u001b[92ma\u001b[90m(97)\u001b[0m | \u001b[92ml\u001b[90m(108)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92mk\u001b[90m(107)\u001b[0m | \u001b[92me\u001b[90m(101)\u001b[0m | \u001b[92mn\u001b[90m(110)\u001b[0m | \u001b[92mo\u001b[90m(111)\u001b[0m | \u001b[92mb\u001b[90m(98)\u001b[0m | \u001b[92mi\u001b[90m(105)\u001b[0m | \u001b[92m<|EOUM|>\u001b[90m(258)\u001b[0m | \n","\n","\t | \u001b[92m<|SOAM|>\u001b[90m(259)\u001b[0m | \u001b[92mH\u001b[90m(72)\u001b[0m | \u001b[92me\u001b[90m(101)\u001b[0m | \u001b[92ml\u001b[90m(108)\u001b[0m | \u001b[92ml\u001b[90m(108)\u001b[0m | \u001b[92mo\u001b[90m(111)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92mt\u001b[90m(116)\u001b[0m | \u001b[92mh\u001b[90m(104)\u001b[0m | \u001b[92me\u001b[90m(101)\u001b[0m | \u001b[92mr\u001b[90m(114)\u001b[0m | \u001b[92me\u001b[90m(101)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92mg\u001b[90m(103)\u001b[0m | \u001b[92mr\u001b[90m(114)\u001b[0m | \u001b[92me\u001b[90m(101)\u001b[0m | \u001b[92mv\u001b[90m(118)\u001b[0m | \u001b[92mi\u001b[90m(105)\u001b[0m | \u001b[92mo\u001b[90m(111)\u001b[0m | \u001b[92mu\u001b[90m(117)\u001b[0m | \u001b[92ms\u001b[90m(115)\u001b[0m | \u001b[92m<|EOAM|>\u001b[90m(260)\u001b[0m | \n","\n","\t | \u001b[92m<|SOUM|>\u001b[90m(257)\u001b[0m | \u001b[92mI\u001b[90m(73)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92ml\u001b[90m(108)\u001b[0m | \u001b[92mo\u001b[90m(111)\u001b[0m | \u001b[92mv\u001b[90m(118)\u001b[0m | \u001b[92me\u001b[90m(101)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92mn\u001b[90m(110)\u001b[0m | \u001b[92me\u001b[90m(101)\u001b[0m | \u001b[92mf\u001b[90m(102)\u001b[0m | \u001b[92mf\u001b[90m(102)\u001b[0m | \u001b[92me\u001b[90m(101)\u001b[0m | \u001b[92mr\u001b[90m(114)\u001b[0m | \u001b[92m<|EOUM|>\u001b[90m(258)\u001b[0m | \n","\n","\t\n"]}]},{"cell_type":"markdown","source":["## Export to HTML\n","\n","Uncomment the final line of the cell below and run it to export this notebook to HTML"],"metadata":{"id":"xSLHa2zwsAlr"}},{"cell_type":"code","source":["import os, json\n","\n","def export_notebook():\n","  L_PATH = \"nd-cse-10124-lectures/Notebooks\"\n","  L = \"Lecture_03_Tokenization\"\n","\n","  try:\n","      from google.colab import _message, files\n","\n","      # where you WANT it to live (repo folder)\n","      repo_ipynb_path = f\"/content/{L_PATH}/{L}.ipynb\"\n","\n","      # grab current notebook contents from the UI\n","      nb = _message.blocking_request(\"get_ipynb\", timeout_sec=1)[\"ipynb\"]\n","\n","      # write it into the repo folder as a real file\n","      os.makedirs(os.path.dirname(repo_ipynb_path), exist_ok=True)\n","      with open(repo_ipynb_path, \"w\", encoding=\"utf-8\") as f:\n","          json.dump(nb, f)\n","\n","      # convert + download pdf\n","      !jupyter nbconvert --to html \"{repo_ipynb_path}\"\n","      files.download(repo_ipynb_path.replace(\".ipynb\", \".html\"))\n","  except:\n","      import subprocess\n","\n","      nb_fp = os.getcwd() + f'{L}.ipynb'\n","      print(os.getcwd())\n","\n","      subprocess.run([\"jupyter\", \"nbconvert\", \"--to\", \"html\", nb_fp], check=True)\n","\n","#export_notebook()"],"metadata":{"id":"1HCtuuZFr_oT"},"execution_count":null,"outputs":[]}]}