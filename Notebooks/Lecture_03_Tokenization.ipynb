{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMEttgQ1C8iLcDC5KUX3Dc+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-001.png\" class=\"img-responsive\"/>\n","</div>\n","\n"],"metadata":{"id":"EUZY_VNVJVKT"}},{"cell_type":"code","source":["l = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n","\n","for i in l:\n","  print(i + 5)\n","\n","def adder(n):\n","    for i in range(n):\n","        print(i + 5)\n","\n","adder(7)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Sj_A8jo8vbE","executionInfo":{"status":"ok","timestamp":1768916255134,"user_tz":300,"elapsed":13,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"b63909b0-9e8d-410e-f5b5-fc558cf2f95d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n"]}]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-002.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"uA06kXddnwg2"}},{"cell_type":"code","source":[],"metadata":{"id":"y6Bcm4OQnzGf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-003.png\" class=\"img-responsive\"/>\n","</div>\n","\n"],"metadata":{"id":"mxAAF5GY8vxw"}},{"cell_type":"code","source":["#print(5 + '5')"],"metadata":{"id":"58YQQupe_7UK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-004.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"mtQM5cbm_6f7"}},{"cell_type":"code","source":["combined = 5 + int('5')\n","print(type(combined))\n","print(combined)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BLGTx_tM-IAn","executionInfo":{"status":"ok","timestamp":1768587751537,"user_tz":300,"elapsed":5,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"f0dcd42a-6056-472f-f061-97754780f920"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'int'>\n","10\n"]}]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-005.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"0jUdJ9FmIsfB"}},{"cell_type":"code","source":["def count_consecutive_pairs(ids, counts=None):\n","\n","    # TODO\n","\n","    return counts"],"metadata":{"id":"MoxgX40-IuO7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-006.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"5spfu41pJHGy"}},{"cell_type":"code","source":["import os\n","\n","try:\n","    import google.colab\n","    REPO_URL = \"https://github.com/wtheisen/nd-cse-10124-lectures.git\"\n","\n","    REPO_NAME = \"/content/nd-cse-10124-lectures\"\n","    L_PATH = \"nd-cse-10124-lectures/Datasets\"\n","\n","    %cd /content/\n","    !rm -r {REPO_NAME}\n","\n","    # Clone repo\n","    if not os.path.exists(REPO_NAME):\n","        !git clone {REPO_URL}\n","\n","        # cd into the data folder\n","        %cd {L_PATH}\n","        !pwd\n","\n","except ImportError:\n","    print(\"Unable to download repo, either:\")\n","    print(\"\\tA.) You're not on colab\")\n","    print(\"\\tB.) It has already been cloned\")\n","\n","\n","import utilities as uts"],"metadata":{"id":"XauIb-liOtSM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768922396435,"user_tz":300,"elapsed":3067,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"01740fdb-9a31-41aa-e885-0309160ca187"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","rm: cannot remove '/content/nd-cse-10124-lectures': No such file or directory\n","Cloning into 'nd-cse-10124-lectures'...\n","remote: Enumerating objects: 79, done.\u001b[K\n","remote: Counting objects: 100% (79/79), done.\u001b[K\n","remote: Compressing objects: 100% (57/57), done.\u001b[K\n","remote: Total 79 (delta 27), reused 72 (delta 20), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (79/79), 16.84 MiB | 13.20 MiB/s, done.\n","Resolving deltas: 100% (27/27), done.\n","/content/nd-cse-10124-lectures/Datasets\n","/content/nd-cse-10124-lectures/Datasets\n"]}]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-007.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-008.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-009.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-010.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-011.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-012.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-013.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-014.png\" class=\"img-responsive\"/>\n","</div>\n","\n"],"metadata":{"id":"Jj4xnpaJqXaK"}},{"cell_type":"code","source":["print('Size of Jabberwocky Graph (Words):', len(uts.build_graph_word('jabberwocky.txt')))\n","print('Size of Zoomer Graph (Words):', len(uts.build_graph_word('zoomer.txt')))\n","print('Size of Shakespeare Graph (Words):', len(uts.build_graph_word('shakespeare.txt')))\n","print('#' * 80)\n","print('Size of Jabberwocky Graph (Chars):', len(uts.build_graph_char('jabberwocky.txt')))\n","print('Size of Shakespeare Graph (Chars):', len(uts.build_graph_char('shakespeare.txt')))\n","print('Size of Zoomer Graph (Chars):', len(uts.build_graph_word('zoomer.txt')))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QUGJIkSao3om","executionInfo":{"status":"ok","timestamp":1768922422360,"user_tz":300,"elapsed":9646,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"d58d8e47-9723-4f1b-e05f-0687aab00f5b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Size of Jabberwocky Graph (Words): 49\n","Size of Zoomer Graph (Words): 91\n","Size of Shakespeare Graph (Words): 91\n","################################################################################\n","Size of Jabberwocky Graph (Chars): 49\n","Size of Shakespeare Graph (Chars): 91\n","Size of Zoomer Graph (Chars): 91\n"]}]},{"cell_type":"code","source":["import json\n","from collections import Counter\n","\n","def save_markov_graph_to_json(graph, filename):\n","    \"\"\"\n","    Save a Markov babbler graph to JSON.\n","\n","    Expected structure:\n","        graph[curr_token][next_token] = count\n","\n","    Output JSON structure:\n","        {\n","            \"token_a\": {\n","                \"token_b\": 12,\n","                \"token_c\": 4\n","            },\n","            ...\n","        }\n","    \"\"\"\n","    serializable = {\n","        src: dict(counter)\n","        for src, counter in graph.items()\n","        if counter  # optional: skip empty entries\n","    }\n","\n","    with open(filename, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(serializable, f, ensure_ascii=False, indent=2)\n","\n"],"metadata":{"id":"kiIpjNvAzu3X","executionInfo":{"status":"ok","timestamp":1768919429679,"user_tz":300,"elapsed":43,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["class Cat():\n","    def __init__(self, color):\n","        self.color = color\n","\n","    # TODO"],"metadata":{"id":"dv3sQu75KJyT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-015.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"Vz8qWn5aq9m-"}},{"cell_type":"code","source":["class BasicTokenizer():\n","    def __init__(self):\n","        pass"],"metadata":{"id":"UJ2jKdJ0o-BO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-016.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"hIgpsgViq_tP"}},{"cell_type":"code","source":["\"\"\"\n","Minimal (byte-level) Byte Pair Encoding tokenizer.\n","\n","Algorithmically follows along the GPT tokenizer:\n","https://github.com/openai/gpt-2/blob/master/src/encoder.py\n","\"\"\"\n","\n","class BPE_Tokenizer():\n","    def __init__(self):\n","        self.merges = {} # (int, int) -> int\n","        self.vocab = self._build_vocab() # int -> bytes\n","\n","    def _build_vocab(self):\n","        # vocab is simply and deterministically derived from merges\n","        vocab = {idx: bytes([idx]) for idx in range(256)}\n","\n","        for (p0, p1), idx in self.merges.items():\n","            vocab[idx] = vocab[p0] + vocab[p1]\n","\n","        return vocab\n","\n","    def _get_stats(self, ids, counts=None):\n","        \"\"\"\n","        Given a list of integers, return a dictionary of counts of consecutive pairs\n","        Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n","        Optionally allows to update an existing dictionary of counts\n","        \"\"\"\n","        counts = {} if counts is None else counts\n","\n","        for pair in zip(ids, ids[1:]): # iterate consecutive elements\n","            counts[pair] = counts.get(pair, 0) + 1\n","\n","        return counts\n","\n","    def _merge(self, ids, pair, idx):\n","        \"\"\"\n","        In the list of integers (ids), replace all consecutive occurrences\n","        of pair with the new integer token idx\n","        Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n","        \"\"\"\n","        newids = []\n","        i = 0\n","\n","        while i < len(ids):\n","            # if not at the very last position AND the pair matches, replace it\n","            if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n","                newids.append(idx)\n","                i += 2\n","            else:\n","                newids.append(ids[i])\n","                i += 1\n","\n","        return newids\n","\n","    def train(self, text, vocab_size, verbose=False):\n","        num_merges = vocab_size - 256\n","\n","        # input text preprocessing\n","        text_bytes = text.encode(\"utf-8\") # raw bytes\n","        ids = list(text_bytes) # list of integers in range 0..255\n","\n","        # iteratively merge the most common pairs to create new tokens\n","        merges = {} # (int, int) -> int\n","        vocab = {idx: bytes([idx]) for idx in range(256)} # int -> bytes\n","\n","        for i in range(num_merges):\n","            # count up the number of times every consecutive pair appears\n","            stats = self._get_stats(ids)\n","\n","            # find the pair with the highest count\n","            pair = max(stats, key=stats.get)\n","\n","            # mint a new token: assign it the next available id\n","            idx = 258 + i\n","\n","            # replace all occurrences of pair in ids with idx\n","            ids = self._merge(ids, pair, idx)\n","\n","            # save the merge\n","            merges[pair] = idx\n","            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n","\n","        # save class variables\n","        print(merges)\n","        self.merges = merges # used in encode()\n","        self.vocab = vocab   # used in decode()\n","\n","    def decode(self, ids):\n","        # given ids (list of integers), return Python string\n","        text_bytes = b\"\".join(self.vocab[idx] for idx in ids)\n","        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n","        return text\n","\n","    def encode(self, text):\n","        # given a string text, return the token ids\n","        text_bytes = text.encode(\"utf-8\") # raw bytes\n","        ids = list(text_bytes) # list of integers in range 0..255\n","\n","        while len(ids) >= 2:\n","            # find the pair with the lowest merge index\n","            stats = self._get_stats(ids)\n","            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n","\n","            # just the first pair in the list, arbitrarily\n","            # we can detect this terminating case by a membership check\n","            if pair not in self.merges:\n","                break # nothing else can be merged anymore\n","\n","            # otherwise let's merge the best pair (lowest merge index)\n","            idx = self.merges[pair]\n","            ids = self._merge(ids, pair, idx)\n","\n","        return ids\n","\n","    def visualize_tokenization(self, ids):\n","        \"\"\"Small helper function useful in debugging: visualize the tokenization of render_conversation\"\"\"\n","        RED = '\\033[91m'\n","        GREEN = '\\033[92m'\n","        RESET = '\\033[0m'\n","        GRAY = '\\033[90m'\n","\n","        tokens = []\n","        for token_id in ids:\n","            token_str = self.decode([token_id])\n","            tokens.append(f\"{GREEN}{token_str}{GRAY}({token_id}){RESET}\")\n","\n","            if token_str in ['<|SOS|>', '<|EOS|>']:\n","                tokens.append('\\n\\n\\t')\n","\n","        return ' | '.join(tokens)"],"metadata":{"id":"FMNlxMh4B7Eu","executionInfo":{"status":"ok","timestamp":1768920415839,"user_tz":300,"elapsed":11,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["training_str = uts.get_file_str('zoomer.txt')\n","testing_lines = uts.get_lines_str('zoomer.txt')\n","\n","tokenizer = BPE_Tokenizer()\n","tokenizer.train(training_str, 512)\n","\n","#print(len(tokenizer.vocab))\n","#print(tokenizer.vocab)\n","\n","tokenized_lines = []\n","for line in testing_lines:\n","    #print(testing_lines[i])\n","\n","    tokenized_str = tokenizer.encode(line.lower())\n","    tokenized_lines.append([tokenizer.decode([token]) for token in tokenized_str])\n","    #print(tokenizer.visualize_tokenization(tokenized_str))\n","\n","    #print('-' * 80)\n","    #print()\n","\n","num_gens = 3\n","\n","word_graph = uts.build_graph_word('zoomer.txt')\n","print('Word Level Generation [zoomer.txt]:')\n","for i in range(0, num_gens):\n","  print(f'\\t{i}: {' '.join(uts.generate_sequence(word_graph, 50))}')\n","\n","char_graph = uts.build_graph_char('zoomer.txt')\n","print('Character Level Generation [zoomer.txt]:')\n","for i in range(0, num_gens):\n","  print(f'\\t{i}: {''.join(uts.generate_sequence(char_graph, 50))}')\n","\n","token_graph = uts.build_graph_word(tokenized_lines)\n","print('Token Level Generation [zoomer.txt]:')\n","for i in range(0, num_gens):\n","  print(f'\\t{i}: {''.join(uts.generate_sequence(token_graph, 50))}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SQw48jfcDSL6","executionInfo":{"status":"ok","timestamp":1768921562378,"user_tz":300,"elapsed":5035,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"0ca89c32-b6f4-4907-ff68-5102f7ca9407"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["{(79, 83): 258, (258, 62): 259, (32, 60): 260, (60, 83): 261, (261, 259): 262, (262, 32): 263, (260, 69): 264, (264, 259): 265, (265, 263): 266, (101, 32): 267, (116, 32): 268, (46, 266): 269, (32, 116): 270, (44, 32): 271, (115, 32): 272, (226, 128): 273, (273, 153): 274, (105, 110): 275, (111, 117): 276, (33, 266): 277, (104, 97): 278, (104, 267): 279, (275, 103): 280, (32, 97): 281, (101, 114): 282, (111, 114): 283, (111, 110): 284, (111, 32): 285, (270, 279): 286, (278, 268): 287, (121, 276): 288, (121, 32): 289, (101, 101): 290, (100, 32): 291, (108, 108): 292, (97, 110): 293, (73, 32): 294, (73, 274): 295, (274, 272): 296, (280, 32): 297, (105, 272): 298, (101, 110): 299, (119, 97): 300, (102, 283): 301, (101, 97): 302, (105, 116): 303, (268, 116): 304, (269, 84): 305, (109, 267): 306, (108, 97): 307, (103, 104): 308, (109, 32): 309, (101, 115): 310, (63, 266): 311, (300, 272): 312, (32, 115): 313, (111, 119): 314, (292, 32): 315, (89, 276): 316, (101, 100): 317, (277, 84): 318, (103, 111): 319, (105, 115): 320, (295, 309): 321, (270, 111): 322, (105, 308): 323, (270, 285): 324, (118, 267): 325, (32, 119): 326, (270, 104): 327, (114, 267): 328, (117, 115): 329, (109, 101): 330, (32, 288): 331, (116, 97): 332, (104, 101): 333, (274, 268): 334, (281, 32): 335, (116, 296): 336, (116, 282): 337, (63, 32): 338, (97, 32): 339, (114, 111): 340, (269, 72): 341, (305, 287): 342, (109, 289): 343, (109, 97): 344, (99, 104): 345, (101, 271): 346, (274, 328): 347, (116, 271): 348, (116, 105): 349, (114, 32): 350, (270, 287): 351, (269, 294): 352, (107, 267): 353, (121, 271): 354, (97, 108): 355, (100, 97): 356, (97, 114): 357, (116, 104): 358, (111, 111): 359, (98, 117): 360, (110, 314): 361, (269, 316): 362, (105, 100): 363, (114, 97): 364, (99, 111): 365, (32, 301): 366, (110, 101): 367, (111, 118): 368, (109, 290): 369, (114, 302): 370, (99, 107): 371, (304, 279): 372, (269, 87): 373, (114, 101): 374, (115, 285): 375, (329, 268): 376, (108, 105): 377, (269, 73): 378, (288, 32): 379, (360, 268): 380, (318, 287): 381, (117, 112): 382, (103, 97): 383, (99, 293): 384, (110, 32): 385, (101, 99): 386, (303, 104): 387, (111, 102): 388, (105, 268): 389, (323, 116): 390, (110, 290): 391, (295, 315): 392, (327, 298): 393, (106, 376): 394, (101, 291): 395, (269, 83): 396, (112, 340): 397, (115, 116): 398, (117, 110): 399, (369, 116): 400, (105, 108): 401, (269, 321): 402, (101, 296): 403, (299, 100): 404, (104, 298): 405, (115, 271): 406, (284, 32): 407, (282, 32): 408, (365, 109): 409, (106, 111): 410, (278, 110): 411, (367, 119): 412, (391, 100): 413, (277, 316): 414, (277, 294): 415, (277, 87): 416, (101, 118): 417, (115, 97): 418, (115, 268): 419, (98, 290): 420, (108, 111): 421, (115, 117): 422, (119, 283): 423, (119, 290): 424, (99, 97): 425, (98, 276): 426, (277, 73): 427, (32, 100): 428, (278, 336): 429, (109, 368): 430, (304, 285): 431, (32, 275): 432, (102, 275): 433, (98, 101): 434, (107, 32): 435, (319, 268): 436, (105, 99): 437, (108, 101): 438, (114, 105): 439, (103, 101): 440, (112, 357): 441, (110, 111): 442, (111, 112): 443, (323, 268): 444, (284, 334): 445, (98, 267): 446, (424, 107): 447, (271, 380): 448, (278, 116): 449, (291, 306): 450, (79, 84): 451, (106, 386): 452, (116, 279): 453, (349, 284): 454, (397, 452): 455, (114, 310): 456, (110, 334): 457, (302, 114): 458, (292, 289): 459, (430, 105): 460, (281, 426): 461, (288, 350): 462, (271, 294): 463, (33, 32): 464, (98, 97): 465, (101, 120): 466, (295, 325): 467, (280, 271): 468, (356, 121): 469, (108, 32): 470, (277, 72): 471, (333, 296): 472, (112, 307): 473, (413, 324): 474, (76, 101): 475, (307, 337): 476, (320, 104): 477, (441, 116): 478, (277, 321): 479, (32, 298): 480, (104, 105): 481, (114, 444): 482, (307, 419): 483, (65, 70): 484, (276, 116): 485, (109, 283): 486, (32, 343): 487, (101, 108): 488, (119, 387): 489, (317, 286): 490, (307, 116): 491, (331, 32): 492, (359, 107): 493, (303, 296): 494, (104, 282): 495, (281, 110): 496, (271, 119): 497, (267, 115): 498, (322, 286): 499, (284, 390): 500, (112, 112): 501, (313, 111): 502, (108, 291): 503, (112, 108): 504, (420, 385): 505, (333, 108): 506, (326, 387): 507, (299, 268): 508, (278, 450): 509, (79, 77): 510, (305, 279): 511, (506, 112): 512, (71, 111): 513}\n","Size of Zoomer Graph (Simple Tokens): 273\n","Token Level Generation [zoomer.txt]:\n","\t<SOS><sosos> you can’t that till offwg is like a! <eos> that i i cor is. <eososing of the movieo\n","\t<SOS><sos> uren can. <eostil last s><EOS>\n","\t<SOS><sos><EOS>\n","\t<SOS><sososos> don’t f i’ll finish a btely, ning the weekenpwers be car. <eosos><EOS>\n","\t<SOS><s> this is oppcm?4bu? <eosos> pluly, m to tifit later. <ey b4ever an a she ak. <eo\n","\t<SOS><s><EOS>\n","\t<SOS><s><EOS>\n"]}]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-0016.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"LasfRRlwqKAD"}},{"cell_type":"code","source":["\"\"\"\n","Minimal (byte-level) Byte Pair Encoding tokenizer.\n","\n","Algorithmically follows along the GPT tokenizer:\n","https://github.com/openai/gpt-2/blob/master/src/encoder.py\n","\n","Unlike BasicTokenizer:\n","- RegexTokenizer handles an optional regex splitting pattern.\n","- RegexTokenizer handles optional special tokens.\n","\"\"\"\n","\n","import regex as re\n","\n","# the main GPT text split patterns, see\n","# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n","GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n","GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n","\n","\n","class RegexTokenizer():\n","    def __init__(self):\n","        self.merges = {} # (int, int) -> int\n","        self.special_tokens = {'<SOS>': 256, '<EOS>': 257} # str -> int, e.g. {'<|endoftext|>': 100257}\n","        self.vocab = {idx: bytes([idx]) for idx in range(256)} | {idx: special.encode(\"utf-8\") for special, idx in self.special_tokens.items()}\n","        #self.vocab = self._init_vocab() # int -> bytes\n","\n","        self.pattern = GPT4_SPLIT_PATTERN\n","        self.compiled_pattern = re.compile(self.pattern)\n","\n","    def _build_vocab(self):\n","        # vocab is simply and deterministically derived from merges\n","        vocab = {idx: bytes([idx]) for idx in range(256)} + {idx: special.encode(\"utf-8\") for special, idx in special_tokens.items()}\n","\n","        for special, idx in self.special_tokens.items():\n","            vocab[idx] = special.encode(\"utf-8\")\n","\n","        return vocab\n","\n","    def _get_stats(self, ids, counts=None):\n","        \"\"\"\n","        Given a list of integers, return a dictionary of counts of consecutive pairs\n","        Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n","        Optionally allows to update an existing dictionary of counts\n","        \"\"\"\n","        counts = {} if counts is None else counts\n","        for pair in zip(ids, ids[1:]): # iterate consecutive elements\n","            counts[pair] = counts.get(pair, 0) + 1\n","        return counts\n","\n","    def _merge(self, ids, pair, idx):\n","        \"\"\"\n","        In the list of integers (ids), replace all consecutive occurrences\n","        of pair with the new integer token idx\n","        Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n","        \"\"\"\n","        newids = []\n","        i = 0\n","        while i < len(ids):\n","            # if not at the very last position AND the pair matches, replace it\n","            if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n","                newids.append(idx)\n","                i += 2\n","            else:\n","                newids.append(ids[i])\n","                i += 1\n","        return newids\n","\n","    def train(self, text, max_vocab_size, verbose=False):\n","        pretrain_vocab_size = len(self.vocab)\n","        num_merges = max_vocab_size - pretrain_vocab_size\n","\n","        # split the text up into text chunks\n","        text_chunks = re.findall(self.compiled_pattern, text)\n","\n","        # input text preprocessing\n","        ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks]\n","\n","        # iteratively merge the most common pairs to create new tokens\n","        for i in range(num_merges):\n","            # count the number of times every consecutive pair appears\n","            stats = {}\n","            for chunk_ids in ids:\n","                # passing in stats will update it in place, adding up counts\n","                self._get_stats(chunk_ids, stats)\n","\n","            # find the pair with the highest count\n","            pair = max(stats, key=stats.get)\n","\n","            # mint a new token: assign it the next available id\n","            idx = pretrain_vocab_size + i\n","\n","            # replace all occurrences of pair in ids with idx\n","            ids = [self._merge(chunk_ids, pair, idx) for chunk_ids in ids]\n","\n","            # save the merge\n","            self.merges[pair] = idx\n","            self.vocab[idx] = self.vocab[pair[0]] + self.vocab[pair[1]]\n","\n","    def decode(self, ids):\n","        # given ids (list of integers), return Python string\n","        part_bytes = []\n","        for idx in ids:\n","            if idx in self.vocab:\n","                part_bytes.append(self.vocab[idx])\n","            else:\n","                part_bytes.append('<UNK>')\n","\n","        text_bytes = b\"\".join(part_bytes)\n","        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n","\n","        return text\n","\n","    def _encode_chunk(self, text_bytes):\n","        # return the token ids\n","        # let's begin. first, convert all bytes to integers in range 0..255\n","        ids = list(text_bytes)\n","\n","        while len(ids) >= 2:\n","            # find the pair with the lowest merge index\n","            stats = self._get_stats(ids)\n","            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n","            # subtle: if there are no more merges available, the key will\n","            # result in an inf for every single pair, and the min will be\n","            # just the first pair in the list, arbitrarily\n","            # we can detect this terminating case by a membership check\n","            if pair not in self.merges:\n","                break # nothing else can be merged anymore\n","            # otherwise let's merge the best pair (lowest merge index)\n","            idx = self.merges[pair]\n","            ids = self._merge(ids, pair, idx)\n","\n","        return ids\n","\n","    def encode(self, text):\n","        # otherwise, we have to be careful with potential special tokens in text\n","        # we handle special tokens by splitting the text\n","        # based on the occurrence of any exact match with any of the special tokens\n","        # we can use re.split for this. note that surrounding the pattern with ()\n","        # makes it into a capturing group, so the special tokens will be included\n","        special_pattern = \"(\" + \"|\".join(re.escape(k) for k in self.special_tokens) + \")\"\n","        special_chunks = re.split(special_pattern, text)\n","\n","        # now all the special characters are separated from the rest of the text\n","        # all chunks of text are encoded separately, then results are joined\n","        ids = []\n","        for part in special_chunks[1:-1]:\n","            if part in self.special_token:\n","                # this is a special token, encode it separately as a special case\n","                ids.append(self.special_tokens[part])\n","            else:\n","                # this is an ordinary sequence, encode it normally\n","                text_chunks = re.findall(self.compiled_pattern, text)\n","\n","                for chunk in text_chunks:\n","                    chunk_bytes = chunk.encode(\"utf-8\") # raw bytes\n","                    chunk_ids = self._encode_chunk(chunk_bytes)\n","                    ids.extend(chunk_ids)\n","\n","        return ids\n","\n","    def visualize_tokenization(self, ids):\n","        \"\"\"Small helper function useful in debugging: visualize the tokenization of render_conversation\"\"\"\n","        RED = '\\033[91m'\n","        GREEN = '\\033[92m'\n","        RESET = '\\033[0m'\n","        GRAY = '\\033[90m'\n","\n","        tokens = []\n","        for token_id in ids:\n","            token_str = self.decode([token_id])\n","            tokens.append(f\"{GREEN}{token_str}{GRAY}({token_id}){RESET}\")\n","\n","        return ' | '.join(tokens)"],"metadata":{"id":"iwuSysCmCJRB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_text = uts.get_file_str('shakespeare.txt')\n","testing_lines = uts.get_lines_str('shakespeare.txt')\n","\n","regex_tokenizer = RegexTokenizer()\n","\n","max_vocab_size = 512\n","regex_tokenizer.train(training_text, max_vocab_size)\n","\n","print(f'Max Vocab Size: {max_vocab_size}, Trained Vocab Size: {len(regex_tokenizer.vocab)}')\n","\n","for i in range(355, 360):\n","    print('-' * 80)\n","    print(testing_lines[i])\n","\n","    tokenized_str = regex_tokenizer.encode(testing_lines[i])\n","    print(regex_tokenizer.visualize_tokenization(tokenized_str))\n","\n","#save_markov_graph_to_json(build_graph_word(uts.get_lines_str('zoomer.txt')), \"zoomer_char_graph.json\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fV0k7aYjckGV","executionInfo":{"status":"ok","timestamp":1768592079547,"user_tz":300,"elapsed":853268,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"b85f99f3-d863-4f79-e5f0-4dcf0c82b2ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Max Vocab Size: 512, Trained Vocab Size: 512\n","--------------------------------------------------------------------------------\n","<SOS> Lo in the orient when the gracious light <EOS>\n","\u001b[92m<SOS>\u001b[90m(256)\u001b[0m | \u001b[92m L\u001b[90m(358)\u001b[0m | \u001b[92mo\u001b[90m(111)\u001b[0m | \u001b[92m in\u001b[90m(318)\u001b[0m | \u001b[92m the\u001b[90m(274)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92mor\u001b[90m(279)\u001b[0m | \u001b[92mi\u001b[90m(105)\u001b[0m | \u001b[92ment\u001b[90m(359)\u001b[0m | \u001b[92m whe\u001b[90m(491)\u001b[0m | \u001b[92mn\u001b[90m(110)\u001b[0m | \u001b[92m the\u001b[90m(274)\u001b[0m | \u001b[92m g\u001b[90m(312)\u001b[0m | \u001b[92mra\u001b[90m(377)\u001b[0m | \u001b[92mc\u001b[90m(99)\u001b[0m | \u001b[92mi\u001b[90m(105)\u001b[0m | \u001b[92mous\u001b[90m(471)\u001b[0m | \u001b[92m li\u001b[90m(389)\u001b[0m | \u001b[92mght\u001b[90m(379)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92m<EOS>\u001b[90m(257)\u001b[0m\n","--------------------------------------------------------------------------------\n","<SOS> Lifts up his burning head, each under eye <EOS>\n","\u001b[92m<SOS>\u001b[90m(256)\u001b[0m | \u001b[92m L\u001b[90m(358)\u001b[0m | \u001b[92mif\u001b[90m(440)\u001b[0m | \u001b[92mt\u001b[90m(116)\u001b[0m | \u001b[92ms\u001b[90m(115)\u001b[0m | \u001b[92m up\u001b[90m(474)\u001b[0m | \u001b[92m his\u001b[90m(369)\u001b[0m | \u001b[92m b\u001b[90m(275)\u001b[0m | \u001b[92mur\u001b[90m(387)\u001b[0m | \u001b[92mn\u001b[90m(110)\u001b[0m | \u001b[92ming\u001b[90m(308)\u001b[0m | \u001b[92m he\u001b[90m(304)\u001b[0m | \u001b[92mad\u001b[90m(361)\u001b[0m | \u001b[92m,\u001b[90m(44)\u001b[0m | \u001b[92m e\u001b[90m(348)\u001b[0m | \u001b[92ma\u001b[90m(97)\u001b[0m | \u001b[92mch\u001b[90m(338)\u001b[0m | \u001b[92m u\u001b[90m(354)\u001b[0m | \u001b[92mnd\u001b[90m(273)\u001b[0m | \u001b[92mer\u001b[90m(278)\u001b[0m | \u001b[92m e\u001b[90m(348)\u001b[0m | \u001b[92my\u001b[90m(121)\u001b[0m | \u001b[92me\u001b[90m(101)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92m<EOS>\u001b[90m(257)\u001b[0m\n","--------------------------------------------------------------------------------\n","<SOS> Doth homage to his new-appearing sight, <EOS>\n","\u001b[92m<SOS>\u001b[90m(256)\u001b[0m | \u001b[92m D\u001b[90m(371)\u001b[0m | \u001b[92mot\u001b[90m(305)\u001b[0m | \u001b[92mh\u001b[90m(104)\u001b[0m | \u001b[92m h\u001b[90m(297)\u001b[0m | \u001b[92mom\u001b[90m(311)\u001b[0m | \u001b[92ma\u001b[90m(97)\u001b[0m | \u001b[92mge\u001b[90m(430)\u001b[0m | \u001b[92m to\u001b[90m(300)\u001b[0m | \u001b[92m his\u001b[90m(369)\u001b[0m | \u001b[92m ne\u001b[90m(463)\u001b[0m | \u001b[92mw\u001b[90m(119)\u001b[0m | \u001b[92m-\u001b[90m(45)\u001b[0m | \u001b[92ma\u001b[90m(97)\u001b[0m | \u001b[92mp\u001b[90m(112)\u001b[0m | \u001b[92mp\u001b[90m(112)\u001b[0m | \u001b[92mear\u001b[90m(441)\u001b[0m | \u001b[92ming\u001b[90m(308)\u001b[0m | \u001b[92m s\u001b[90m(267)\u001b[0m | \u001b[92might\u001b[90m(452)\u001b[0m | \u001b[92m,\u001b[90m(44)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92m<EOS>\u001b[90m(257)\u001b[0m\n","--------------------------------------------------------------------------------\n","<SOS> Serving with looks his sacred majesty, <EOS>\n","\u001b[92m<SOS>\u001b[90m(256)\u001b[0m | \u001b[92m S\u001b[90m(317)\u001b[0m | \u001b[92mer\u001b[90m(278)\u001b[0m | \u001b[92mv\u001b[90m(118)\u001b[0m | \u001b[92ming\u001b[90m(308)\u001b[0m | \u001b[92m with\u001b[90m(356)\u001b[0m | \u001b[92m l\u001b[90m(288)\u001b[0m | \u001b[92moo\u001b[90m(351)\u001b[0m | \u001b[92mk\u001b[90m(107)\u001b[0m | \u001b[92ms\u001b[90m(115)\u001b[0m | \u001b[92m his\u001b[90m(369)\u001b[0m | \u001b[92m s\u001b[90m(267)\u001b[0m | \u001b[92ma\u001b[90m(97)\u001b[0m | \u001b[92mc\u001b[90m(99)\u001b[0m | \u001b[92mre\u001b[90m(271)\u001b[0m | \u001b[92md\u001b[90m(100)\u001b[0m | \u001b[92m m\u001b[90m(269)\u001b[0m | \u001b[92ma\u001b[90m(97)\u001b[0m | \u001b[92mj\u001b[90m(106)\u001b[0m | \u001b[92mest\u001b[90m(416)\u001b[0m | \u001b[92my\u001b[90m(121)\u001b[0m | \u001b[92m,\u001b[90m(44)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92m<EOS>\u001b[90m(257)\u001b[0m\n","--------------------------------------------------------------------------------\n","<SOS> And having climbed the steep-up heavenly hill, <EOS>\n","\u001b[92m<SOS>\u001b[90m(256)\u001b[0m | \u001b[92m And\u001b[90m(355)\u001b[0m | \u001b[92m ha\u001b[90m(321)\u001b[0m | \u001b[92mv\u001b[90m(118)\u001b[0m | \u001b[92ming\u001b[90m(308)\u001b[0m | \u001b[92m c\u001b[90m(286)\u001b[0m | \u001b[92ml\u001b[90m(108)\u001b[0m | \u001b[92mim\u001b[90m(336)\u001b[0m | \u001b[92mb\u001b[90m(98)\u001b[0m | \u001b[92med\u001b[90m(340)\u001b[0m | \u001b[92m the\u001b[90m(274)\u001b[0m | \u001b[92m st\u001b[90m(373)\u001b[0m | \u001b[92me\u001b[90m(101)\u001b[0m | \u001b[92me\u001b[90m(101)\u001b[0m | \u001b[92mp\u001b[90m(112)\u001b[0m | \u001b[92m-\u001b[90m(45)\u001b[0m | \u001b[92mu\u001b[90m(117)\u001b[0m | \u001b[92mp\u001b[90m(112)\u001b[0m | \u001b[92m he\u001b[90m(304)\u001b[0m | \u001b[92ma\u001b[90m(97)\u001b[0m | \u001b[92mven\u001b[90m(503)\u001b[0m | \u001b[92mly\u001b[90m(385)\u001b[0m | \u001b[92m h\u001b[90m(297)\u001b[0m | \u001b[92mill\u001b[90m(344)\u001b[0m | \u001b[92m,\u001b[90m(44)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92m<EOS>\u001b[90m(257)\u001b[0m\n"]}]},{"cell_type":"code","source":["!pip install rustbpe\n","\n","# -----------------------------------------------------------------------------\n","# Tokenizer based on rustbpe + tiktoken combo\n","import rustbpe\n","import tiktoken\n","\n","SPECIAL_TOKENS = [\n","    \"<|SOS|>\", # every document begins with the Start of Sequence (SOS) token that delimits documents\n","    \"<|SOUM|>\", # user message start\n","    \"<|EOUM|>\", # user message end\n","    \"<|SOAM|>\", # assistant message start\n","    \"<|EOAM|>\", # assistant message end\n","]\n","\n","SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,2}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n","\n","class RustBPETokenizer:\n","    def __init__(self, enc, bos_token):\n","        self.enc = enc\n","        self.bos_token_id = self.encode_special(bos_token)\n","\n","    @classmethod\n","    def train_from_iterator(cls, text_iterator, vocab_size):\n","        # 1) train using rustbpe\n","        tokenizer = rustbpe.Tokenizer()\n","\n","        # the special tokens are inserted later in __init__, we don't train them here\n","        vocab_size_no_special = vocab_size - len(SPECIAL_TOKENS)\n","        tokenizer.train_from_iterator(text_iterator, vocab_size_no_special, pattern=SPLIT_PATTERN)\n","\n","        # 2) construct the associated tiktoken encoding for inference\n","        pattern = tokenizer.get_pattern()\n","        mergeable_ranks_list = tokenizer.get_mergeable_ranks()\n","        mergeable_ranks = {bytes(k): v for k, v in mergeable_ranks_list}\n","        tokens_offset = len(mergeable_ranks)\n","        special_tokens = {name: tokens_offset + i for i, name in enumerate(SPECIAL_TOKENS)}\n","\n","        enc = tiktoken.Encoding(name=\"rustbpe\", pat_str=pattern, mergeable_ranks=mergeable_ranks, special_tokens=special_tokens)\n","\n","        return cls(enc, \"<|SOS|>\")\n","\n","    def encode(self, text):\n","        return self.enc.encode_ordinary(text)\n","\n","    def encode_special(self, text):\n","        return self.enc.encode_single_token(text)\n","\n","    def decode(self, ids):\n","        return self.enc.decode(ids)\n","\n","    def render_conversation(self, conversation, max_tokens=2048):\n","        \"\"\"\n","        Tokenize a single Chat conversation (which we call a \"doc\" or \"document\" here).\n","        Returns:\n","        - ids: list[int] is a list of token ids of this rendered conversation\n","        \"\"\"\n","        messages = conversation[\"messages\"]\n","\n","        # now we can tokenize the conversation\n","        ids = [self.encode_special('<|SOS|>')]\n","\n","        for i, message in enumerate(messages):\n","            content = message[\"content\"]\n","\n","            if message[\"role\"] == \"user\":\n","                ids += [self.encode_special('<|SOUM|>')] + self.encode(content) + [self.encode_special('<|EOUM|>')]\n","\n","            elif message[\"role\"] == \"assistant\":\n","                ids += [self.encode_special('<|SOAM|>')] + self.encode(content) + [self.encode_special('<|EOAM|>')]\n","\n","        # truncate to max_tokens tokens MAX (helps prevent OOMs)\n","        ids = ids[:max_tokens]\n","\n","        return ids\n","\n","    def visualize_tokenization(self, ids):\n","        \"\"\"Small helper function useful in debugging: visualize the tokenization of render_conversation\"\"\"\n","        RED = '\\033[91m'\n","        GREEN = '\\033[92m'\n","        RESET = '\\033[0m'\n","        GRAY = '\\033[90m'\n","\n","        tokens = []\n","        for i, token_id in enumerate(ids):\n","            token_str = self.decode([token_id])\n","            tokens.append(f\"{GREEN}{token_str}{GRAY}({token_id}){RESET}\")\n","\n","            if token_str in ['<|SOS|>', '<|EOUM|>', '<|EOAM|>']:\n","                tokens.append('\\n\\n\\t')\n","\n","        return ' | '.join(tokens)"],"metadata":{"id":"nPVeFvJzd7d5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_text = uts.get_file_str('shakespeare.txt')\n","\n","nanochat_tokenizer = RustBPETokenizer.train_from_iterator(training_text, 256 + len(SPECIAL_TOKENS))\n","\n","conversation = {\n","    'messages': [\n","        {'role': 'user', 'content': 'Hello there general kenobi'},\n","        {'role': 'assistant', 'content': 'Hello there grevious'},\n","        {'role': 'user', 'content': 'I love neffer'},\n","    ]\n","}\n","\n","ids = nanochat_tokenizer.render_conversation(conversation)\n","print(nanochat_tokenizer.visualize_tokenization(ids))"],"metadata":{"id":"oO7H8-XJfg84"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Export to HTML\n","\n","Uncomment the final line of the cell below and run it to export this notebook to HTML"],"metadata":{"id":"xSLHa2zwsAlr"}},{"cell_type":"code","source":["import os, json\n","\n","def export_notebook():\n","  L_PATH = \"nd-cse-10124-lectures/Notebooks\"\n","  L = \"Lecture_03_Tokenization\"\n","\n","  try:\n","      from google.colab import _message, files\n","\n","      # where you WANT it to live (repo folder)\n","      repo_ipynb_path = f\"/content/{L_PATH}/{L}.ipynb\"\n","\n","      # grab current notebook contents from the UI\n","      nb = _message.blocking_request(\"get_ipynb\", timeout_sec=1)[\"ipynb\"]\n","\n","      # write it into the repo folder as a real file\n","      os.makedirs(os.path.dirname(repo_ipynb_path), exist_ok=True)\n","      with open(repo_ipynb_path, \"w\", encoding=\"utf-8\") as f:\n","          json.dump(nb, f)\n","\n","      # convert + download pdf\n","      !jupyter nbconvert --to html \"{repo_ipynb_path}\"\n","      files.download(repo_ipynb_path.replace(\".ipynb\", \".html\"))\n","  except:\n","      import subprocess\n","\n","      nb_fp = os.getcwd() + f'{L}.ipynb'\n","      print(os.getcwd())\n","\n","      subprocess.run([\"jupyter\", \"nbconvert\", \"--to\", \"html\", nb_fp], check=True)\n","\n","#export_notebook()"],"metadata":{"id":"1HCtuuZFr_oT"},"execution_count":null,"outputs":[]}]}