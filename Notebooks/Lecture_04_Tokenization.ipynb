{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNDv7rD43lkxTQF9I14ii85"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-001.png\" class=\"img-responsive\"/>\n","</div>\n","\n"],"metadata":{"id":"EUZY_VNVJVKT"}},{"cell_type":"code","source":["l = list(range(90, 100))\n","\n","for num in l:\n","    print(num + 5)\n","#print(l)\n","\n","#print(range(5))\n","#print(list(range(5)))\n","\n","def add_five(max_num):\n","    for num in range(max_num):\n","        print(num + '5')\n","\n","add_five(5)"],"metadata":{"id":"3Sj_A8jo8vbE","colab":{"base_uri":"https://localhost:8080/","height":471},"executionInfo":{"status":"error","timestamp":1768934756514,"user_tz":300,"elapsed":12,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"db4d0175-82ee-4deb-b267-b2950a99e38a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["95\n","96\n","97\n","98\n","99\n","100\n","101\n","102\n","103\n","104\n"]},{"output_type":"error","ename":"TypeError","evalue":"unsupported operand type(s) for +: 'int' and 'str'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-379716175.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0madd_five\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-379716175.py\u001b[0m in \u001b[0;36madd_five\u001b[0;34m(max_num)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madd_five\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0madd_five\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"]}]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-002.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-003.png\" class=\"img-responsive\"/>\n","</div>\n","\n"],"metadata":{"id":"mxAAF5GY8vxw"}},{"cell_type":"code","source":["#print(5 + '5')"],"metadata":{"id":"58YQQupe_7UK","colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"status":"error","timestamp":1768931049389,"user_tz":300,"elapsed":43,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"4a1ca15f-6fcb-42c2-e8c9-7e8321079f96"},"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"unsupported operand type(s) for +: 'int' and 'str'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-433515484.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"]}]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-004.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"mtQM5cbm_6f7"}},{"cell_type":"code","source":["combined = str(5) + '5'\n","print(type(combined))\n","print(combined)"],"metadata":{"id":"BLGTx_tM-IAn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768931136295,"user_tz":300,"elapsed":42,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"7d839f95-14e0-4a95-f326-8c2d5b886f2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'str'>\n","55\n"]}]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-005.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"0jUdJ9FmIsfB"}},{"cell_type":"code","source":["def count_consecutive_pairs(nums):\n","    counts = {}\n","\n","    for pair in zip(nums, nums[1:]):\n","        counts[pair] = counts.get(pair, 0) + 1\n","\n","    return counts\n","\n","print(count_consecutive_pairs([1, 2, 3, 1, 2]))\n","\n","for pair in zip([1, 2, 3], [4, 5, 6]):\n","    print(pair)"],"metadata":{"id":"MoxgX40-IuO7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768931715095,"user_tz":300,"elapsed":43,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"4fe05c37-9ef7-47b1-f32c-645f60f87aca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{(1, 2): 2, (2, 3): 1, (3, 1): 1}\n","(1, 4)\n","(2, 5)\n","(3, 6)\n"]}]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-006.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-007.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"Fan_E23oGShw"}},{"cell_type":"code","source":["l = ['cats', 'dogs', 'fish', 'birds']\n","\n","#print(l[0])\n","#print(l[0:2])\n","print(l[2:])\n","#print(l[:2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3sFj9cA1GS2_","executionInfo":{"status":"ok","timestamp":1768931906450,"user_tz":300,"elapsed":4,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"6c6c0fc8-7ec3-4431-e786-bc5850c6748f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['fish', 'birds']\n"]}]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-008.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-009.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-010.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-011.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-012.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-013.png\" class=\"img-responsive\"/>\n","</div>\n","\n","\n","\n","\n"],"metadata":{"id":"5spfu41pJHGy"}},{"cell_type":"code","source":["import os\n","\n","try:\n","    import google.colab\n","    REPO_URL = \"https://github.com/wtheisen/nd-cse-10124-lectures.git\"\n","\n","    REPO_NAME = \"/content/nd-cse-10124-lectures\"\n","    L_PATH = \"nd-cse-10124-lectures/Datasets\"\n","\n","    %cd /content/\n","    !rm -r {REPO_NAME}\n","\n","    # Clone repo\n","    if not os.path.exists(REPO_NAME):\n","        !git clone {REPO_URL}\n","\n","        # cd into the data folder\n","        %cd {L_PATH}\n","        !pwd\n","\n","except ImportError:\n","    print(\"Unable to download repo, either:\")\n","    print(\"\\tA.) You're not on colab\")\n","    print(\"\\tB.) It has already been cloned\")\n","\n","\n","import utilities as uts"],"metadata":{"id":"XauIb-liOtSM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768931933006,"user_tz":300,"elapsed":1330,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"590bb686-c698-48fa-e4fc-350a3fe6c630"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","Cloning into 'nd-cse-10124-lectures'...\n","remote: Enumerating objects: 111, done.\u001b[K\n","remote: Counting objects: 100% (111/111), done.\u001b[K\n","remote: Compressing objects: 100% (77/77), done.\u001b[K\n","remote: Total 111 (delta 54), reused 89 (delta 32), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (111/111), 17.37 MiB | 42.05 MiB/s, done.\n","Resolving deltas: 100% (54/54), done.\n","/content/nd-cse-10124-lectures/Datasets\n","/content/nd-cse-10124-lectures/Datasets\n"]}]},{"cell_type":"code","source":["print('Size of Jabberwocky Graph (Words):', len(uts.build_graph_word('jabberwocky.txt')))\n","print('Size of Zoomer Graph (Words):', len(uts.build_graph_word('zoomer.txt')))\n","print('Size of Shakespeare Graph (Words):', len(uts.build_graph_word('shakespeare.txt')))\n","print('#' * 80)\n","print('Size of Jabberwocky Graph (Chars):', len(uts.build_graph_char('jabberwocky.txt')))\n","print('Size of Shakespeare Graph (Chars):', len(uts.build_graph_char('shakespeare.txt')))\n","print('Size of Zoomer Graph (Chars):', len(uts.build_graph_char('zoomer.txt')))\n","print('#' * 80)\n","\n","num_gens = 3\n","\n","word_graph = uts.build_graph_word('zoomer.txt')\n","print('Word Level Generation [zoomer.txt]:')\n","for i in range(0, num_gens):\n","  print(f'\\t{i}: {' '.join(uts.generate_sequence(word_graph, 50))}')\n","\n","print('#' * 80)\n","\n","char_graph = uts.build_graph_char('zoomer.txt')\n","print('Character Level Generation [zoomer.txt]:')\n","for i in range(0, num_gens):\n","  print(f'\\t{i}: {''.join(uts.generate_sequence(char_graph, 50))}')\n","\n","print('#' * 80)\n","\n","\n","\n"],"metadata":{"id":"QUGJIkSao3om","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768932276360,"user_tz":300,"elapsed":1997,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"31b62a13-5b02-4871-d0a3-ed4a67f083ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Size of Jabberwocky Graph (Words): 103\n","Size of Zoomer Graph (Words): 3698\n","Size of Shakespeare Graph (Words): 67506\n","################################################################################\n","Size of Jabberwocky Graph (Chars): 46\n","Size of Shakespeare Graph (Chars): 91\n","Size of Zoomer Graph (Chars): 90\n","################################################################################\n","Word Level Generation [zoomer.txt]:\n","\t0: <SOS> Sorry, ;S, can complain all day, OO! <EOS>\n","\t1: <SOS> That image is too many beers last night! <EOS>\n","\t2: <SOS> Make sure you don’t get your CAM for the good now, take several seats. <EOS>\n","################################################################################\n","Character Level Generation [zoomer.txt]:\n","\t0: <SOS>PChinuriltelaton Thin’t y.<EOS>\n","\t1: <SOS>YSA o ts bu’sodn nsodit sst llere sou, m igouss.<EOS>\n","\t2: <SOS>MURMOYSSNDMWKI’t.<EOS>\n"]}]},{"cell_type":"code","source":["from tokenizer import Simple_Tokenizer\n","\n","def create_token_graph(file_name):\n","    training_str = uts.get_file_as_string(file_name)\n","\n","\n","    tokenizer = Simple_Tokenizer()\n","    tokenizer.train(training_str, 512)\n","\n","    tokenized_lines = []\n","    for line in testing_lines:\n","        tokenized_str = tokenizer.encode(line.lower())\n","        tokenized_lines.append([tokenizer.decode([token]) for token in tokenized_str])\n","\n","    token_graph = uts.build_graph_word(''.join(tokenized_lines))\n","\n","    return token_graph\n","\n","print('Size of Jabberwocky Graph (Tokens):', len(create_token_graph('jabberwocky.txt')))\n","print('Size of Shakespeare Graph (Tokens):', len(create_token_graph('shakespeare.txt')))\n","print('Size of Zoomer Graph (Tokens):', len(create_token_graph('zoomer.txt')))\n","print('#' * 80)\n","\n","word_graph = create_token_graph('zoomer.txt')\n","print('Token Level Generation [zoomer.txt]:')\n","for i in range(0, num_gens):\n","  print(f'\\t{i}: {''.join(uts.generate_sequence(, 50))}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":108},"id":"-0aQhw4u1u0w","executionInfo":{"status":"error","timestamp":1768936653243,"user_tz":300,"elapsed":14,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"e22c78fb-f966-403e-e8b3-07e58b1e0562"},"execution_count":1,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"f-string: expecting '=', or '!', or ':', or '}' (ipython-input-2366303375.py, line 27)","traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2366303375.py\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    print(f'\\t{i}: {''.join(uts.generate_sequence(, 50))}')\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: expecting '=', or '!', or ':', or '}'\n"]}]},{"cell_type":"markdown","source":["\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-014.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-016.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-017.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-018.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-019.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-020.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"-JW9ueYDH-tv"}},{"cell_type":"code","source":["class Cat():\n","    def __init__(self, color):\n","        self.color = color\n","\n","    # TODO"],"metadata":{"id":"dv3sQu75KJyT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-019.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"hIgpsgViq_tP"}},{"cell_type":"code","source":["\"\"\"\n","Minimal (byte-level) Byte Pair Encoding tokenizer.\n","\n","Algorithmically follows along the GPT tokenizer:\n","https://github.com/openai/gpt-2/blob/master/src/encoder.py\n","\"\"\"\n","\n","class BPE_Tokenizer():\n","    def __init__(self):\n","        self.merges = {} # (int, int) -> int\n","        self.vocab = self._build_vocab() # int -> bytes\n","\n","    def _build_vocab(self):\n","        # vocab is simply and deterministically derived from merges\n","        vocab = {idx: bytes([idx]) for idx in range(256)}\n","\n","        for (p0, p1), idx in self.merges.items():\n","            vocab[idx] = vocab[p0] + vocab[p1]\n","\n","        return vocab\n","\n","    def _get_stats(self, ids, counts=None):\n","        \"\"\"\n","        Given a list of integers, return a dictionary of counts of consecutive pairs\n","        Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n","        Optionally allows to update an existing dictionary of counts\n","        \"\"\"\n","        counts = {} if counts is None else counts\n","\n","        for pair in zip(ids, ids[1:]): # iterate consecutive elements\n","            counts[pair] = counts.get(pair, 0) + 1\n","\n","        return counts\n","\n","    def _merge(self, ids, pair, idx):\n","        \"\"\"\n","        In the list of integers (ids), replace all consecutive occurrences\n","        of pair with the new integer token idx\n","        Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n","        \"\"\"\n","        newids = []\n","        i = 0\n","\n","        while i < len(ids):\n","            # if not at the very last position AND the pair matches, replace it\n","            if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n","                newids.append(idx)\n","                i += 2\n","            else:\n","                newids.append(ids[i])\n","                i += 1\n","\n","        return newids\n","\n","    def train(self, text, vocab_size, verbose=False):\n","        num_merges = vocab_size - 256\n","\n","        # input text preprocessing\n","        text_bytes = text.encode(\"utf-8\") # raw bytes\n","        ids = list(text_bytes) # list of integers in range 0..255\n","\n","        # iteratively merge the most common pairs to create new tokens\n","        merges = {} # (int, int) -> int\n","        vocab = {idx: bytes([idx]) for idx in range(256)} # int -> bytes\n","\n","        for i in range(num_merges):\n","            # count up the number of times every consecutive pair appears\n","            stats = self._get_stats(ids)\n","\n","            # find the pair with the highest count\n","            pair = max(stats, key=stats.get)\n","\n","            # mint a new token: assign it the next available id\n","            idx = 258 + i\n","\n","            # replace all occurrences of pair in ids with idx\n","            ids = self._merge(ids, pair, idx)\n","\n","            # save the merge\n","            merges[pair] = idx\n","            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n","\n","        # save class variables\n","        print(merges)\n","        self.merges = merges # used in encode()\n","        self.vocab = vocab   # used in decode()\n","\n","    def decode(self, ids):\n","        # given ids (list of integers), return Python string\n","        text_bytes = b\"\".join(self.vocab[idx] for idx in ids)\n","        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n","        return text\n","\n","    def encode(self, text):\n","        # given a string text, return the token ids\n","        text_bytes = text.encode(\"utf-8\") # raw bytes\n","        ids = list(text_bytes) # list of integers in range 0..255\n","\n","        while len(ids) >= 2:\n","            # find the pair with the lowest merge index\n","            stats = self._get_stats(ids)\n","            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n","\n","            # just the first pair in the list, arbitrarily\n","            # we can detect this terminating case by a membership check\n","            if pair not in self.merges:\n","                break # nothing else can be merged anymore\n","\n","            # otherwise let's merge the best pair (lowest merge index)\n","            idx = self.merges[pair]\n","            ids = self._merge(ids, pair, idx)\n","\n","        return ids\n","\n","    def visualize_tokenization(self, ids):\n","        \"\"\"Small helper function useful in debugging: visualize the tokenization of render_conversation\"\"\"\n","        RED = '\\033[91m'\n","        GREEN = '\\033[92m'\n","        RESET = '\\033[0m'\n","        GRAY = '\\033[90m'\n","\n","        tokens = []\n","        for token_id in ids:\n","            token_str = self.decode([token_id])\n","            tokens.append(f\"{GREEN}{token_str}{GRAY}({token_id}){RESET}\")\n","\n","            if token_str in ['<|SOS|>', '<|EOS|>']:\n","                tokens.append('\\n\\n\\t')\n","\n","        return ' | '.join(tokens)"],"metadata":{"id":"FMNlxMh4B7Eu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VO1XHopDquMT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_str = uts.get_file_as_string('zoomer.txt')\n","testing_lines = uts.get_file_as_list_strs('zoomer.txt')\n","\n","tokenizer = BPE_Tokenizer()\n","tokenizer.train(training_str, 512)\n","\n","#print(len(tokenizer.vocab))\n","#print(tokenizer.vocab)\n","\n","# tokenized_lines = []\n","# for line in testing_lines[:5]:\n","#     print(testing_lines[i])\n","\n","#     tokenized_str = tokenizer.encode(line.lower())\n","#     #tokenized_lines.append([tokenizer.decode([token]) for token in tokenized_str])\n","#     print(tokenizer.visualize_tokenization(tokenized_str))\n","\n","#     print('-' * 80)\n","#     print()\n","\n","tokenized_lines = []\n","for line in testing_lines:\n","    #print(testing_lines[i])\n","\n","    tokenized_str = tokenizer.encode(line.lower())\n","    tokenized_lines.append([tokenizer.decode([token]) for token in tokenized_str])\n","    #print(tokenizer.visualize_tokenization(tokenized_str))\n","\n","    #print('-' * 80)\n","    #print()\n","\n","num_gens = 3\n","\n","word_graph = uts.build_graph_word('zoomer.txt')\n","print('Word Level Generation [zoomer.txt]:')\n","for i in range(0, num_gens):\n","  print(f'\\t{i}: {' '.join(uts.generate_sequence(word_graph, 50))}')\n","\n","char_graph = uts.build_graph_char('zoomer.txt')\n","print('Character Level Generation [zoomer.txt]:')\n","for i in range(0, num_gens):\n","  print(f'\\t{i}: {''.join(uts.generate_sequence(char_graph, 50))}')\n","\n","token_graph = uts.build_graph_word(''.join(tokenized_lines))\n","print('Token Level Generation [zoomer.txt]:')\n","for i in range(0, num_gens):\n","  print(f'\\t{i}: {''.join(uts.generate_sequence(token_graph, 50))}')"],"metadata":{"id":"SQw48jfcDSL6","colab":{"base_uri":"https://localhost:8080/","height":550},"executionInfo":{"status":"error","timestamp":1768934143782,"user_tz":300,"elapsed":2679,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"b6297397-dfa1-48a7-aa8f-fd2103b54a0e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{(101, 32): 258, (116, 32): 259, (32, 116): 260, (44, 32): 261, (115, 32): 262, (226, 128): 263, (263, 153): 264, (105, 110): 265, (111, 117): 266, (104, 97): 267, (104, 258): 268, (265, 103): 269, (32, 97): 270, (101, 114): 271, (111, 114): 272, (111, 110): 273, (111, 32): 274, (260, 268): 275, (267, 259): 276, (121, 266): 277, (121, 32): 278, (101, 101): 279, (100, 32): 280, (108, 108): 281, (97, 110): 282, (73, 32): 283, (73, 264): 284, (264, 262): 285, (269, 32): 286, (105, 262): 287, (101, 110): 288, (119, 97): 289, (102, 272): 290, (101, 97): 291, (105, 116): 292, (259, 116): 293, (46, 84): 294, (109, 258): 295, (108, 97): 296, (103, 104): 297, (109, 32): 298, (101, 115): 299, (289, 262): 300, (32, 115): 301, (111, 119): 302, (281, 32): 303, (89, 266): 304, (101, 100): 305, (33, 84): 306, (103, 111): 307, (105, 115): 308, (284, 298): 309, (260, 111): 310, (105, 297): 311, (260, 274): 312, (118, 258): 313, (32, 119): 314, (260, 104): 315, (114, 258): 316, (117, 115): 317, (109, 101): 318, (32, 277): 319, (116, 97): 320, (104, 101): 321, (264, 259): 322, (270, 32): 323, (116, 285): 324, (116, 271): 325, (63, 32): 326, (97, 32): 327, (114, 111): 328, (46, 72): 329, (294, 276): 330, (109, 278): 331, (109, 97): 332, (99, 104): 333, (101, 261): 334, (116, 261): 335, (264, 316): 336, (116, 105): 337, (114, 32): 338, (260, 276): 339, (46, 283): 340, (107, 258): 341, (121, 261): 342, (97, 108): 343, (100, 97): 344, (97, 114): 345, (116, 104): 346, (111, 111): 347, (98, 117): 348, (110, 302): 349, (46, 304): 350, (105, 100): 351, (114, 97): 352, (99, 111): 353, (32, 290): 354, (110, 101): 355, (111, 118): 356, (109, 279): 357, (114, 291): 358, (99, 107): 359, (293, 268): 360, (46, 87): 361, (114, 101): 362, (115, 274): 363, (317, 259): 364, (108, 105): 365, (46, 73): 366, (277, 32): 367, (348, 259): 368, (306, 276): 369, (117, 112): 370, (103, 97): 371, (99, 282): 372, (110, 32): 373, (101, 99): 374, (292, 104): 375, (111, 102): 376, (105, 259): 377, (311, 116): 378, (110, 279): 379, (284, 303): 380, (315, 287): 381, (106, 364): 382, (101, 280): 383, (46, 83): 384, (112, 328): 385, (115, 116): 386, (117, 110): 387, (357, 116): 388, (105, 108): 389, (46, 309): 390, (101, 285): 391, (288, 100): 392, (104, 287): 393, (115, 261): 394, (273, 32): 395, (271, 32): 396, (353, 109): 397, (106, 111): 398, (267, 110): 399, (355, 119): 400, (379, 100): 401, (33, 304): 402, (33, 283): 403, (33, 87): 404, (101, 118): 405, (115, 97): 406, (115, 259): 407, (98, 279): 408, (108, 111): 409, (115, 117): 410, (119, 272): 411, (119, 279): 412, (99, 97): 413, (98, 266): 414, (33, 73): 415, (32, 100): 416, (267, 324): 417, (109, 356): 418, (293, 274): 419, (32, 265): 420, (102, 265): 421, (98, 101): 422, (107, 32): 423, (307, 259): 424, (105, 99): 425, (108, 101): 426, (114, 105): 427, (103, 101): 428, (112, 345): 429, (110, 111): 430, (111, 112): 431, (311, 259): 432, (273, 322): 433, (98, 258): 434, (412, 107): 435, (261, 368): 436, (267, 116): 437, (280, 295): 438, (79, 84): 439, (106, 374): 440, (116, 268): 441, (337, 273): 442, (385, 440): 443, (114, 299): 444, (110, 322): 445, (33, 32): 446, (291, 114): 447, (281, 278): 448, (418, 105): 449, (270, 414): 450, (277, 338): 451, (261, 283): 452, (98, 97): 453, (101, 120): 454, (284, 313): 455, (269, 261): 456, (344, 121): 457, (108, 32): 458, (33, 72): 459, (321, 285): 460, (112, 296): 461, (401, 312): 462, (76, 101): 463, (296, 325): 464, (308, 104): 465, (429, 116): 466, (33, 309): 467, (32, 287): 468, (104, 105): 469, (114, 432): 470, (296, 407): 471, (65, 70): 472, (266, 116): 473, (109, 272): 474, (32, 331): 475, (101, 108): 476, (119, 375): 477, (305, 275): 478, (296, 116): 479, (319, 32): 480, (347, 107): 481, (292, 285): 482, (104, 271): 483, (270, 110): 484, (261, 119): 485, (258, 115): 486, (310, 275): 487, (273, 378): 488, (112, 112): 489, (301, 111): 490, (108, 280): 491, (112, 108): 492, (408, 373): 493, (321, 108): 494, (314, 375): 495, (288, 259): 496, (267, 438): 497, (79, 77): 498, (294, 268): 499, (494, 112): 500, (71, 111): 501, (282, 32): 502, (388, 269): 503, (405, 271): 504, (435, 392): 505, (266, 297): 506, (101, 336): 507, (267, 313): 508, (400, 32): 509, (46, 67): 510, (104, 111): 511, (107, 101): 512, (421, 465): 513}\n","Word Level Generation [zoomer.txt]:\n","\t0: <SOS> You’re slaying that game was canceled. <EOS>\n","\t1: <SOS> I’m free later, K? <EOS>\n","\t2: <SOS> The presentation was awful, IFYP. <EOS>\n","Character Level Generation [zoomer.txt]:\n","\t0: <SOS>He. qul but bed tizzaie FC.<EOS>\n","\t1: <SOS>BME.<EOS>\n","\t2: <SOS>No wooviks ourons, It itpinonig TRBBThasle ce winy\n"]},{"output_type":"error","ename":"TypeError","evalue":"expected str, bytes or os.PathLike object, not list","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-686927892.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\t{i}: {'\u001b[0m\u001b[0;34m'.join(uts.generate_sequence(char_graph, 50))}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mtoken_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Token Level Generation [zoomer.txt]:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_gens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/nd-cse-10124-lectures/Datasets/utilities.py\u001b[0m in \u001b[0;36mbuild_graph_word\u001b[0;34m(source_file, graph)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_graph_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_file_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/nd-cse-10124-lectures/Datasets/utilities.py\u001b[0m in \u001b[0;36mget_file_as_list\u001b[0;34m(source_file)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_file_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Define a function named 'get_lines_str' that takes an argument called 'source_file'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Create a variable named 'lines' points to a list data structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# For each item in the variable 'f', set the variable named 'line' equal to it, one by one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<|sos|>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'<|eos|>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Add the content of the variable 'line' to the end of the list named 'lines' (after removing whitespace and newlines on either end)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not list"]}]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-022.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-021.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"LasfRRlwqKAD"}},{"cell_type":"code","source":["\"\"\"\n","Minimal (byte-level) Byte Pair Encoding tokenizer.\n","\n","Algorithmically follows along the GPT tokenizer:\n","https://github.com/openai/gpt-2/blob/master/src/encoder.py\n","\n","Unlike BasicTokenizer:\n","- RegexTokenizer handles an optional regex splitting pattern.\n","- RegexTokenizer handles optional special tokens.\n","\"\"\"\n","\n","import regex as re\n","\n","# the main GPT text split patterns, see\n","# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n","GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n","GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n","\n","\n","class RegexTokenizer():\n","    def __init__(self):\n","        self.merges = {} # (int, int) -> int\n","        self.special_tokens = {'<|sos|>': 256, '<|eos|>': 257} # str -> int, e.g. {'<|endoftext|>': 100257}\n","        self.vocab = {idx: bytes([idx]) for idx in range(256)} | {idx: special.encode(\"utf-8\") for special, idx in self.special_tokens.items()}\n","        #self.vocab = self._init_vocab() # int -> bytes\n","\n","        self.pattern = GPT4_SPLIT_PATTERN\n","        self.compiled_pattern = re.compile(self.pattern)\n","\n","    def _build_vocab(self):\n","        # vocab is simply and deterministically derived from merges\n","        vocab = {idx: bytes([idx]) for idx in range(256)} + {idx: special.encode(\"utf-8\") for special, idx in special_tokens.items()}\n","\n","        for special, idx in self.special_tokens.items():\n","            vocab[idx] = special.encode(\"utf-8\")\n","\n","        return vocab\n","\n","    def _get_stats(self, ids, counts=None):\n","        \"\"\"\n","        Given a list of integers, return a dictionary of counts of consecutive pairs\n","        Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n","        Optionally allows to update an existing dictionary of counts\n","        \"\"\"\n","        counts = {} if counts is None else counts\n","        for pair in zip(ids, ids[1:]): # iterate consecutive elements\n","            counts[pair] = counts.get(pair, 0) + 1\n","        return counts\n","\n","    def _merge(self, ids, pair, idx):\n","        \"\"\"\n","        In the list of integers (ids), replace all consecutive occurrences\n","        of pair with the new integer token idx\n","        Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n","        \"\"\"\n","        newids = []\n","        i = 0\n","        while i < len(ids):\n","            # if not at the very last position AND the pair matches, replace it\n","            if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n","                newids.append(idx)\n","                i += 2\n","            else:\n","                newids.append(ids[i])\n","                i += 1\n","        return newids\n","\n","    def chunk(self, text):\n","        return re.findall(self.compiled_pattern, text)\n","\n","    def train(self, text, max_vocab_size, verbose=False):\n","        pretrain_vocab_size = len(self.vocab)\n","        num_merges = max_vocab_size - pretrain_vocab_size\n","\n","        # split the text up into text chunks\n","        text_chunks = re.findall(self.compiled_pattern, text)\n","\n","        # input text preprocessing\n","        ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks]\n","\n","        # iteratively merge the most common pairs to create new tokens\n","        for i in range(num_merges):\n","            # count the number of times every consecutive pair appears\n","            stats = {}\n","            for chunk_ids in ids:\n","                # passing in stats will update it in place, adding up counts\n","                self._get_stats(chunk_ids, stats)\n","\n","            # find the pair with the highest count\n","            pair = max(stats, key=stats.get)\n","\n","            # mint a new token: assign it the next available id\n","            idx = pretrain_vocab_size + i\n","\n","            # replace all occurrences of pair in ids with idx\n","            ids = [self._merge(chunk_ids, pair, idx) for chunk_ids in ids]\n","\n","            # save the merge\n","            self.merges[pair] = idx\n","            self.vocab[idx] = self.vocab[pair[0]] + self.vocab[pair[1]]\n","\n","    def decode(self, ids):\n","        # given ids (list of integers), return Python string\n","        part_bytes = []\n","        for idx in ids:\n","            if idx in self.vocab:\n","                part_bytes.append(self.vocab[idx])\n","            else:\n","                part_bytes.append('<|unk|>')\n","\n","        text_bytes = b\"\".join(part_bytes)\n","        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n","\n","        return text\n","\n","    def _encode_chunk(self, text_bytes):\n","        # return the token ids\n","        # let's begin. first, convert all bytes to integers in range 0..255\n","        ids = list(text_bytes)\n","\n","        while len(ids) >= 2:\n","            # find the pair with the lowest merge index\n","            stats = self._get_stats(ids)\n","            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n","            # subtle: if there are no more merges available, the key will\n","            # result in an inf for every single pair, and the min will be\n","            # just the first pair in the list, arbitrarily\n","            # we can detect this terminating case by a membership check\n","            if pair not in self.merges:\n","                break # nothing else can be merged anymore\n","            # otherwise let's merge the best pair (lowest merge index)\n","            idx = self.merges[pair]\n","            ids = self._merge(ids, pair, idx)\n","\n","        return ids\n","\n","    def encode(self, text):\n","        # otherwise, we have to be careful with potential special tokens in text\n","        # we handle special tokens by splitting the text\n","        # based on the occurrence of any exact match with any of the special tokens\n","        # we can use re.split for this. note that surrounding the pattern with ()\n","        # makes it into a capturing group, so the special tokens will be included\n","        special_pattern = \"(\" + \"|\".join(re.escape(k) for k in self.special_tokens) + \")\"\n","        special_chunks = re.split(special_pattern, text)\n","\n","        # now all the special characters are separated from the rest of the text\n","        # all chunks of text are encoded separately, then results are joined\n","        ids = []\n","        for part in special_chunks[1:-1]:\n","            if part in self.special_token:\n","                # this is a special token, encode it separately as a special case\n","                ids.append(self.special_tokens[part])\n","            else:\n","                # this is an ordinary sequence, encode it normally\n","                text_chunks = re.findall(self.compiled_pattern, text)\n","\n","                for chunk in text_chunks:\n","                    chunk_bytes = chunk.encode(\"utf-8\") # raw bytes\n","                    chunk_ids = self._encode_chunk(chunk_bytes)\n","                    ids.extend(chunk_ids)\n","\n","        return ids\n","\n","    def visualize_tokenization(self, ids):\n","        \"\"\"Small helper function useful in debugging: visualize the tokenization of render_conversation\"\"\"\n","        RED = '\\033[91m'\n","        GREEN = '\\033[92m'\n","        RESET = '\\033[0m'\n","        GRAY = '\\033[90m'\n","\n","        tokens = []\n","        for token_id in ids:\n","            token_str = self.decode([token_id])\n","            tokens.append(f\"{GREEN}{token_str}{GRAY}({token_id}){RESET}\")\n","\n","        return ' | '.join(tokens)"],"metadata":{"id":"iwuSysCmCJRB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_text = uts.get_file_as_string('zoomer.txt')\n","testing_lines = uts.get_file_as_list_strs('zoomer.txt')\n","\n","regex_tokenizer = RegexTokenizer()\n","print(regex_tokenizer.chunk(''.join(testing_lines[:5])))\n","\n","# max_vocab_size = 512\n","# regex_tokenizer.train(training_text, max_vocab_size)\n","\n","# print(f'Trained Vocab Size: {len(regex_tokenizer.vocab)}')\n","\n","# for i in range(355, 360):\n","#     print('-' * 80)\n","#     print(testing_lines[i])\n","\n","#     tokenized_str = regex_tokenizer.encode(testing_lines[i])\n","#     print(regex_tokenizer.visualize_tokenization(tokenized_str))\n","\n","#save_markov_graph_to_json(build_graph_word(uts.get_lines_str('zoomer.txt')), \"zoomer_char_graph.json\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fV0k7aYjckGV","executionInfo":{"status":"ok","timestamp":1768928009845,"user_tz":300,"elapsed":6,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"317ac41a-a655-4c17-eb9a-d725ab8451ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Got', ' the', ' job', ' today', ',', ' big', ' W', '!I', ' forgot', ' my', ' wallet', ' at', ' home', ',', ' that', '’s', ' an', ' L', '.Your', ' tweet', ' got', ' ', '5', ' likes', ' and', ' ', '100', ' replies', ' calling', ' you', ' out', '.', ' L', ' +', ' ratio', '.That', ' meme', ' is', ' so', ' dank', '!That', ' phrase', ' is', ' so', ' cheugy', ',', ' no', ' one', ' says', ' that', ' anymore', '.']\n"]}]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-023.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"RmehSmB6KapC"}},{"cell_type":"code","source":["!pip install rustbpe\n","\n","# -----------------------------------------------------------------------------\n","# Tokenizer based on rustbpe + tiktoken combo\n","import rustbpe\n","import tiktoken\n","\n","SPECIAL_TOKENS = [\n","    \"<|sos|>\", # every document begins with the Start of Sequence (SOS) token that delimits documents\n","    \"<|soum|>\", # start of user message\n","    \"<|eoum|>\", # end of user message\n","    \"<|soam|>\", # start of assistant message\n","    \"<|eoam|>\", # end of assistant message\n","]\n","\n","SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,2}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n","\n","class RustBPETokenizer:\n","    def __init__(self, enc, bos_token):\n","        self.enc = enc\n","        self.bos_token_id = self.encode_special(bos_token)\n","\n","    @classmethod\n","    def train_from_iterator(cls, text_iterator, vocab_size):\n","        # 1) train using rustbpe\n","        tokenizer = rustbpe.Tokenizer()\n","\n","        # the special tokens are inserted later in __init__, we don't train them here\n","        vocab_size_no_special = vocab_size - len(SPECIAL_TOKENS)\n","        tokenizer.train_from_iterator(text_iterator, vocab_size_no_special, pattern=SPLIT_PATTERN)\n","\n","        # 2) construct the associated tiktoken encoding for inference\n","        pattern = tokenizer.get_pattern()\n","        mergeable_ranks_list = tokenizer.get_mergeable_ranks()\n","        mergeable_ranks = {bytes(k): v for k, v in mergeable_ranks_list}\n","        tokens_offset = len(mergeable_ranks)\n","        special_tokens = {name: tokens_offset + i for i, name in enumerate(SPECIAL_TOKENS)}\n","\n","        enc = tiktoken.Encoding(name=\"rustbpe\", pat_str=pattern, mergeable_ranks=mergeable_ranks, special_tokens=special_tokens)\n","\n","        return cls(enc, \"<|sos|>\")\n","\n","    def encode(self, text):\n","        return self.enc.encode_ordinary(text)\n","\n","    def encode_special(self, text):\n","        return self.enc.encode_single_token(text)\n","\n","    def decode(self, ids):\n","        return self.enc.decode(ids)\n","\n","    def render_conversation(self, conversation, max_tokens=2048):\n","        \"\"\"\n","        Tokenize a single Chat conversation (which we call a \"doc\" or \"document\" here).\n","        Returns:\n","        - ids: list[int] is a list of token ids of this rendered conversation\n","        \"\"\"\n","        messages = conversation[\"messages\"]\n","\n","        # now we can tokenize the conversation\n","        ids = [self.encode_special('<|sos|>')]\n","\n","        for i, message in enumerate(messages):\n","            content = message[\"content\"]\n","\n","            if message[\"role\"] == \"user\":\n","                ids += [self.encode_special('<|soum|>')] + self.encode(content) + [self.encode_special('<|eoum|>')]\n","\n","            elif message[\"role\"] == \"assistant\":\n","                ids += [self.encode_special('<|soam|>')] + self.encode(content) + [self.encode_special('<|eoam|>')]\n","\n","        # truncate to max_tokens tokens MAX (helps prevent OOMs)\n","        ids = ids[:max_tokens]\n","\n","        return ids\n","\n","    def visualize_tokenization(self, ids):\n","        \"\"\"Small helper function useful in debugging: visualize the tokenization of render_conversation\"\"\"\n","        RED = '\\033[91m'\n","        GREEN = '\\033[92m'\n","        RESET = '\\033[0m'\n","        GRAY = '\\033[90m'\n","\n","        tokens = []\n","        for i, token_id in enumerate(ids):\n","            token_str = self.decode([token_id])\n","            tokens.append(f\"{GREEN}{token_str}{GRAY}({token_id}){RESET}\")\n","\n","            if token_str in ['<|sos|>', '<|eoum|>', '<|eoam|>']:\n","                tokens.append('\\n\\n\\t')\n","\n","        return ' | '.join(tokens)"],"metadata":{"id":"nPVeFvJzd7d5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_text = uts.get_file_str('shakespeare.txt')\n","\n","nanochat_tokenizer = RustBPETokenizer.train_from_iterator(training_text, 256 + len(SPECIAL_TOKENS))\n","\n","conversation = {\n","    'messages': [\n","        {'role': 'user', 'content': 'Hello there general kenobi'},\n","        {'role': 'assistant', 'content': 'Hello there grevious'},\n","        {'role': 'user', 'content': 'I love neffer'},\n","    ]\n","}\n","\n","ids = nanochat_tokenizer.render_conversation(conversation)\n","print(nanochat_tokenizer.visualize_tokenization(ids))"],"metadata":{"id":"oO7H8-XJfg84"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Export to HTML\n","\n","Uncomment the final line of the cell below and run it to export this notebook to HTML"],"metadata":{"id":"xSLHa2zwsAlr"}},{"cell_type":"code","source":["import os, json\n","\n","def export_notebook():\n","  L_PATH = \"nd-cse-10124-lectures/Notebooks\"\n","  L = \"Lecture_03_Tokenization\"\n","\n","  try:\n","      from google.colab import _message, files\n","\n","      # where you WANT it to live (repo folder)\n","      repo_ipynb_path = f\"/content/{L_PATH}/{L}.ipynb\"\n","\n","      # grab current notebook contents from the UI\n","      nb = _message.blocking_request(\"get_ipynb\", timeout_sec=1)[\"ipynb\"]\n","\n","      # write it into the repo folder as a real file\n","      os.makedirs(os.path.dirname(repo_ipynb_path), exist_ok=True)\n","      with open(repo_ipynb_path, \"w\", encoding=\"utf-8\") as f:\n","          json.dump(nb, f)\n","\n","      # convert + download pdf\n","      !jupyter nbconvert --to html \"{repo_ipynb_path}\"\n","      files.download(repo_ipynb_path.replace(\".ipynb\", \".html\"))\n","  except:\n","      import subprocess\n","\n","      nb_fp = os.getcwd() + f'{L}.ipynb'\n","      print(os.getcwd())\n","\n","      subprocess.run([\"jupyter\", \"nbconvert\", \"--to\", \"html\", nb_fp], check=True)\n","\n","#export_notebook()"],"metadata":{"id":"1HCtuuZFr_oT"},"execution_count":null,"outputs":[]}]}