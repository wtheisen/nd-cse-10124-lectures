{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNcBMqfsbf51tgCeEwRKSj5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-001.png\" class=\"img-responsive\"/>\n","</div>\n","\n"],"metadata":{"id":"EUZY_VNVJVKT"}},{"cell_type":"code","source":["def count_chars(input_str):\n","    pass\n","\n","print(count_chars('hello there'))"],"metadata":{"id":"3Sj_A8jo8vbE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-002.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-003.png\" class=\"img-responsive\"/>\n","</div>\n"],"metadata":{"id":"mxAAF5GY8vxw"}},{"cell_type":"code","source":["def replace_pairs(lst, target_pair, combined_id):\n","    pass\n","\n","\n","print(replace_pairs([1, 2, 3, 1, 2], (1, 2), 4))"],"metadata":{"id":"58YQQupe_7UK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-004.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-005.png\" class=\"img-responsive\"/>\n","</div>\n","\n","\n"],"metadata":{"id":"5spfu41pJHGy"}},{"cell_type":"code","source":["animals = ('cat', 'dog')\n","print(type(animals))\n","print(animals)\n","print(animals[0])\n","#animals[0] = 'fish'"],"metadata":{"id":"rZCzpPqFiHWo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-006.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-007.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-008.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-009.png\" class=\"img-responsive\"/>\n","</div>\n","\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-010.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"end21iY4iISK"}},{"cell_type":"code","source":["import os\n","\n","try:\n","    import google.colab\n","    REPO_URL = \"https://github.com/wtheisen/nd-cse-10124-lectures.git\"\n","\n","    REPO_NAME = \"/content/nd-cse-10124-lectures\"\n","    L_PATH = \"nd-cse-10124-lectures/Datasets\"\n","\n","    %cd /content/\n","    !rm -r {REPO_NAME}\n","\n","    # Clone repo\n","    if not os.path.exists(REPO_NAME):\n","        !git clone {REPO_URL}\n","\n","        # cd into the data folder\n","        %cd {L_PATH}\n","        !pwd\n","\n","except ImportError:\n","    print(\"Unable to download repo, either:\")\n","    print(\"\\tA.) You're not on colab\")\n","    print(\"\\tB.) It has already been cloned\")\n","\n","\n","import utilities as uts"],"metadata":{"id":"XauIb-liOtSM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769099459960,"user_tz":300,"elapsed":2249,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"a1bddc5e-46f2-44fa-8976-795c9d32cf13"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","rm: cannot remove '/content/nd-cse-10124-lectures': No such file or directory\n","Cloning into 'nd-cse-10124-lectures'...\n","remote: Enumerating objects: 222, done.\u001b[K\n","remote: Counting objects: 100% (222/222), done.\u001b[K\n","remote: Compressing objects: 100% (154/154), done.\u001b[K\n","remote: Total 222 (delta 138), reused 150 (delta 66), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (222/222), 19.46 MiB | 36.56 MiB/s, done.\n","Resolving deltas: 100% (138/138), done.\n","/content/nd-cse-10124-lectures/Datasets\n","/content/nd-cse-10124-lectures/Datasets\n"]}]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-011.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"t2CMenMb7LMi"}},{"cell_type":"code","source":["print('Size of Jabberwocky Graph (Words):', len(uts.build_graph_word('jabberwocky.txt')))\n","print('Size of Zoomer Graph (Words):', len(uts.build_graph_word('zoomer.txt')))\n","print('Size of Shakespeare Graph (Words):', len(uts.build_graph_word('shakespeare.txt')))\n","print('#' * 80)\n","print('Size of Jabberwocky Graph (Chars):', len(uts.build_graph_char('jabberwocky.txt')))\n","print('Size of Shakespeare Graph (Chars):', len(uts.build_graph_char('shakespeare.txt')))\n","print('Size of Zoomer Graph (Chars):', len(uts.build_graph_char('zoomer.txt')))\n","print('#' * 80)"],"metadata":{"id":"KYk3jDIwd7_9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-012.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-013.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"lECVuBmReB2x"}},{"cell_type":"code","source":["print(list('hello there 金'.encode('utf-8')))\n","\n","print([bin(num) for num in list('hello there 金'.encode('utf-8'))])"],"metadata":{"id":"jJZ4kI897ZEk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-014.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-015.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-016.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-017.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"E1Zt1bds7c1i"}},{"cell_type":"code","source":["from collections import Counter\n","\n","def count_pairs(nums):\n","    pass\n","\n","prompt = 'rug pug hug pun bun hugs run gun'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":233},"id":"6IA77Zb5FEAr","executionInfo":{"status":"error","timestamp":1769099177287,"user_tz":300,"elapsed":7,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"95ed27a7-1bfd-474d-cff3-ed2bca3f8be2"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'tuple'>\n","('cat', 'dog')\n","cat\n"]},{"output_type":"error","ename":"TypeError","evalue":"'tuple' object does not support item assignment","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-859281834.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manimals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manimals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0manimals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'fish'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: 'tuple' object does not support item assignment"]}]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-018.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-019.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"y1rrw6gH0o5Q"}},{"cell_type":"code","source":["class Cat():\n","    def __init__(self, name, color):\n","        self.name = name\n","        self.color = color\n","\n","cute_kitty = Cat('roger', 'orange')\n","print(type(cute_kitty))"],"metadata":{"id":"G1IisgIj7feQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-020.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"OPdPw_Jieywk"}},{"cell_type":"code","source":["\"\"\"\n","Minimal (byte-level) Byte Pair Encoding tokenizer.\n","\n","Algorithmically follows along the GPT tokenizer:\n","https://github.com/openai/gpt-2/blob/master/src/encoder.py\n","\"\"\"\n","\n","class Simple_Tokenizer():\n","    def __init__(self):\n","        self.merges = {} # (int, int) -> int\n","\n","        self.special_tokens = {'<|sos|>': 256, '<|eos|>': 257} # str -> int, e.g. {'<|endoftext|>': 100257}\n","        self.vocab = {idx: bytes([idx]) for idx in range(256)} | {idx: special.encode(\"utf-8\") for special, idx in self.special_tokens.items()}\n","\n","    def _count_pairs(self, ids, counts=None):\n","        pass\n","\n","    def _merge_pairs(self, ids, pair, idx):\n","        pass\n","\n","    def train(self, text, max_vocab_size):\n","        pass\n","\n","    def decode(self, ids):\n","        pass\n","\n","    def encode(self, text):\n","        pass\n","\n","    def visualize_tokenization(self, ids):\n","        \"\"\"Small helper function useful in debugging: visualize the tokenization of render_conversation\"\"\"\n","        GREEN = '\\033[92m'\n","        RESET = '\\033[0m'\n","        GRAY = '\\033[90m'\n","\n","        tokens = []\n","        for token_id in ids:\n","            token_str = self.decode([token_id])\n","            tokens.append(f\"{GREEN}{token_str}{GRAY}({token_id}){RESET}\")\n","\n","            if token_str in ['<|sos|>', '<|eos|>']:\n","                tokens.append('\\n\\n\\t')\n","\n","        return ' | '.join(tokens)\n","\n","\"\"\"\n","Minimal (byte-level) Byte Pair Encoding tokenizer.\n","\n","Algorithmically follows along the GPT tokenizer:\n","https://github.com/openai/gpt-2/blob/master/src/encoder.py\n","\"\"\"\n","\n","class Simple_Tokenizer():\n","    def __init__(self):\n","        self.merges = {} # (int, int) -> int\n","\n","        self.special_tokens = {'<|sos|>': 256, '<|eos|>': 257} # str -> int, e.g. {'<|endoftext|>': 100257}\n","        self.vocab = {idx: bytes([idx]) for idx in range(256)} | {idx: special.encode(\"utf-8\") for special, idx in self.special_tokens.items()}\n","\n","    def _count_pairs(self, ids, counts=None):\n","        \"\"\"\n","        Given a list of integers, return a dictionary of counts of consecutive pairs\n","        Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n","        Optionally allows to update an existing dictionary of counts\n","        \"\"\"\n","        counts = {} if counts is None else counts\n","\n","        for pair in zip(ids, ids[1:]): # iterate consecutive elements\n","            counts[pair] = counts.get(pair, 0) + 1\n","\n","        return counts\n","\n","    def _merge_pairs(self, ids, pair, idx):\n","        \"\"\"\n","        In the list of integers (ids), replace all consecutive occurrences\n","        of pair with the new integer token idx\n","        Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n","        \"\"\"\n","        newids = []\n","        i = 0\n","\n","        while i < len(ids):\n","            # if not at the very last position AND the pair matches, replace it\n","            if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n","                newids.append(idx)\n","                i += 2\n","            else:\n","                newids.append(ids[i])\n","                i += 1\n","\n","        return newids\n","\n","    def train(self, text, max_vocab_size):\n","        vocab_size = len(self.vocab)\n","        num_merges = max_vocab_size - vocab_size\n","\n","        # input text preprocessing\n","        text_bytes = text.encode(\"utf-8\") # raw bytes\n","        ids = list(text_bytes) # list of integers in range 0..255\n","\n","        for i in range(num_merges):\n","            # count up the number of times every consecutive pair appears\n","            stats = self._count_pairs(ids)\n","\n","            # find the pair with the highest count\n","            pair = max(stats, key=stats.get)\n","\n","            # mint a new token: assign it the next available id\n","            idx = vocab_size + i\n","\n","            # replace all occurrences of pair in ids with idx\n","            ids = self._merge_pairs(ids, pair, idx)\n","\n","            # save the merge\n","            self.merges[pair] = idx\n","            self.vocab[idx] = self.vocab[pair[0]] + self.vocab[pair[1]]\n","\n","    def decode(self, ids):\n","        # given ids (list of integers), return Python string\n","        text_bytes = b\"\".join(self.vocab[idx] for idx in ids)\n","        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n","        return text\n","\n","    def encode(self, text):\n","        import re\n","        special_pattern = \"(\" + \"|\".join(re.escape(k) for k in self.special_tokens) + \")\"\n","        special_chunks = re.split(special_pattern, text)\n","\n","        encoded_str = []\n","        for chunk in special_chunks[1:-1]:\n","            if chunk in self.special_tokens:\n","                # this is a special token, encode it separately as a special case\n","                encoded_str.append(self.special_tokens[chunk])\n","            else:\n","                # given a string text, return the token ids\n","                text_bytes = chunk.encode(\"utf-8\") # raw bytes\n","                chunk_ids = list(text_bytes) # list of integers in range 0..255\n","\n","                while len(chunk_ids) >= 2:\n","                    # find the pair with the lowest merge index\n","                    counted_pairs = self._count_pairs(chunk_ids)\n","                    earliest_pair = min(counted_pairs, key=lambda p: self.merges.get(p, float(\"inf\")))\n","\n","                    # just the first pair in the list, arbitrarily\n","                    # we can detect this terminating case by a membership check\n","                    if earliest_pair not in self.merges:\n","                        break # nothing else can be merged anymore\n","\n","                    # otherwise let's merge the best pair (lowest merge index)\n","                    pair_idx = self.merges[earliest_pair]\n","                    chunk_ids = self._merge_pairs(chunk_ids, earliest_pair, pair_idx)\n","\n","                encoded_str += chunk_ids\n","\n","        return encoded_str\n","\n","    def visualize_tokenization(self, ids):\n","        \"\"\"Small helper function useful in debugging: visualize the tokenization of render_conversation\"\"\"\n","        GREEN = '\\033[92m'\n","        RESET = '\\033[0m'\n","        GRAY = '\\033[90m'\n","\n","        tokens = []\n","        for token_id in ids:\n","            token_str = self.decode([token_id])\n","            tokens.append(f\"{GREEN}{token_str}{GRAY}({token_id}){RESET}\")\n","\n","        return ' | '.join(tokens)"],"metadata":{"id":"cSR_No12OBEj","executionInfo":{"status":"ok","timestamp":1769099989150,"user_tz":300,"elapsed":12,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["#print('Size of Jabberwocky Graph (Tokens):', len(uts.create_token_graph('jabberwocky.txt')))\n","#print('Size of Shakespeare Graph (Tokens):', len(uts.create_token_graph('shakespeare.txt')))\n","# Size of Shakespeare Graph (Tokens): 286 (this takes like ~7 minutes to run)\n","\n","num_gens = 3\n","\n","char_graph = uts.build_graph_char('zoomer.txt')\n","print('Size of Zoomer Graph (Characters):', len(char_graph))\n","print('Character Level Generation [zoomer.txt]:')\n","for i in range(0, num_gens):\n","  print(f'\\t{i}: {''.join(uts.generate_sequence(char_graph, 50))}')\n","\n","print('#' * 80)\n","\n","for vocab_size in [258, 512, 1024, 2048, 4096]:\n","    s_t = Simple_Tokenizer()\n","\n","    token_graph = uts.build_graph_token('zoomer.txt', s_t, vocab_size=vocab_size)\n","    print(s_t.visualize_tokenization(s_t.encode(\"<|sos|>No cap are u rolling the party tonight?<|eos|>\")))\n","    # print('Size of Zoomer Graph (Tokens, No Regex):', len(token_graph))\n","\n","    # print(f'Token Level Generation (No Regex, {vocab_size} Vocab Size) [zoomer.txt]:')\n","    # for i in range(0, num_gens):\n","    #     print(f'\\t{i}: {''.join(uts.generate_sequence(token_graph, 50))}')\n","    # print('#' * 80)\n","\n","word_graph = uts.build_graph_word('zoomer.txt', special_tokens=True)\n","print('Size of Zoomer Graph (Words):', len(word_graph))\n","print('Word Level Generation [zoomer.txt]:')\n","for i in range(0, num_gens):\n","  print(f'\\t{i}: {' '.join(uts.generate_sequence(word_graph, 50))}')\n"],"metadata":{"id":"-0aQhw4u1u0w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769100071409,"user_tz":300,"elapsed":79725,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"d9283a3d-e2a0-4605-e776-c87c024c0caf"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Size of Zoomer Graph (Characters): 90\n","Character Level Generation [zoomer.txt]:\n","\t0: <|sos|>Goulpplimed whos lg meract.<|eos|>\n","\t1: <|sos|>Wes I’she’meye s u chadilat fut fork, bet sere was\n","\t2: <|sos|>L.<|eos|>\n","################################################################################\n","\u001b[92m<|sos|>\u001b[90m(256)\u001b[0m | \u001b[92mN\u001b[90m(78)\u001b[0m | \u001b[92mo\u001b[90m(111)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92mc\u001b[90m(99)\u001b[0m | \u001b[92ma\u001b[90m(97)\u001b[0m | \u001b[92mp\u001b[90m(112)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92ma\u001b[90m(97)\u001b[0m | \u001b[92mr\u001b[90m(114)\u001b[0m | \u001b[92me\u001b[90m(101)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92mu\u001b[90m(117)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92mr\u001b[90m(114)\u001b[0m | \u001b[92mo\u001b[90m(111)\u001b[0m | \u001b[92ml\u001b[90m(108)\u001b[0m | \u001b[92ml\u001b[90m(108)\u001b[0m | \u001b[92mi\u001b[90m(105)\u001b[0m | \u001b[92mn\u001b[90m(110)\u001b[0m | \u001b[92mg\u001b[90m(103)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92mt\u001b[90m(116)\u001b[0m | \u001b[92mh\u001b[90m(104)\u001b[0m | \u001b[92me\u001b[90m(101)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92mp\u001b[90m(112)\u001b[0m | \u001b[92ma\u001b[90m(97)\u001b[0m | \u001b[92mr\u001b[90m(114)\u001b[0m | \u001b[92mt\u001b[90m(116)\u001b[0m | \u001b[92my\u001b[90m(121)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92mt\u001b[90m(116)\u001b[0m | \u001b[92mo\u001b[90m(111)\u001b[0m | \u001b[92mn\u001b[90m(110)\u001b[0m | \u001b[92mi\u001b[90m(105)\u001b[0m | \u001b[92mg\u001b[90m(103)\u001b[0m | \u001b[92mh\u001b[90m(104)\u001b[0m | \u001b[92mt\u001b[90m(116)\u001b[0m | \u001b[92m?\u001b[90m(63)\u001b[0m | \u001b[92m<|eos|>\u001b[90m(257)\u001b[0m\n","\u001b[92m<|sos|>\u001b[90m(256)\u001b[0m | \u001b[92mN\u001b[90m(78)\u001b[0m | \u001b[92mo \u001b[90m(274)\u001b[0m | \u001b[92mca\u001b[90m(413)\u001b[0m | \u001b[92mp\u001b[90m(112)\u001b[0m | \u001b[92m a\u001b[90m(270)\u001b[0m | \u001b[92mre \u001b[90m(316)\u001b[0m | \u001b[92mu\u001b[90m(117)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92mro\u001b[90m(328)\u001b[0m | \u001b[92mll\u001b[90m(281)\u001b[0m | \u001b[92ming\u001b[90m(269)\u001b[0m | \u001b[92m the \u001b[90m(275)\u001b[0m | \u001b[92mpart\u001b[90m(466)\u001b[0m | \u001b[92my\u001b[90m(121)\u001b[0m | \u001b[92m t\u001b[90m(260)\u001b[0m | \u001b[92monight\u001b[90m(488)\u001b[0m | \u001b[92m?\u001b[90m(63)\u001b[0m | \u001b[92m<|eos|>\u001b[90m(257)\u001b[0m\n","\u001b[92m<|sos|>\u001b[90m(256)\u001b[0m | \u001b[92mN\u001b[90m(78)\u001b[0m | \u001b[92mo \u001b[90m(274)\u001b[0m | \u001b[92mca\u001b[90m(413)\u001b[0m | \u001b[92mp\u001b[90m(112)\u001b[0m | \u001b[92m a\u001b[90m(270)\u001b[0m | \u001b[92mre \u001b[90m(316)\u001b[0m | \u001b[92mu\u001b[90m(117)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92mro\u001b[90m(328)\u001b[0m | \u001b[92mll\u001b[90m(281)\u001b[0m | \u001b[92ming the \u001b[90m(587)\u001b[0m | \u001b[92mparty\u001b[90m(710)\u001b[0m | \u001b[92m tonight\u001b[90m(550)\u001b[0m | \u001b[92m?\u001b[90m(63)\u001b[0m | \u001b[92m<|eos|>\u001b[90m(257)\u001b[0m\n","\u001b[92m<|sos|>\u001b[90m(256)\u001b[0m | \u001b[92mN\u001b[90m(78)\u001b[0m | \u001b[92mo \u001b[90m(274)\u001b[0m | \u001b[92mca\u001b[90m(413)\u001b[0m | \u001b[92mp\u001b[90m(112)\u001b[0m | \u001b[92m a\u001b[90m(270)\u001b[0m | \u001b[92mre \u001b[90m(316)\u001b[0m | \u001b[92mu\u001b[90m(117)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92mro\u001b[90m(328)\u001b[0m | \u001b[92mll\u001b[90m(281)\u001b[0m | \u001b[92ming the \u001b[90m(587)\u001b[0m | \u001b[92mparty tonight\u001b[90m(1658)\u001b[0m | \u001b[92m?\u001b[90m(63)\u001b[0m | \u001b[92m<|eos|>\u001b[90m(257)\u001b[0m\n","\u001b[92m<|sos|>\u001b[90m(256)\u001b[0m | \u001b[92mN\u001b[90m(78)\u001b[0m | \u001b[92mo \u001b[90m(274)\u001b[0m | \u001b[92mca\u001b[90m(413)\u001b[0m | \u001b[92mp\u001b[90m(112)\u001b[0m | \u001b[92m a\u001b[90m(270)\u001b[0m | \u001b[92mre \u001b[90m(316)\u001b[0m | \u001b[92mu\u001b[90m(117)\u001b[0m | \u001b[92m \u001b[90m(32)\u001b[0m | \u001b[92mroll\u001b[90m(2086)\u001b[0m | \u001b[92ming the \u001b[90m(587)\u001b[0m | \u001b[92mparty tonight\u001b[90m(1658)\u001b[0m | \u001b[92m?\u001b[90m(63)\u001b[0m | \u001b[92m<|eos|>\u001b[90m(257)\u001b[0m\n","Size of Zoomer Graph (Words): 3698\n","Word Level Generation [zoomer.txt]:\n","\t0: <|sos|> That party is amazing, GGMSOT! <|eos|>\n","\t1: <|sos|> I’m unarmed, PDS! <|eos|>\n","\t2: <|sos|> Friends 4EVA! <|eos|>\n"]}]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-021.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-022.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"qg5A-_onfRLz"}},{"cell_type":"code","source":["from tokenizer import RegexTokenizer\n","\n","r_t = RegexTokenizer()\n","\n","print(r_t.GPT4_SPLIT_PATTERN)\n","\n","print(r_t.chunk(\"No cap are u rolling the party tonight?\"))\n","\n","for vocab_size in [512, 1024, 2048, 4096]:\n","    r_t = RegexTokenizer()\n","\n","    token_graph = uts.build_graph_token('zoomer.txt', r_t, vocab_size=vocab_size)\n","    print('#' * 80)\n","    print(f'''Size of Zoomer Graph (Tokens, Regex, {vocab_size} Vocab Size):', len(token_graph))\n","\n","Token Level Generation (Regex, {vocab_size} Vocab Size) [zoomer.txt]:''')\n","\n","    for i in range(0, num_gens):\n","        print(f'\\t{i}: {''.join(uts.generate_sequence(token_graph, 50))}')"],"metadata":{"id":"dv3sQu75KJyT","colab":{"base_uri":"https://localhost:8080/","height":251},"executionInfo":{"status":"error","timestamp":1769099468592,"user_tz":300,"elapsed":52,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"077aa115-aba5-4976-8b01-c1a82fbaf42d"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\n","['No', ' cap', ' are', ' u', ' rolling', ' the', ' party', ' tonight', '?']\n"]},{"output_type":"error","ename":"TypeError","evalue":"build_graph_token() got an unexpected keyword argument 'special_tokens'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-936983487.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mr_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegexTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtoken_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'zoomer.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'#'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     print(f'''Size of Zoomer Graph (Tokens, Regex, {vocab_size} Vocab Size):', len(token_graph))\n","\u001b[0;31mTypeError\u001b[0m: build_graph_token() got an unexpected keyword argument 'special_tokens'"]}]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-024.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"Xs-8GO8-fZIp"}},{"cell_type":"markdown","source":["## Export to HTML\n","\n","Uncomment the final line of the cell below and run it to export this notebook to HTML"],"metadata":{"id":"xSLHa2zwsAlr"}},{"cell_type":"code","source":["import os, json\n","\n","def export_notebook():\n","  L_PATH = \"nd-cse-10124-lectures/Notebooks\"\n","  L = \"Lecture_04_Tokenization\"\n","\n","  try:\n","      from google.colab import _message, files\n","\n","      # where you WANT it to live (repo folder)\n","      repo_ipynb_path = f\"/content/{L_PATH}/{L}.ipynb\"\n","\n","      # grab current notebook contents from the UI\n","      nb = _message.blocking_request(\"get_ipynb\", timeout_sec=1)[\"ipynb\"]\n","\n","      # write it into the repo folder as a real file\n","      os.makedirs(os.path.dirname(repo_ipynb_path), exist_ok=True)\n","      with open(repo_ipynb_path, \"w\", encoding=\"utf-8\") as f:\n","          json.dump(nb, f)\n","\n","      # convert + download pdf\n","      !jupyter nbconvert --to html \"{repo_ipynb_path}\"\n","      files.download(repo_ipynb_path.replace(\".ipynb\", \".html\"))\n","  except:\n","      import subprocess\n","\n","      nb_fp = os.getcwd() + f'{L}.ipynb'\n","      print(os.getcwd())\n","\n","      subprocess.run([\"jupyter\", \"nbconvert\", \"--to\", \"html\", nb_fp], check=True)\n","\n","#export_notebook()"],"metadata":{"id":"1HCtuuZFr_oT"},"execution_count":null,"outputs":[]}]}