{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPM5HlkLcZtQ7w1aNT1haOW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-001.png\" class=\"img-responsive\"/>\n","</div>\n","\n"],"metadata":{"id":"EUZY_VNVJVKT"}},{"cell_type":"code","source":["def count_chars(input_str):\n","    pass\n","\n","count_chars('hello there')"],"metadata":{"id":"3Sj_A8jo8vbE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-002.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-003.png\" class=\"img-responsive\"/>\n","</div>\n"],"metadata":{"id":"mxAAF5GY8vxw"}},{"cell_type":"code","source":["def replace_pairs(lst, pair, idx):\n","    pass\n","\n","print(replace_pairs([1, 2, 3, 1, 2, 3], (1, 2), 4))"],"metadata":{"id":"58YQQupe_7UK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-004.png\" class=\"img-responsive\"/>\n","</div>\n","\n","\n","\n"],"metadata":{"id":"5spfu41pJHGy"}},{"cell_type":"code","source":["import os\n","\n","try:\n","    import google.colab\n","    REPO_URL = \"https://github.com/wtheisen/nd-cse-10124-lectures.git\"\n","\n","    REPO_NAME = \"/content/nd-cse-10124-lectures\"\n","    L_PATH = \"nd-cse-10124-lectures/Datasets\"\n","\n","    %cd /content/\n","    !rm -r {REPO_NAME}\n","\n","    # Clone repo\n","    if not os.path.exists(REPO_NAME):\n","        !git clone {REPO_URL}\n","\n","        # cd into the data folder\n","        %cd {L_PATH}\n","        !pwd\n","\n","except ImportError:\n","    print(\"Unable to download repo, either:\")\n","    print(\"\\tA.) You're not on colab\")\n","    print(\"\\tB.) It has already been cloned\")\n","\n","\n","import utilities as uts"],"metadata":{"id":"XauIb-liOtSM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Size of Jabberwocky Graph (Words):', len(uts.build_graph_word('jabberwocky.txt')))\n","print('Size of Zoomer Graph (Words):', len(uts.build_graph_word('zoomer.txt')))\n","print('Size of Shakespeare Graph (Words):', len(uts.build_graph_word('shakespeare.txt')))\n","print('#' * 80)\n","print('Size of Jabberwocky Graph (Chars):', len(uts.build_graph_char('jabberwocky.txt')))\n","print('Size of Shakespeare Graph (Chars):', len(uts.build_graph_char('shakespeare.txt')))\n","print('Size of Zoomer Graph (Chars):', len(uts.build_graph_char('zoomer.txt')))\n","print('#' * 80)"],"metadata":{"id":"QUGJIkSao3om"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Minimal (byte-level) Byte Pair Encoding tokenizer.\n","\n","Algorithmically follows along the GPT tokenizer:\n","https://github.com/openai/gpt-2/blob/master/src/encoder.py\n","\"\"\"\n","\n","class Simple_Tokenizer():\n","    def __init__(self):\n","        self.merges = {} # (int, int) -> int\n","\n","        vocab = {idx: bytes([idx]) for idx in range(256)} # int -> bytes\n","\n","\n","    def _get_stats(self, ids, counts=None):\n","        \"\"\"\n","        Given a list of integers, return a dictionary of counts of consecutive pairs\n","        Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n","        Optionally allows to update an existing dictionary of counts\n","        \"\"\"\n","        counts = {} if counts is None else counts\n","\n","        for pair in zip(ids, ids[1:]): # iterate consecutive elements\n","            counts[pair] = counts.get(pair, 0) + 1\n","\n","        return counts\n","\n","    def _merge(self, ids, pair, idx):\n","        \"\"\"\n","        In the list of integers (ids), replace all consecutive occurrences\n","        of pair with the new integer token idx\n","        Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n","        \"\"\"\n","        newids = []\n","        i = 0\n","\n","        while i < len(ids):\n","            # if not at the very last position AND the pair matches, replace it\n","            if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n","                newids.append(idx)\n","                i += 2\n","            else:\n","                newids.append(ids[i])\n","                i += 1\n","\n","        return newids\n","\n","    def train(self, text, vocab_size, verbose=False):\n","        num_merges = vocab_size - 258\n","\n","        # input text preprocessing\n","        text_bytes = text.encode(\"utf-8\") # raw bytes\n","        ids = list(text_bytes) # list of integers in range 0..255\n","\n","        for i in range(num_merges):\n","            # count up the number of times every consecutive pair appears\n","            stats = self._get_stats(ids)\n","\n","            # find the pair with the highest count\n","            pair = max(stats, key=stats.get)\n","\n","            # mint a new token: assign it the next available id\n","            idx = 258 + i\n","\n","            # replace all occurrences of pair in ids with idx\n","            ids = self._merge(ids, pair, idx)\n","\n","            # save the merge\n","            self.merges[pair] = idx\n","            self.vocab[idx] = self.vocab[pair[0]] + self.vocab[pair[1]]\n","\n","    def decode(self, ids):\n","        # given ids (list of integers), return Python string\n","        text_bytes = b\"\".join(self.vocab[idx] for idx in ids)\n","        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n","        return text\n","\n","    def encode(self, text):\n","        # given a string text, return the token ids\n","        text_bytes = text.encode(\"utf-8\") # raw bytes\n","        ids = list(text_bytes) # list of integers in range 0..255\n","\n","        while len(ids) >= 2:\n","            # find the pair with the lowest merge index\n","            stats = self._get_stats(ids)\n","            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n","\n","            # just the first pair in the list, arbitrarily\n","            # we can detect this terminating case by a membership check\n","            if pair not in self.merges:\n","                break # nothing else can be merged anymore\n","\n","            # otherwise let's merge the best pair (lowest merge index)\n","            idx = self.merges[pair]\n","            ids = self._merge(ids, pair, idx)\n","\n","        return ids\n","\n","    def visualize_tokenization(self, ids):\n","        \"\"\"Small helper function useful in debugging: visualize the tokenization of render_conversation\"\"\"\n","        RED = '\\033[91m'\n","        GREEN = '\\033[92m'\n","        RESET = '\\033[0m'\n","        GRAY = '\\033[90m'\n","\n","        tokens = []\n","        for token_id in ids:\n","            token_str = self.decode([token_id])\n","            tokens.append(f\"{GREEN}{token_str}{GRAY}({token_id}){RESET}\")\n","\n","            if token_str in ['<|SOS|>', '<|EOS|>']:\n","                tokens.append('\\n\\n\\t')\n","\n","        return ' | '.join(tokens)"],"metadata":{"id":"cSR_No12OBEj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tokenizer import Simple_Tokenizer\n","\n","def create_token_graph(file_name):\n","    training_str = uts.get_file_as_string(file_name)\n","    testing_lines = uts.get_file_as_list_strs(file_name)\n","\n","    tokenizer = Simple_Tokenizer()\n","    tokenizer.train(training_str, 512)\n","\n","    tokenized_lines = []\n","    for line in testing_lines:\n","        tokenized_str = tokenizer.encode(line.lower())\n","        tokenized_lines.append([tokenizer.decode([token]) for token in tokenized_str])\n","\n","    token_graph = uts.build_graph_word(tokenized_lines, file=False)\n","\n","    return token_graph\n","\n","print('Size of Jabberwocky Graph (Tokens):', len(create_token_graph('jabberwocky.txt')))\n","#print('Size of Shakespeare Graph (Tokens):', len(create_token_graph('shakespeare.txt')))\n","\n","token_graph = create_token_graph('zoomer.txt')\n","print(token_graph)\n","print('Size of Zoomer Graph (Tokens):', len(token_graph))\n","print('#' * 80)\n","\n","num_gens = 3\n","\n","word_graph = uts.build_graph_word('zoomer.txt')\n","print('Word Level Generation [zoomer.txt]:')\n","for i in range(0, num_gens):\n","  print(f'\\t{i}: {' '.join(uts.generate_sequence(word_graph, 50))}')\n","\n","print('#' * 80)\n","\n","char_graph = uts.build_graph_char('zoomer.txt')\n","print('Character Level Generation [zoomer.txt]:')\n","for i in range(0, num_gens):\n","  print(f'\\t{i}: {''.join(uts.generate_sequence(char_graph, 50))}')\n","\n","print('#' * 80)\n","\n","print('Token Level Generation [zoomer.txt]:')\n","for i in range(0, num_gens):\n","  print(f'\\t{i}: {''.join(uts.generate_sequence(token_graph, 50))}')"],"metadata":{"id":"-0aQhw4u1u0w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-014.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-016.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-017.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-018.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-019.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-020.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"-JW9ueYDH-tv"}},{"cell_type":"code","source":["class Cat():\n","    def __init__(self, color):\n","        self.color = color\n","\n","    # TODO"],"metadata":{"id":"dv3sQu75KJyT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-019.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"hIgpsgViq_tP"}},{"cell_type":"code","source":[],"metadata":{"id":"VO1XHopDquMT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-022.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-021.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"LasfRRlwqKAD"}},{"cell_type":"code","source":["\"\"\"\n","Minimal (byte-level) Byte Pair Encoding tokenizer.\n","\n","Algorithmically follows along the GPT tokenizer:\n","https://github.com/openai/gpt-2/blob/master/src/encoder.py\n","\n","Unlike BasicTokenizer:\n","- RegexTokenizer handles an optional regex splitting pattern.\n","- RegexTokenizer handles optional special tokens.\n","\"\"\"\n","\n","import regex as re\n","\n","# the main GPT text split patterns, see\n","# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n","GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n","GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n","\n","\n","class RegexTokenizer():\n","    def __init__(self):\n","        self.merges = {} # (int, int) -> int\n","        self.special_tokens = {'<|sos|>': 256, '<|eos|>': 257} # str -> int, e.g. {'<|endoftext|>': 100257}\n","        self.vocab = {idx: bytes([idx]) for idx in range(256)} | {idx: special.encode(\"utf-8\") for special, idx in self.special_tokens.items()}\n","        #self.vocab = self._init_vocab() # int -> bytes\n","\n","        self.pattern = GPT4_SPLIT_PATTERN\n","        self.compiled_pattern = re.compile(self.pattern)\n","\n","    def _build_vocab(self):\n","        # vocab is simply and deterministically derived from merges\n","        vocab = {idx: bytes([idx]) for idx in range(256)} + {idx: special.encode(\"utf-8\") for special, idx in special_tokens.items()}\n","\n","        for special, idx in self.special_tokens.items():\n","            vocab[idx] = special.encode(\"utf-8\")\n","\n","        return vocab\n","\n","    def _get_stats(self, ids, counts=None):\n","        \"\"\"\n","        Given a list of integers, return a dictionary of counts of consecutive pairs\n","        Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n","        Optionally allows to update an existing dictionary of counts\n","        \"\"\"\n","        counts = {} if counts is None else counts\n","        for pair in zip(ids, ids[1:]): # iterate consecutive elements\n","            counts[pair] = counts.get(pair, 0) + 1\n","        return counts\n","\n","    def _merge(self, ids, pair, idx):\n","        \"\"\"\n","        In the list of integers (ids), replace all consecutive occurrences\n","        of pair with the new integer token idx\n","        Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n","        \"\"\"\n","        newids = []\n","        i = 0\n","        while i < len(ids):\n","            # if not at the very last position AND the pair matches, replace it\n","            if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n","                newids.append(idx)\n","                i += 2\n","            else:\n","                newids.append(ids[i])\n","                i += 1\n","        return newids\n","\n","    def chunk(self, text):\n","        return re.findall(self.compiled_pattern, text)\n","\n","    def train(self, text, max_vocab_size, verbose=False):\n","        pretrain_vocab_size = len(self.vocab)\n","        num_merges = max_vocab_size - pretrain_vocab_size\n","\n","        # split the text up into text chunks\n","        text_chunks = re.findall(self.compiled_pattern, text)\n","\n","        # input text preprocessing\n","        ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks]\n","\n","        # iteratively merge the most common pairs to create new tokens\n","        for i in range(num_merges):\n","            # count the number of times every consecutive pair appears\n","            stats = {}\n","            for chunk_ids in ids:\n","                # passing in stats will update it in place, adding up counts\n","                self._get_stats(chunk_ids, stats)\n","\n","            # find the pair with the highest count\n","            pair = max(stats, key=stats.get)\n","\n","            # mint a new token: assign it the next available id\n","            idx = pretrain_vocab_size + i\n","\n","            # replace all occurrences of pair in ids with idx\n","            ids = [self._merge(chunk_ids, pair, idx) for chunk_ids in ids]\n","\n","            # save the merge\n","            self.merges[pair] = idx\n","            self.vocab[idx] = self.vocab[pair[0]] + self.vocab[pair[1]]\n","\n","    def decode(self, ids):\n","        # given ids (list of integers), return Python string\n","        part_bytes = []\n","        for idx in ids:\n","            if idx in self.vocab:\n","                part_bytes.append(self.vocab[idx])\n","            else:\n","                part_bytes.append('<|unk|>')\n","\n","        text_bytes = b\"\".join(part_bytes)\n","        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n","\n","        return text\n","\n","    def _encode_chunk(self, text_bytes):\n","        # return the token ids\n","        # let's begin. first, convert all bytes to integers in range 0..255\n","        ids = list(text_bytes)\n","\n","        while len(ids) >= 2:\n","            # find the pair with the lowest merge index\n","            stats = self._get_stats(ids)\n","            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n","            # subtle: if there are no more merges available, the key will\n","            # result in an inf for every single pair, and the min will be\n","            # just the first pair in the list, arbitrarily\n","            # we can detect this terminating case by a membership check\n","            if pair not in self.merges:\n","                break # nothing else can be merged anymore\n","            # otherwise let's merge the best pair (lowest merge index)\n","            idx = self.merges[pair]\n","            ids = self._merge(ids, pair, idx)\n","\n","        return ids\n","\n","    def encode(self, text):\n","        # otherwise, we have to be careful with potential special tokens in text\n","        # we handle special tokens by splitting the text\n","        # based on the occurrence of any exact match with any of the special tokens\n","        # we can use re.split for this. note that surrounding the pattern with ()\n","        # makes it into a capturing group, so the special tokens will be included\n","        special_pattern = \"(\" + \"|\".join(re.escape(k) for k in self.special_tokens) + \")\"\n","        special_chunks = re.split(special_pattern, text)\n","\n","        # now all the special characters are separated from the rest of the text\n","        # all chunks of text are encoded separately, then results are joined\n","        ids = []\n","        for part in special_chunks[1:-1]:\n","            if part in self.special_token:\n","                # this is a special token, encode it separately as a special case\n","                ids.append(self.special_tokens[part])\n","            else:\n","                # this is an ordinary sequence, encode it normally\n","                text_chunks = re.findall(self.compiled_pattern, text)\n","\n","                for chunk in text_chunks:\n","                    chunk_bytes = chunk.encode(\"utf-8\") # raw bytes\n","                    chunk_ids = self._encode_chunk(chunk_bytes)\n","                    ids.extend(chunk_ids)\n","\n","        return ids\n","\n","    def visualize_tokenization(self, ids):\n","        \"\"\"Small helper function useful in debugging: visualize the tokenization of render_conversation\"\"\"\n","        RED = '\\033[91m'\n","        GREEN = '\\033[92m'\n","        RESET = '\\033[0m'\n","        GRAY = '\\033[90m'\n","\n","        tokens = []\n","        for token_id in ids:\n","            token_str = self.decode([token_id])\n","            tokens.append(f\"{GREEN}{token_str}{GRAY}({token_id}){RESET}\")\n","\n","        return ' | '.join(tokens)"],"metadata":{"id":"iwuSysCmCJRB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_text = uts.get_file_as_string('zoomer.txt')\n","testing_lines = uts.get_file_as_list_strs('zoomer.txt')\n","\n","regex_tokenizer = RegexTokenizer()\n","print(regex_tokenizer.chunk(''.join(testing_lines[:5])))\n","\n","# max_vocab_size = 512\n","# regex_tokenizer.train(training_text, max_vocab_size)\n","\n","# print(f'Trained Vocab Size: {len(regex_tokenizer.vocab)}')\n","\n","# for i in range(355, 360):\n","#     print('-' * 80)\n","#     print(testing_lines[i])\n","\n","#     tokenized_str = regex_tokenizer.encode(testing_lines[i])\n","#     print(regex_tokenizer.visualize_tokenization(tokenized_str))\n","\n","#save_markov_graph_to_json(build_graph_word(uts.get_lines_str('zoomer.txt')), \"zoomer_char_graph.json\")"],"metadata":{"id":"fV0k7aYjckGV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture03/slide-023.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"RmehSmB6KapC"}},{"cell_type":"code","source":["!pip install rustbpe\n","\n","# -----------------------------------------------------------------------------\n","# Tokenizer based on rustbpe + tiktoken combo\n","import rustbpe\n","import tiktoken\n","\n","SPECIAL_TOKENS = [\n","    \"<|sos|>\", # every document begins with the Start of Sequence (SOS) token that delimits documents\n","    \"<|soum|>\", # start of user message\n","    \"<|eoum|>\", # end of user message\n","    \"<|soam|>\", # start of assistant message\n","    \"<|eoam|>\", # end of assistant message\n","]\n","\n","SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,2}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n","\n","class RustBPETokenizer:\n","    def __init__(self, enc, bos_token):\n","        self.enc = enc\n","        self.bos_token_id = self.encode_special(bos_token)\n","\n","    @classmethod\n","    def train_from_iterator(cls, text_iterator, vocab_size):\n","        # 1) train using rustbpe\n","        tokenizer = rustbpe.Tokenizer()\n","\n","        # the special tokens are inserted later in __init__, we don't train them here\n","        vocab_size_no_special = vocab_size - len(SPECIAL_TOKENS)\n","        tokenizer.train_from_iterator(text_iterator, vocab_size_no_special, pattern=SPLIT_PATTERN)\n","\n","        # 2) construct the associated tiktoken encoding for inference\n","        pattern = tokenizer.get_pattern()\n","        mergeable_ranks_list = tokenizer.get_mergeable_ranks()\n","        mergeable_ranks = {bytes(k): v for k, v in mergeable_ranks_list}\n","        tokens_offset = len(mergeable_ranks)\n","        special_tokens = {name: tokens_offset + i for i, name in enumerate(SPECIAL_TOKENS)}\n","\n","        enc = tiktoken.Encoding(name=\"rustbpe\", pat_str=pattern, mergeable_ranks=mergeable_ranks, special_tokens=special_tokens)\n","\n","        return cls(enc, \"<|sos|>\")\n","\n","    def encode(self, text):\n","        return self.enc.encode_ordinary(text)\n","\n","    def encode_special(self, text):\n","        return self.enc.encode_single_token(text)\n","\n","    def decode(self, ids):\n","        return self.enc.decode(ids)\n","\n","    def render_conversation(self, conversation, max_tokens=2048):\n","        \"\"\"\n","        Tokenize a single Chat conversation (which we call a \"doc\" or \"document\" here).\n","        Returns:\n","        - ids: list[int] is a list of token ids of this rendered conversation\n","        \"\"\"\n","        messages = conversation[\"messages\"]\n","\n","        # now we can tokenize the conversation\n","        ids = [self.encode_special('<|sos|>')]\n","\n","        for i, message in enumerate(messages):\n","            content = message[\"content\"]\n","\n","            if message[\"role\"] == \"user\":\n","                ids += [self.encode_special('<|soum|>')] + self.encode(content) + [self.encode_special('<|eoum|>')]\n","\n","            elif message[\"role\"] == \"assistant\":\n","                ids += [self.encode_special('<|soam|>')] + self.encode(content) + [self.encode_special('<|eoam|>')]\n","\n","        # truncate to max_tokens tokens MAX (helps prevent OOMs)\n","        ids = ids[:max_tokens]\n","\n","        return ids\n","\n","    def visualize_tokenization(self, ids):\n","        \"\"\"Small helper function useful in debugging: visualize the tokenization of render_conversation\"\"\"\n","        RED = '\\033[91m'\n","        GREEN = '\\033[92m'\n","        RESET = '\\033[0m'\n","        GRAY = '\\033[90m'\n","\n","        tokens = []\n","        for i, token_id in enumerate(ids):\n","            token_str = self.decode([token_id])\n","            tokens.append(f\"{GREEN}{token_str}{GRAY}({token_id}){RESET}\")\n","\n","            if token_str in ['<|sos|>', '<|eoum|>', '<|eoam|>']:\n","                tokens.append('\\n\\n\\t')\n","\n","        return ' | '.join(tokens)"],"metadata":{"id":"nPVeFvJzd7d5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_text = uts.get_file_str('shakespeare.txt')\n","\n","nanochat_tokenizer = RustBPETokenizer.train_from_iterator(training_text, 256 + len(SPECIAL_TOKENS))\n","\n","conversation = {\n","    'messages': [\n","        {'role': 'user', 'content': 'Hello there general kenobi'},\n","        {'role': 'assistant', 'content': 'Hello there grevious'},\n","        {'role': 'user', 'content': 'I love neffer'},\n","    ]\n","}\n","\n","ids = nanochat_tokenizer.render_conversation(conversation)\n","print(nanochat_tokenizer.visualize_tokenization(ids))"],"metadata":{"id":"oO7H8-XJfg84"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Export to HTML\n","\n","Uncomment the final line of the cell below and run it to export this notebook to HTML"],"metadata":{"id":"xSLHa2zwsAlr"}},{"cell_type":"code","source":["import os, json\n","\n","def export_notebook():\n","  L_PATH = \"nd-cse-10124-lectures/Notebooks\"\n","  L = \"Lecture_03_Tokenization\"\n","\n","  try:\n","      from google.colab import _message, files\n","\n","      # where you WANT it to live (repo folder)\n","      repo_ipynb_path = f\"/content/{L_PATH}/{L}.ipynb\"\n","\n","      # grab current notebook contents from the UI\n","      nb = _message.blocking_request(\"get_ipynb\", timeout_sec=1)[\"ipynb\"]\n","\n","      # write it into the repo folder as a real file\n","      os.makedirs(os.path.dirname(repo_ipynb_path), exist_ok=True)\n","      with open(repo_ipynb_path, \"w\", encoding=\"utf-8\") as f:\n","          json.dump(nb, f)\n","\n","      # convert + download pdf\n","      !jupyter nbconvert --to html \"{repo_ipynb_path}\"\n","      files.download(repo_ipynb_path.replace(\".ipynb\", \".html\"))\n","  except:\n","      import subprocess\n","\n","      nb_fp = os.getcwd() + f'{L}.ipynb'\n","      print(os.getcwd())\n","\n","      subprocess.run([\"jupyter\", \"nbconvert\", \"--to\", \"html\", nb_fp], check=True)\n","\n","#export_notebook()"],"metadata":{"id":"1HCtuuZFr_oT"},"execution_count":null,"outputs":[]}]}