{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPiy2OUqpzDXZJJFNXk0j4Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture05/slide-001.png\" class=\"img-responsive\"/>\n","</div>\n","\n"],"metadata":{"id":"EUZY_VNVJVKT"}},{"cell_type":"code","source":["def create_matrix(rows, cols):\n","    return [[0] * cols for r in range(rows)]\n","\n","print(create_matrix(2, 2))"],"metadata":{"id":"3Sj_A8jo8vbE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769529125450,"user_tz":300,"elapsed":2,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"3d59f9d4-e54a-4ec0-827f-ebe3d948ce07"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0, 0], [0, 0]]\n"]}]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture05/slide-002.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture05/slide-003.png\" class=\"img-responsive\"/>\n","</div>\n"],"metadata":{"id":"mxAAF5GY8vxw"}},{"cell_type":"code","source":["def replace_pairs(lst, target_pair, combined_id):\n","    new_lst = []\n","\n","    i = 0\n","    while i < len(lst):\n","        if (lst[i], lst[i+1]) == target_pair:\n","            new_lst.append(combined_id)\n","            i += 2\n","        else:\n","            new_lst.append(lst[i])\n","            i += 1\n","\n","    return new_lst\n","\n","\n","print(replace_pairs([1, 2, 3, 1, 2], (1, 2), 4))\n","#[4, 3, 4]"],"metadata":{"id":"58YQQupe_7UK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769104401185,"user_tz":300,"elapsed":11,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"a88616cd-cf86-424e-fd4b-704bb9c68def"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[4, 3, 4]\n"]}]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture05/slide-004.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture05/slide-005.png\" class=\"img-responsive\"/>\n","</div>\n","\n"],"metadata":{"id":"5spfu41pJHGy"}},{"cell_type":"code","source":["import os\n","\n","try:\n","    import google.colab\n","    REPO_URL = \"https://github.com/wtheisen/nd-cse-10124-lectures.git\"\n","\n","    REPO_NAME = \"/content/nd-cse-10124-lectures\"\n","    L_PATH = \"nd-cse-10124-lectures/Datasets\"\n","\n","    %cd /content/\n","    !rm -r {REPO_NAME}\n","\n","    # Clone repo\n","    if not os.path.exists(REPO_NAME):\n","        !git clone {REPO_URL}\n","\n","        # cd into the data folder\n","        %cd {L_PATH}\n","        !pwd\n","\n","except ImportError:\n","    print(\"Unable to download repo, either:\")\n","    print(\"\\tA.) You're not on colab\")\n","    print(\"\\tB.) It has already been cloned\")\n","\n","\n","import utilities as uts"],"metadata":{"id":"XauIb-liOtSM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769105000956,"user_tz":300,"elapsed":1639,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"acd9bc7e-a0be-4884-8406-7c9949428359"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","Cloning into 'nd-cse-10124-lectures'...\n","remote: Enumerating objects: 230, done.\u001b[K\n","remote: Counting objects: 100% (230/230), done.\u001b[K\n","remote: Compressing objects: 100% (162/162), done.\u001b[K\n","remote: Total 230 (delta 145), reused 151 (delta 66), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (230/230), 19.88 MiB | 36.75 MiB/s, done.\n","Resolving deltas: 100% (145/145), done.\n","/content/nd-cse-10124-lectures/Datasets\n","/content/nd-cse-10124-lectures/Datasets\n"]}]},{"cell_type":"code","source":["from collections import Counter\n","\n","def _count_pairs(tokens):\n","    pair_counts = Counter()\n","\n","    for pair in zip(tokens, tokens[1:]):\n","        pair_counts[pair] += 1\n","\n","    return pair_counts\n","\n","def _merge_pair(tokens, target_pair, combined_id):\n","    merged_tokens = []\n","\n","    i = 0\n","    while i < len(tokens):\n","        if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == target_pair:\n","            merged_tokens.append(combined_id)\n","            i += 2\n","        else:\n","            merged_tokens.append(tokens[i])\n","            i += 1\n","\n","    return merged_tokens\n","\n","prompt = 'rug pug hug pun bun hugs run gun bug'\n","\n","string_bytes = list(prompt.encode('utf-8'))\n","print('String ASCII Values (bytes):', string_bytes)\n","\n","vocab = {idx: bytes([idx]) for idx in range(256)}\n","print('Initial Vocab:', vocab)\n","\n","pairs = _count_pairs(string_bytes)\n","print('Pairs and Counts:', pairs)\n","\n","merged_string = _merge_pair(string_bytes, (117, 103), 257)\n","print('After First Merge:', merged_string)\n","\n","merged_string = _merge_pair(merged_string, (117, 110), 258)\n","print('After Second Merge:', merged_string)"],"metadata":{"id":"6IA77Zb5FEAr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769110927743,"user_tz":300,"elapsed":19,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"d360c24e-b18a-48a1-9c51-4025a3108171"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["String ASCII Values (bytes): [114, 117, 103, 32, 112, 117, 103, 32, 104, 117, 103, 32, 112, 117, 110, 32, 98, 117, 110, 32, 104, 117, 103, 115, 32, 114, 117, 110, 32, 103, 117, 110, 32, 98, 117, 103]\n","Initial Vocab: {0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff'}\n","Pairs and Counts: Counter({(117, 103): 5, (117, 110): 4, (110, 32): 4, (103, 32): 3, (114, 117): 2, (32, 112): 2, (112, 117): 2, (32, 104): 2, (104, 117): 2, (32, 98): 2, (98, 117): 2, (103, 115): 1, (115, 32): 1, (32, 114): 1, (32, 103): 1, (103, 117): 1})\n","After First Merge: [114, 257, 32, 112, 257, 32, 104, 257, 32, 112, 117, 110, 32, 98, 117, 110, 32, 104, 257, 115, 32, 114, 117, 110, 32, 103, 117, 110, 32, 98, 257]\n","After Second Merge: [114, 257, 32, 112, 257, 32, 104, 257, 32, 112, 258, 32, 98, 258, 32, 104, 257, 115, 32, 114, 258, 32, 103, 258, 32, 98, 257]\n"]}]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-019.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"y1rrw6gH0o5Q"}},{"cell_type":"code","source":["class Cat():\n","    def __init__(self, name, color):\n","        self.name = name\n","        self.color = color\n","\n","    def purr(self):\n","        print('purrrrrrrr')\n","\n","cute_kitty = Cat('roger', 'orange')\n","print(type(cute_kitty))\n","\n","sad_kitty = Cat('fred', 'blue')\n","\n","print(cute_kitty.color)\n","\n","print(sad_kitty.name)"],"metadata":{"id":"G1IisgIj7feQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769106578988,"user_tz":300,"elapsed":41,"user":{"displayName":"William Theisen","userId":"17727777209816459717"}},"outputId":"e18b872d-1b3a-482d-a417-e17c691c0c4d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class '__main__.Cat'>\n","orange\n","fred\n"]}]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-020.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"OPdPw_Jieywk"}},{"cell_type":"code","source":["\"\"\"\n","Minimal (byte-level) Byte Pair Encoding tokenizer.\n","\n","Algorithmically follows along the GPT tokenizer:\n","https://github.com/openai/gpt-2/blob/master/src/encoder.py\n","\"\"\"\n","\n","class Simple_Tokenizer():\n","    def __init__(self):\n","        self.merges = {} # our mapping of pairs to their token id\n","\n","        self.special_tokens = {'<|sos|>': 256, '<|eos|>': 257} # special token mapping {str:int}\n","        self.vocab = {idx: bytes([idx]) for idx in range(256)} # Default vocab of the 256 ASCII characters {int:char}\n","\n","        # Make sure we add our special tokens to our vocab to start\n","        self.vocab[256] = '<|sos|>'\n","        self.vocab[256] = '<|eos|>'\n","\n","    def _count_pairs(self, tokens, counts=None):\n","        pair_counts = Counter()\n","\n","        for pair in zip(tokens, tokens[1:]):\n","            pair_counts[pair] += 1\n","\n","        return pair_counts\n","\n","    def _merge_pair(self, tokens, target_pair, combined_id):\n","        merged_tokens = []\n","\n","        i = 0\n","        while i < len(tokens):\n","            if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == target_pair:\n","                merged_tokens.append(combined_id)\n","                i += 2\n","            else:\n","                merged_tokens.append(tokens[i])\n","                i += 1\n","\n","        return merged_tokens\n","\n","    def train(self, text, max_vocab_size):\n","        vocab_size = len(self.vocab)\n","        num_merges = max_vocab_size - vocab_size\n","\n","        text_bytes = text.encode('utf-8')\n","        tokens = list(text_bytes)\n","\n","        for i in range(num_merges):\n","            pairs = self._count_pairs(ids)\n","\n","            pair = pairs.most_common()[0][0]\n","\n","            merged_token_id = vocab_size + i\n","\n","            ids = self._merge_pair(tokens, pair, merged_token_id)\n","\n","            self.merges[pair] = merged_token_id\n","            self.vocab[merged_token_id] = self.vocab[pair[0]] + self.vocab[pair[1]]\n","\n","    def decode(self, ids):\n","        pass\n","\n","    def encode(self, text):\n","        pass\n","\n","    def visualize_tokenization(self, ids):\n","        \"\"\"Small helper function useful in debugging: visualize the tokenization of render_conversation\"\"\"\n","        GREEN = '\\033[92m'\n","        RESET = '\\033[0m'\n","        GRAY = '\\033[90m'\n","\n","        tokens = []\n","        for token_id in ids:\n","            token_str = self.decode([token_id])\n","            tokens.append(f\"{GREEN}{token_str}{GRAY}({token_id}){RESET}\")\n","\n","            if token_str in ['<|sos|>', '<|eos|>']:\n","                tokens.append('\\n\\n\\t')\n","\n","        return ' | '.join(tokens)"],"metadata":{"id":"cSR_No12OBEj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#print('Size of Jabberwocky Graph (Tokens):', len(uts.create_token_graph('jabberwocky.txt')))\n","#print('Size of Shakespeare Graph (Tokens):', len(uts.create_token_graph('shakespeare.txt')))\n","# Size of Shakespeare Graph (Tokens): 286 (this takes like ~7 minutes to run)\n","\n","num_gens = 3\n","\n","char_graph = uts.build_graph_char('zoomer.txt')\n","print('Size of Zoomer Graph (Characters):', len(char_graph))\n","print('Character Level Generation [zoomer.txt]:')\n","for i in range(0, num_gens):\n","  print(f'\\t{i}: {''.join(uts.generate_sequence(char_graph, 50))}')\n","\n","print('#' * 80)\n","\n","for vocab_size in [258, 512, 1024, 2048, 4096]:\n","    s_t = Simple_Tokenizer()\n","\n","    token_graph = uts.build_graph_token('zoomer.txt', s_t, vocab_size=vocab_size)\n","    print('Size of Zoomer Graph (Tokens, No Regex):', len(token_graph))\n","\n","    print(f'Token Level Generation (No Regex, {vocab_size} Vocab Size) [zoomer.txt]:')\n","    for i in range(0, num_gens):\n","        print(f'\\t{i}: {''.join(uts.generate_sequence(token_graph, 50))}')\n","    print('#' * 80)\n","\n","word_graph = uts.build_graph_word('zoomer.txt', special_tokens=True)\n","print('Size of Zoomer Graph (Words):', len(word_graph))\n","print('Word Level Generation [zoomer.txt]:')\n","for i in range(0, num_gens):\n","  print(f'\\t{i}: {' '.join(uts.generate_sequence(word_graph, 50))}')\n"],"metadata":{"id":"-0aQhw4u1u0w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-021.png\" class=\"img-responsive\"/>\n","</div>\n","\n","<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-022.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"qg5A-_onfRLz"}},{"cell_type":"code","source":["from tokenizer import RegexTokenizer\n","\n","r_t = RegexTokenizer()\n","\n","print(r_t.GPT4_SPLIT_PATTERN)\n","\n","print(r_t.chunk(\"No cap are u rolling the party tonight?\"))\n","\n","for vocab_size in [512, 1024, 2048, 4096]:\n","    r_t = RegexTokenizer()\n","\n","    token_graph = uts.build_graph_token('zoomer.txt', r_t, vocab_size=vocab_size)\n","    print('#' * 80)\n","    print(f'''Size of Zoomer Graph (Tokens, Regex, {vocab_size} Vocab Size):', len(token_graph))\n","\n","Token Level Generation (Regex, {vocab_size} Vocab Size) [zoomer.txt]:''')\n","\n","    for i in range(0, num_gens):\n","        print(f'\\t{i}: {''.join(uts.generate_sequence(token_graph, 50))}')"],"metadata":{"id":"dv3sQu75KJyT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"thumbnail\">\n","    <img src=\"https://williamtheisen.com/nd-cse-10124-lectures/Lecture_Images/Lecture04/slide-024.png\" class=\"img-responsive\"/>\n","</div>"],"metadata":{"id":"Xs-8GO8-fZIp"}},{"cell_type":"markdown","source":["## Export to HTML\n","\n","Uncomment the final line of the cell below and run it to export this notebook to HTML"],"metadata":{"id":"xSLHa2zwsAlr"}},{"cell_type":"code","source":["import os, json\n","\n","def export_notebook():\n","  L_PATH = \"nd-cse-10124-lectures/Notebooks\"\n","  L = \"Lecture_04_Tokenization\"\n","\n","  try:\n","      from google.colab import _message, files\n","\n","      # where you WANT it to live (repo folder)\n","      repo_ipynb_path = f\"/content/{L_PATH}/{L}.ipynb\"\n","\n","      # grab current notebook contents from the UI\n","      nb = _message.blocking_request(\"get_ipynb\", timeout_sec=1)[\"ipynb\"]\n","\n","      # write it into the repo folder as a real file\n","      os.makedirs(os.path.dirname(repo_ipynb_path), exist_ok=True)\n","      with open(repo_ipynb_path, \"w\", encoding=\"utf-8\") as f:\n","          json.dump(nb, f)\n","\n","      # convert + download pdf\n","      !jupyter nbconvert --to html \"{repo_ipynb_path}\"\n","      files.download(repo_ipynb_path.replace(\".ipynb\", \".html\"))\n","  except:\n","      import subprocess\n","\n","      nb_fp = os.getcwd() + f'{L}.ipynb'\n","      print(os.getcwd())\n","\n","      subprocess.run([\"jupyter\", \"nbconvert\", \"--to\", \"html\", nb_fp], check=True)\n","\n","#export_notebook()"],"metadata":{"id":"1HCtuuZFr_oT"},"execution_count":null,"outputs":[]}]}